# Simple Linear regression

{{< include ../math.qmd >}}

## SLR


::: {#def-}
# A First-Order Model
$$
y=\beta_0+\beta_1x+\varepsilon
$$
where

- $y$: The **response** variable
- $x$: The **independent** variable (variable used as a **predictor** of $y$)
- $\Exp(y)=\beta_0+\beta_1x$: Deterministic component
- $\varepsilon$: Random error component
- $\beta_0$: **$y$-intercept**
- $\beta_1$: **Slope**

For $\varepsilon$, we have the following assumptions:

- $\varepsilon$ follows Normal distribution for all $x$.
- The mean is constant to be $0$.
- The variance is constant $\sigma^2$ (homoscedastic).
- The errors associated with two observations are independent.

In other words, $\varepsilon_i\sim N(0,\sigma^2)$ i.i.d.
:::


::: {#def-}
## Regression equation
Simple linear regression equation is
$$
\Exp(y)=\beta_0+\beta_1x.
$$
:::


::: {#def-}
## Estimated regression equation
The estimated simple linear regression equation is
$$
\hat y=\hat\beta_0+\hat\beta_1x.
$$
$\hat\beta_0$ and $\hat\beta_1$ are estimators of $\beta_0$ and $\beta_1$.
:::

The purpose of the section is to estimate $\beta_0$, $\beta_1$ as well as $\sigma^2$. 

<!-- 
### Fitting the model

MLE=LSE under the assumption that $\epsilon$ is normal


different estimators 




first assuming that $\sigma$ is constant.



- Sum of squares of deviations for the $x$s: $SS_{xx}=\sum(x_i-\bar{x})^2$
- Sum of squares of deviations for the $y$s: $SS_{yy}=\sum(y_i-\bar{y})^2$
- Sum of cross-products: $SS_{xy}=\sum(x_i-\bar{x})(y_i-\bar{y})$



::: {#thm-}
Estimate $\beta_0$, $\beta_1$ and $\sigma^2$. The NLL is
$$
NLL = -\frac{n}{2}\ln\qty(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum\qty(y_i-\beta_0-\beta_1x_i)^2.
$$
:::


::: {.solution}
$$
\begin{split}
\pdv{NLL}{\beta_0}&=
\end{split}
$$
::: -->



## Estimation

<!-- b0, b1 are estimations

1. b0, b1 formula
2. E(b0) E(b1)
3. Var(b0), Var(b1)
4. relations to y: b1, ybar independent, b0, b1 are independent -->
   

### OLS estimators

The standard approach is known as ordinary least squares (OLS) estimation. In this method, model parameters are chosen to minimize the sum of squared residuals between the observed responses and the values predicted by the model. From an optimization perspective, OLS minimizes a loss function that measures the discrepancy between the data and the fitted model. 

Here are more details.

Consider the linear regression model 
$$
y_i=\beta_0+\beta_1x_i+\varepsilon_i,\quad i=1,\ldots, n.
$$

In order to find the best parameters, we need to minimize the sum of squared errors 
$$
L(\beta_0, \beta_1) = \sum_{i=1}^n\varepsilon_i^2=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.
$$
Since this loss can be treated as a function of $\beta_0$ and $\beta_1$, to minimize it, we could consider the critical point, which is 

$$
\pdv{L}{\beta_0}=0,\quad \pdv{L}{\beta_1}=0.
$$
In other words, we have

$$
\begin{split}
\pdv{L}{\beta_0}&=\sum_{i=1}^n2(y_i-\beta_0-\beta_1x_i)(-1)=-2\qty(\sum_{i=1}^ny_i-n\beta_0-\beta_1(\sum_{i=1}^nx_i))=-2n(\bar y-\beta_0-\beta_1\bar x)=0,\\
\pdv{L}{\beta_1}&=\sum_{i=1}^n2(y_i-\beta_0-\beta_1x_i)(-x_i).
\end{split}
$$
The first equation gives us $\beta_0=\bar y-\beta_1\bar x$. Then the second equation is
$$
\begin{split}
\pdv{L}{\beta_1}&=\sum_{i=1}^n2(y_i-\beta_0-\beta_1x_i)(-x_i)=-2\sum_{i=1}^n(y_i-\bar y+\beta_1\bar x-\beta_1x_i)(x_i)\\
&=-2\sum_{i=1}^n(y_i-\bar y-\beta_1(x_i-\bar x))(x_i)=-2\sum_{i=1}^n(y_i-\bar y-\beta_1(x_i-\bar x))(x_i-\bar x)\\
&=-2\qty(\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)-\beta_1\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x))\\
&=-2\qty(SS_{xy}-\beta_1SS_{xx})=0.
\end{split}
$$

Therefore, the solution to the equations, which is the best parameters that minimize the sum of square errors, is

::: {#thm-}

$$
\hat{\beta}_1=\frac{SS_{xy}}{SS_{xx}},\quad \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}.
$$

:::


### MLE estimators


Since we assume $\varepsilon_i\sim \mathcal N(0,\sigma^2)$ i.i.d, we could actually compute the likelihood function explicitly. Recall that $y_i=\beta_0+\beta_1x_i+\varepsilon_i$, then
$$
\begin{split}
p(y_i\mid x_i,\beta_0,\beta_1,\sigma^2)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp\qty(-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}).
\end{split}
$$
Take all data into consideration. Let $\mathbf x=(x_1,\ldots,x_n)$ and $\mathbf y=(y_1,\ldots,y_n)$. Assuming that all $\varepsilon_i$'s are independent, then the likelihood function is
$$
L(\beta_0,\beta_1,\sigma^2\mid \mathbf x, \mathbf y)=\prod_{i=1}^n\frac1{\sqrt{2\pi\sigma^2}}\exp\qty(-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}).
$$
We could take negative log to make it simpler
$$
\nll(\beta_0,\beta_1,\sigma^2\mid \mathbf x, \mathbf y)=\frac n2\ln\qty(2\pi\sigma^2)+\frac1{2\sigma^2}\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.
$$
This derived loss function is called the negative log-likelihood (NLL). Maximizing the likelihood is equivalent to minimizing $\nll$.

For fixed $\sigma^2$, minimizing $\nll$ over $(\beta_0,\beta_1)$ is equivalent to minimizing $\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2$, so the MLE of $(\beta_0,\beta_1)$ is the same as the OLS estimator. 

To estimate $\sigma^2$, treat $\sigma^2$ as a variable and differentiate:
$$
\pdv{\nll}{\sigma^2}=\frac{n}2\frac1{\sigma^2}-\frac12\frac{1}{(\sigma^2)^2}\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2=0.
$$
Setting this derivative to zero yields

::: {#thm-}

$$
\hat\sigma_{MLE}^2=\frac1n\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\frac{SSE}{n}. 
$$

:::
Similar to the sample variance scenario, the MLE estimator for variance is biased. The unbiased estimator of the error variance is the sample variance of the residuals. 
$$
s^2=\frac{SSE}{n-2}.
$$



### Properties of these estimators


::: {#thm-}
$$
\Exp(\hat{\beta}_0)=\beta_0, \quad \Exp(\hat{\beta}_1)=\beta_1,\quad \Exp(s^2)=\sigma^2. 
$$

<details>
<summary><strong>Click for proof.</strong></summary>

The key point is to treat all $x_i$'s as constants, and $y_i=\beta_0+\beta_1x_i+\varepsilon_i$ is a random variable due to $\varepsilon_i$. Note that $\Exp(y_i)=\beta_0+\beta_1x_i$ and $\Exp(\bar y)=\frac1n\sum_{i=1}^n\Exp(y_i)=\beta_0+\beta_1\bar x$.
$$
\begin{split}
\Exp(SS_{xy})&=\Exp\qty(\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y))\\
&=\sum_{i=1}^n(x_i-\bar x)(\Exp(y_i)-\Exp(\bar y))\\
&=\sum_{i=1}^n(x_i-\bar x)(\beta_0+\beta_1x_i-\beta_0-\beta_1\bar x)\\
&=\sum_{i=1}^n(x_i-\bar x)\beta_1(x_i-\bar x)\\
&=\beta_1SS_{xx}.
\end{split}
$$
Therefore 
$$
\Exp(\hat\beta_1)=\Exp\qty(\frac{SS_{xy}}{SS_{xx}})=\beta_1.
$$
We now show that $\hat\beta_0$ is an unbiased estimator of $\beta_0$.
Recall that
$$
\hat\beta_0 = \bar y - \hat\beta_1 \bar x.
$$
Taking expectations and using linearity of expectation,
$$
\begin{aligned}
\Exp(\hat\beta_0)
&= \Exp(\bar y) - \bar x\,\Exp(\hat\beta_1) \\
&= (\beta_0 + \beta_1 \bar x) - \bar x\,\beta_1 \\
&= \beta_0.
\end{aligned}
$$
Therefore, $\hat\beta_0$ is an unbiased estimator of $\beta_0$.

For $s^2$, it requires more knowledge from $\chi^22$ distribution. 

- $\varepsilon_i\sim \mathcal N(0,\sigma^2)$ i.i.d, and $x_i$'s are all constants.
- $e_i=y_i-\hat y_i=\varepsilon_i-(\hat\beta_0-\beta_0)+(\hat\beta_1-\beta_1)x_i$. Note that $\hat\beta_0-\beta_0$ and $\hat\beta_1-\beta_1$ are functions of all $\varepsilon_i$'s so each residual depends on the entire error vector.
- Since $SSE=\sum_{i=1}^ne_i^2$, by @thm-norm, one can show that $\frac{SSE}{\sigma^2}\sim \chi_{n-2}^2$. The loss of 2 degrees of freedom comes from estimating $\beta_0$ and $\beta_1$.
- Therefore $\Exp\qty(\frac{SSE}{\sigma^2})=n-2$.
- Therefore $\Exp(s^2)=\frac1{n-2}\Exp(SSE)=\sigma^2$.

</details>
:::




::: {#thm-}

$$
\Var(\hat{\beta}_0)=\frac{\sigma^2}{S_{xx}}, \quad \Var(\hat{\beta}_1)=\qty(\frac{1}{n}+\frac{\bar x^2}{nS_{xx}})\sigma^2. 
$$

<details>
<summary><strong>Click for proof.</strong></summary>
$$
\Var(\hat{\beta}_1)=\frac{1}{S_{xx}^2}\Var(S_{xy})=\frac{1}{S_{xx}^2}\Var\qty[\sum x_i(y_i-\bar{y})]=\frac{1}{S_{xx}^2}S_{xx}\sigma^2=\frac{\sigma^2}{S_{xx}}.
$$
$$
\begin{split}
\Var(\hat{\beta}_0)&=\Var\qty(\bar y-\hat{\beta}_1\bar x)=\Var(\bar y)+\bar{x}^2\Var(\hat{\beta}_1)=\frac{\sigma^2}{n}+\bar x^2\frac{\sigma^2}{nS_{xx}}=\qty(\frac{1}{n}+\frac{\bar x^2}{nS_{xx}})\sigma^2.
\end{split}
$$
</details>



:::






::: {#thm-}

$$
\Var(s^2)=
$$

<details>
<summary><strong>Click for proof.</strong></summary>

</details>
:::










## $\beta_1$

hypotheis: $\beta_1\neq0$

$\hat{\beta}_1$ is normal, so t-test:

$$
t=\frac{\hat{\beta}_1-0}{s_{\hat{\beta}_1}}=\frac{\hat{\beta}_1}{s/\sqrt{S_{xx}}},\quad s^2=\frac{SSE}{n-2}
$$

- one tailed test:
  - Ha: $\beta_1<0$: $t<-t_{\alpha}$: $p=\Pr(t<t_c)$, $t_c<0$
  - Ha: $\beta_1>0$: $t>t_{\alpha}$: $p=\Pr(t>t_c)$, $t_c>0$
- two tailed test:
  - Ha: $\beta_1\neq0$: $\abs{t}>t_{\alpha/2}$: $p=2\Pr(t>t_c)$, $t_c>0$

$t_c$ is the computed value

$\Pr(t>t_{\alpha})=\alpha=\Pr(\text{Type I error})=\Pr(\text{Reject }H_0\mid H_0\text{ is true.})$



## coeffcient of correlation

$$
r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
$$

it is related to COV


$\rho$ population coefficient of correlation, is estimated by $r$

Hypothesis:

$t=r\sqrt{n-2}/{sqrt{1-r^2}}$


## coefficient of determination
$$
r^2=\frac{S_{yy}-SSE}{S_{yy}}=1-\frac{SSE}{S_{yy}}
$$

proportion of total saple variability of the y explained by the linear relationship between y and x

About 100(r2)% of the sample variation in y (measured by the total sum
of squares of deviations of the sample y-values about their mean ji) can be
explained by (or attributed to) using x to predict yin the straight-line model.


$$
SSR=\sum(\hat y_i-\bar y)^2=\frac{S_{xy}^2}{S_{xx}}
$$




## predicted interval and confidence interval

$\sigma_{\hat y}$

Since $\bar y$ and $\hat{\beta}_1$ are independent, we have
$$
\begin{split}
\Var(\hat y(x))&=\Var(\hat{\beta}_0+\hat{\beta}_1x)=\Var(\bar y-\hat{\beta}_1(x-\bar x))=\Var(\bar y)+(x-\bar x)^2\Var(\hat{\beta}_1)\\
&=\frac1n\sigma^2+(x-\bar x)^2\frac1{S_{xx}}\sigma^2=\sigma^2\qty[\frac1n+\frac{(x-\bar x)^2}{S_{xx}}].
\end{split}
$$


$$
\Var(y-\hat y)=\sigma^2+\sigma^2\qty[\frac1n+\frac{(x-\bar x)^2}{S_{xx}}]=\sigma^2\qty[1+\frac1n+\frac{(x-\bar x)^2}{S_{xx}}].
$$




```{r}
library(MASS)

fit <- lm(medv~lstat, data=Boston)

newx = data.frame(lstat=seq(1, 40, by=0.1))
pred.int <- predict(fit, newx, interval='prediction')
conf.int <- predict(fit, newx, interval='confidence')
```

Both `pred.int` and `conf.int` are matrices, where the first column is the fitted value, the second column and the third column are the corresponding interval bounds.


```{r}
plot(Boston$lstat, Boston$medv)

lines(newx$lstat, pred.int[,1])

lines(newx$lstat, pred.int[,2], col='red')
lines(newx$lstat, pred.int[,3], col='red')

lines(newx$lstat, conf.int[,2], col='blue')
lines(newx$lstat, conf.int[,3], col='blue')
```

