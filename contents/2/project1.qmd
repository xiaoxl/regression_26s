# Example: SLR phase

## The Boston Housing Dataset



The Boston housing dataset originates from a hedonic price analysis of 506 census tracts in the Boston Standard Metropolitan Statistical Area conducted by Harrison and Rubinfeld to estimate households’ willingness to pay for improvements in air quality, particularly reductions in nitrogen oxides (NOx) concentrations @Harrison1978. The data were assembled primarily from 1970 U.S. Census housing tabulations combined with modeled environmental pollution measures. Since its original publication, the dataset has been widely disseminated as the `Boston` dataset in econometrics, statistics, and machine learning, where it is commonly used to illustrate and evaluate linear regression methods. It was notably employed by Belsley, Kuh, and Welsch in Regression Diagnostics: Identifying Influential Data and Sources of Collinearity (pp. 244–261) @belsley1980regression to demonstrate diagnostic techniques for detecting influential observations and multicollinearity, and has subsequently appeared in numerous methodological studies, including Quinlan’s work on combining instance-based and model-based learning @quinlan1993combining.

In R, the `MASS` library contains `Boston` data set, which has 506 rows and 14 columns. The dataset records `medv` (median house value) and 13 predictors for 506 neighborhoods around Boston. 


## Explore the Boston Housing Dataset

First of all, let's take a look at the variables in the dataset Boston.

```{r}
library(MASS)
head(Boston)
names(Boston)
dim(Boston)
```

> - `names()` --- print out all the variable names;
> - `head()` --- print out the first 5 rows of the dataset;
> - `dim()` --- print out the shape of the dataset.

You may use `?Boston` to look at the brief describption.


Since we would like to directly work with varibles in the dataset, it is better to attach it to the working space.
```{r}
attach(Boston)
```



### Data Type

First we need to know the data type of every variable. Based on the information we have, we know 

| Variable  | Description                                                                 | Type                    |
|-----------|-----------------------------------------------------------------------------|-------------------------|
| `crim`    | Per capita crime rate by town                                                | Numerical, continuous   |
| `zn`      | Proportion of residential land zoned for lots over 25,000 sq.ft.            | Numerical, continuous   |
| `indus`   | Proportion of non-retail business acres per town                             | Numerical, continuous   |
| `chas`    | Charles River dummy variable (1 if tract bounds river; 0 otherwise)          | Categorical, nominal   |
| `nox`     | Nitrogen oxides concentration (parts per 10 million)                        | Numerical, continuous   |
| `rm`      | Average number of rooms per dwelling                                         | Numerical, continuous   |
| `age`     | Proportion of owner-occupied units built prior to 1940                       | Numerical, continuous   |
| `dis`     | Weighted mean distance to five Boston employment centers                     | Numerical, continuous   |
| `rad`     | Index of accessibility to radial highways (larger = better accessibility)   | Categorical, ordinal   |
| `tax`     | Full-value property-tax rate per \$10,000                                    | Numerical, continuous   |
| `ptratio` | Pupil–teacher ratio by town                                                  | Numerical, continuous   |
| `black`   | $1000(B_k - 0.63)^2$, where $B_k$ is the proportion of Black residents   | Numerical, continuous   |
| `lstat`   | Percentage of lower-status population                                       | Numerical, continuous   |
| `medv`    | Median value of owner-occupied homes (in \$1,000s)                          | Numerical, continuous   |



### Summary of all the variables

Usually we would like to check two things: the distribution of each variable, and finding all missing values.

Are there any missing values?

```{r}
sapply(Boston, anyNA)
```
From the output, we know there is no missing values in the `Boston` dataset.



Next, we find the summary of all the 13 variables as follows. These summary shows a brief glance of the distributions of all variables.

```{r}
summary(Boston)
```

Note that `chas` and `rad` are supposed to be categorical data. In this summary they are still treated as numerical. Therefore we could change their type if necessary.


```{r}
chas <- factor(chas)
rad <- factor(rad)

summary(Boston)
```



### Histogram

We could draw histogram for a particular variable, say `medv`.


```{r}
hist(medv)
```


We could also draw all the histogram for all numerical variables in the data frame `Boston` with the help of `Hmisc::hist.data.frame. Note that categorical data are automatically removed.

```{r}
#| fig-height: 8
#| fig-width: 10
#| warning: false
library(Hmisc)
hist.data.frame(Boston)
```




### Scatter plot

Finally, the most import plot for regression should be the scatter plot. For example, let us see the relation between `lstat` and `medv`.

```{r}
plot(lstat, medv)
```



## Simple Linear Regression 

We will seek to predict `medv` using 13 predictors such as `rm` (average number of rooms per house), `age` (proportion of the owner-occupied units built prior to 1940), or `lstat` (percent of households with low socioeconomic status).



### Find a strongest linear correlation 

In general there has to be a more complicated analysis about choosing the best variable. In this case in order to demonstrate the idea, we will only use the most correlted numerical variable. 


Here, we use corrlation matrix to find the independent variable which has the strongest linear correlation to the dependent variable. 

We first find out the numeric columns.


```{r}
Boston_num <- Boston[, sapply(Boston, is.numeric)] 
```

Then we compute the corrlation matrix. We would like to find the column that is most correlated to `medv`. We find that `lstat` and `medv` has the strongest linear correlation. 
```{r}
cor_matrix <- cor(Boston_num)
round(cor_matrix, 2)
```


The matrix can be visualized. `lstat` is also the most obvious choice from the plot.

```{r}
#| warning: false
library(ggcorrplot)
ggcorrplot(cor_matrix, hc.order = TRUE)
```


For the next step, we will explore the linear relationship between the two variables. That is, $y$=`medv`, $x$=`lstat`.

```{r}
plot(lstat, medv)
```

The data appears to demostrate a straight-line relationshiop. As the percentage of lower status of the population (`lstat`) increase, the median home value decrease, which fits with common sense. The scatterplot shows curvilinear relation, which suggests a curivilinear model might be a better fit. In this chapter, we fit the straight-line model using `lm`.   

Here is a quick glance about how `lm` works.
```{r}
model <- lm(medv ~ lstat, data = Boston)
model
coef(model)
```

The estimated regression equation by using least squares method is $\hat{y}$=`{r} coef(model)[1]`$+$(`{r} coef(model)[2]`)$x$. 

### Test of model utility


```{r}
#| echo: false
source("../../anova_alt.R")
```

We would like to see the output of the model.
```{r}
anova_alt(model)
summary(model)
```


Let us check the confidence interval for $\beta_1$.

```{r}
# Combine to display the coef and confidence interval for the parameters together;
coef <- coefficients(summary(model))
coef_CI <- confint(model, level = 0.95)
cbind(coef, coef_CI)
```

- **Significance**: We test the $H_0:\beta_1=0$ vs $H_a: \beta_1\neq 0$. For simple linear regression, the result would be the same as the correlation hypothesis test above. We will conclude percentage of lower status (`lstat`) is a significant predictor to the median house price (`medv`). 
- **CI for $\beta_1$**: The confidence interval for slope $\beta_1$ is (`r coef_CI[2,]`). We are 95\% confident that the mean median home price decreases around 0.874 to 1.026 thousands of dollors per additional percent increase in low-status of the population.
- **sd for $\beta_1$**: The estimated standard deviation of $\varepsilon$ is $s$=`r summary(model)$sigma`, which implies that most of the observed median home price will fall within in approximately $2s$=12.4 thousands of dollars of their respective predicted values when using the least squares line. 
- **$R^2$**: The coefficient of determination is `r round(summary(model)$r.squared,3)`. This value implies that about 54% of the sample variation in median home price is explained by the low-status percent and the median home price. However, as we note that $R^2$ is not very high, we need to modify our model in the future.
- **Prediction**:  With relatively small $2s$, significant linear relationship between `lstat` and `medv`, we could get confidence interval and prediction interval as follows. 
```{r}
plot(lstat, medv)

newx <- data.frame(lstat = seq(min(lstat), max(lstat), by = 0.1))
pred.int <- predict(model, newx, interval = "prediction")
conf.int <- predict(model, newx, interval = "confidence")

lines(newx$lstat, pred.int[, "fit"])

lines(newx$lstat, pred.int[, "lwr"], col = "red")
lines(newx$lstat, pred.int[, "upr"], col = "red")

lines(newx$lstat, conf.int[, "lwr"], col = "blue")
lines(newx$lstat, conf.int[, "upr"], col = "blue")
```

 
 
 
### Residual Analysis

The detailed residual analysis will be covered in the future. Here we only show the very basic analysis, that is the plot of the residual vs the predictor variable `lstat`.

```{r}
res <- resid(model)
plot(lstat, res)

```


We could find that there is a clear u-shape in the residual plot. And the variance shows non-constant variance issue, as the size of the residuals decreases as the value of lstat increase. This residual plot indicates that a multiplicative model may be appropriate. 

