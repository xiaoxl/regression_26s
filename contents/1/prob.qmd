


{{< include ../math.qmd >}}



# Probability

## Notations
- $Y$: a random variable (captial letters)
- $y$: a sample of $Y$
- $\Pr\qty(Y\in A\mid\theta)$: the probability of $Y$ being in $A$
- $p(y\mid\theta)=\Pr\qty(Y=y\mid\theta)$: the probability mass function (discrete case)
- $f(y\mid\theta)=\displaystyle\dv{y}\Pr\qty(Y\leq y\mid\theta)$: the probability density function (continuous case)
- $\Exp\qty(Y)$: the expectation of $Y$
- $\Var\qty(Y)$: the variance of $Y$



## Random variables


::: {#def-}
# Expectation
$$
\Exp\mqty[u(X)] = \int_{-\infty}^{\infty}u(x)f(x)\dl3x.
$$
:::


::: {#def-}
1. $\mu=\Exp(X)$ is called the **mean value** of $X$.
2. $\sigma^2=\Var(X)=\Exp\mqty[(X-\mu)^2]$ is called the **variance** of $X$.
3. $M_X(t)=\Exp\mqty[\me^{tX}]$ is called the **moment generating function** of $X$.
:::


::: {#prp-}
1. $\Exp\mqty[ag(X)+bh(X)]=a\Exp\mqty[g(X)]+b\Exp\mqty[h(X)]$.
2. $\Var\mqty[X]=\Exp\mqty[(X-\mu)^2]=\Exp(X^2)-\mu^2$.
3. If $X$ and $Y$ are independent, $\Var\mqty[aX+bY]=a^2\Var(X)+b^2\Var(Y)$.

<details>
<summary><strong>Click for proof.</strong></summary>


$$
\begin{split}
\Exp\mqty[ag(X)+bh(X)]&=\int_{-\infty}^{\infty}\mqty[ag(x)+bh(x)]f(x)\dl3x\\
                 &=a\int_{-\infty}^{\infty}g(x)f(x)\dl3x+b\int_{-\infty}^{\infty}h(x)f(x)\dl3x\\
                 &=a\Exp\mqty[g(X)]+b\Exp\mqty[h(X)].
\end{split}
$$

$$
\begin{split}
\Exp\mqty[(X-\mu)^2]&=\Exp\mqty[\qty(X^2-2\mu X+\mu^2)]=\Exp(X^2)-2\mu\Exp(X)+\Exp(\mu^2)\\
&=\Exp(X^2)-2\mu\mu+\mu^2=\Exp(X^2)-\mu^2.
\end{split}
$$

$$
\begin{split}
\Var\mqty[aX]&=\Exp(a^2X^2)-a^2\mu^2=a^2\qty(\Exp(X^2)-\mu^2)=a^2\Var(X),\\
\Var\mqty[X+Y]&=\Exp((X+Y)^2)-(\Exp(X+Y))^2\\
&=\Exp(X^2)+\Exp(Y^2)+2\Exp(XY)-\Exp(X)^2-\Exp(Y)^2-2\Exp(X)\Exp(Y)\\
&=\Var(X)+\Var(Y)+2(E(XY)-E(X)E(Y))\\
&=\Var(X)+\Var(Y),\\
\Var\mqty[aX+bY]&=a^2\Var(X)+b^2\Var(Y).
\end{split}
$$

</details>
:::



::: {.callout-note}
Assume $X_1,\ldots, X_n$ i.i.d. with mean $\mu$ and variance $\sigma^2$. Then $\Var(\frac1n\sum X_i)=\sigma^2/n$. This implies that the more samples you pick, the smaller the variance is. This explains why, when possible, we want a large sample size. Note that we don't specify any concrete distribution in this remark. This is related to estimation, which will be discussed in detail later.
:::



### R code
R has built-in random variables with different distributions. The naming convention is a prefix `d-`, `p-`, `q-` and `r-` together with the name of distribution. 

- `d-`: density function of the given distribution;
- `p-`: cumulative density function of the given distribution;
- `q-`: quantile function of the given distribution (which is the inverse of `p-` function);
- `r-`: random sampling from the given distribution.


::: {#exm-}
## Normal distribution


<details>
<summary><strong>Click to expand.</strong></summary>


```{r}
x <- seq(-4, 4, length=100)
y <- dnorm(x, mean=2, sd=0.5)
plot(x, y, type="l")
```


```{r}
x <- seq(-4, 4, length=100)
y <- pnorm(x, mean=2, sd=0.5)
plot(x, y, type="l")
```


```{r}
qnorm(0.025)
qnorm(0.5)
qnorm(0.975)
```


```{r}
rnorm(10)
```

</details>


:::




## Some important distributions

### Normal Distribution

::: {#thm-norm}
## Normal Sample Meanâ€“Variance
Let $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ i.i.d. Define

- Sample mean: $\bar X=\frac1n\sum_{i=1}^nX_i$
- Sample variance: $s^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2$

Then

1. $\bar X\sim N(\mu,\frac{\sigma^2}n)$
2. $\frac{(n-1)s^2}{\sigma^2}\sim \chi^2_{n-1}$
3. $\bar X$ and $s^2$ are independent.



:::


The complete proof is lengthy and out of scope of the course. Please refer to @Hog2019 for details.





### Student's t-Distribution {#sec-t-dist}


::: {#thm-t}
## Student's t-Distribution @Hog2019
Let 

- $Z\sim N(0,1)$ be a standard normal random variable
- $U\sim \chi_{\nu}^2$ be a chi-square random variable with $\nu$ degrees of freedom. 
- $Z$ and $U$ are independent.

Then
$$
T=\frac{Z}{\sqrt{U/\nu}}
$$
has a Student's t-distribution with $\nu$ degrees of freedom: $T\sim t_{\nu}$.
:::




These two theorems are usually used together. Let $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ i.i.d. Then 

- $\bar X\sim N(\mu,\sigma^2/n)$;
- $U=\frac{(n-1)s^2}{\sigma^2}\sim \chi^2_{n-1}$.

Therefore 

- $Z=\frac{\bar X-\mu}{\sigma/\sqrt n}\sim N(0,1)$;
- $Z$ and $U$ are indepedent.

So
$$
T=\frac{Z}{\sqrt{U/\nu}}=\frac{\frac1{\sigma/\sqrt n}(\bar X-\mu)}{\sqrt{\frac{(n-1)s^2}{\sigma^2}/(n-1)}}=\frac{\bar X-\mu}{s/\sqrt n}\sim t_{n-1}.
$$
 
In other words, if we standardize the sample mean using the sample standard deviation, we obtain a statistic that follows a t-distribution. This result is mainly used in the t-test.



::: {.callout-note}
The normal distribution and the t-distribution are both bell-shaped and symmetric. When the sample size is large (typically \(n > 30\)), the t-distribution becomes very similar to the standard normal distribution, so the normal approximation is usually acceptable. When the sample size is small, however, the t-distribution has noticeably heavier tails, and it is better to use the t-distribution directly for inference.
:::