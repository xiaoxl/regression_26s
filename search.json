[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression 26 Spring",
    "section": "",
    "text": "Preface\nThese are notes for STAT 3113 Regression 2026 Spring at Arkansas Tech University. If you have any questions please contact me through email.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Regression 26 Spring",
    "section": "References",
    "text": "References\n\n\n[1] Lindenmayer, D.,\nViggers, K., Cunningham, R. and Donnelly, C. (1995). Morphological variation among\npopulations of the mountain brushtail possum, trichosurus-caninus ogilby\n(phalangeridae, marsupialia). Australian Journal of Zoology\n43 449‚Äì58.\n\n\n[2] Wickham, H.\n(2014). Tidy data.\nJournal of Statistical Software 59.\n\n\n[3] Hogg, R. V.,\nMcKean, J. W. and Craig, A. T. (2020). Introduction to\nmathematical statistics. Pearson, Harlow, Essex, United\nKingdom.\n\n\n[4] Mendenhall, W.\nand Sincich, T. (2020). A second\ncourse in statistics: Regression analyisis. Pearson Education, Inc,\n[Hoboken, New Jersey].\n\n\n[5] Healy, K.\n(2025). Gssrdoc: General\nsocial survey documentation for use in r.\n\n\n[6] Maindonald, J.\nH. and Braun, W. J. (2011). Data\nanalysis and graphics using r. An example-based approach. Cambridge\nUniversity Press, Cambridge.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/1/basics.html",
    "href": "contents/1/basics.html",
    "title": "1¬† Basics",
    "section": "",
    "text": "1.1 Quick overview\nGiven a set of observations of variables, we would like to understand the relationships among these variables.\nThe standard procedure for developing a statistical model is as follows:\nThere are two primary goals and related ways to assess the model:\nBoth Statistical Learning and specific methods like Regression Analysis may use both assessment strategies, but the emphasis differs. In this course, the focus is on the principles of modeling and statistical inference, with prediction discussed but not treated as the primary objective.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#quick-overview",
    "href": "contents/1/basics.html#quick-overview",
    "title": "1¬† Basics",
    "section": "",
    "text": "Proposing a model informed by exploratory data analysis and relevant domain knowledge.\nEstimating the parameters of the proposed model using the available data.\nAssessing the adequacy and quality of the fitted model.\nRevising the model based on the assessment results.\n\n\n\nPrediction (Generalization): We evaluate the fitted model on unseen data, using metrics like MSE or classification error. This assesses the model‚Äôs generalization ability, which is the primary focus of subjects like Statistical Learning or Machine Learning.\nExplanation (Inference): We use measures such as \\(R^2\\) to assess the goodness-of-fit of the fitted model, and we use confidence intervals and p-values to perform statistical inference about population parameters. Although these calculations are based on the observed sample, they are interpreted as statements about the population. This approach focuses on understanding relationships among variables and is central to traditional Statistical Modeling, including Regression Analysis.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#population-vs-sample",
    "href": "contents/1/basics.html#population-vs-sample",
    "title": "1¬† Basics",
    "section": "1.2 Population vs Sample",
    "text": "1.2 Population vs Sample\n\nPopulation: the set of all elements of interest in a particular study.\nSample: is a subset of the population\nStatistical Inference: The process of using data collected on a sample to draw conclusions about a population is called statistical inference.\n\nA population is described by parameters (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\beta\\)). A sample produces statistics (e.g., \\(\\bar X\\), \\(s\\), \\(\\hat\\beta\\)), which vary across repeated samples. Statistical inference uses the sampling distribution of these statistics to draw conclusions about population parameters. This part will be revisited later in Chapter 3.\n\nExample 1.1 (gssr) ¬†\n\n\nClick to expand.\n\nAccording to a previous national media report, the average age of U.S. adults who watch national TV news is 60 years. A data analyst hypothesizes that the average age of frequent TV-news watchers is greater than 60. To test this, she uses the General Social Survey (GSS), accessible via the gssr R package [1], to draw a sample of 500 respondents and record their ages.\n\nDescribe the population: All U.S. adults who watch TV news several times a week or more often, as defined by the GSS survey question on TV-news frequency.\nDescribe the sample: A random sample of 500 GSS respondents who meet the above criterion and have valid age data.\nSample size: \\(n = 500\\)\nDescribe the statistical inference: We want to determine whether the mean age of frequent TV-news watchers is greater than 60 based on the sampled data.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#data-structure",
    "href": "contents/1/basics.html#data-structure",
    "title": "1¬† Basics",
    "section": "1.3 Data structure",
    "text": "1.3 Data structure\nA dataset is a collection of values. Values are organized in two ways. Every value belongs to a variable and an observation.\n\nDefinition 1.1 (Variables and Observations [2]) ¬†\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units.\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\n\n\n\nExample 1.2 (possum) ¬†\n\n\nClick to expand.\n\nResearchers collected body measurements for bushtail possums in Eastern Australia. They trapped 104 possums and recorded location where possum was trapped, age, gender, head length, skull width, total length, tail length, etc. for each possum. For more details see Section A.1.\n\nlibrary(DAAG)\ndata(possum)\nhead(possum)\n##     case site Pop sex age hdlngth skullw totlngth taill footlgth earconch  eye\n## C3     1    1 Vic   m   8    94.1   60.4     89.0  36.0     74.5     54.5 15.2\n## C5     2    1 Vic   f   6    92.5   57.6     91.5  36.5     72.5     51.2 16.0\n## C10    3    1 Vic   f   6    94.0   60.0     95.5  39.0     75.4     51.9 15.5\n## C15    4    1 Vic   f   6    93.2   57.1     92.0  38.0     76.1     52.2 15.2\n## C23    5    1 Vic   f   2    91.5   56.3     85.5  36.0     71.0     53.2 15.1\n## C24    6    1 Vic   f   1    93.1   54.8     90.5  35.5     73.2     53.6 14.2\n##     chest belly\n## C3   28.0    36\n## C5   28.5    33\n## C10  30.0    34\n## C15  28.0    34\n## C23  28.5    33\n## C24  30.0    32\n\nThe dimension of the dataset is\n\ndim(possum)\n## [1] 104  14\n\nIn this example, there are 14 variables, and 104 observations. Each column is a variable. In R you can use the column name to get access to the variable.\n\npossum$hdlngth\n##   [1]  94.1  92.5  94.0  93.2  91.5  93.1  95.3  94.8  93.4  91.8  93.3  94.9\n##  [13]  95.1  95.4  92.9  91.6  94.7  93.5  94.4  94.8  95.9  96.3  92.5  94.4\n##  [25]  95.8  96.0  90.5  93.8  92.8  92.1  92.8  94.3  91.4  90.6  94.4  93.3\n##  [37]  89.3  92.4  84.7  91.0  88.4  85.3  90.0  85.1  90.7  91.4  90.1  98.6\n##  [49]  95.4  91.6  95.6  97.6  93.1  96.9 103.1  99.9  95.1  94.5 102.5  91.3\n##  [61]  95.7  91.3  92.0  96.9  93.5  90.4  93.3  94.1  98.0  91.9  92.8  85.9\n##  [73]  82.5  88.7  93.8  92.4  93.6  86.5  85.8  86.7  90.6  86.0  90.0  88.4\n##  [85]  89.5  88.2  98.5  89.6  97.7  92.6  97.8  90.7  89.2  91.8  91.6  94.8\n##  [97]  91.0  93.2  93.3  89.5  88.6  92.4  91.5  93.6\n\n\n\n\nDefinition 1.2 (Quantitative and Qualitative [3]) ¬†\n\nQuantitative data are observations measured on a naturally occurring numerical scale. It is also called numerical data.\nQualitative data are nonnumerical data that can only be classified into one of a group of categories. It is also called categorical data.\n\n\nIn regression, qualitative variables must be encoded using indicator (dummy) variables.\n\nDefinition 1.3 (Discrete and Continuous [3]) ¬†\n\nDiscrete random variable: A random variable that assumes either a finite number of values or an infinite sequence of values such as 0, 1, 2‚Ä¶\nContinuous random variable: A random variable that may assume any numerical value in an interval or collection of intervals.\n\n\n\n\n\n\n\ngraph TD\n    A[All Variables] --&gt; B[Numerical]\n    A --&gt; C[Categorical]\n    B --&gt; D[Continuous]\n    B --&gt; E[Discrete]\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom variables\n\n\n\nA variable in a dataset can be modeled by a random variable. The probability density function / probability mass function of the random variable can describe the distribution of all possible values of the variable in a dataset.\nMaking a measurement is equivalent to taking a sample from the corresponding random variable. This will be discussed in detail in Chapter 3.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#data-visualization",
    "href": "contents/1/basics.html#data-visualization",
    "title": "1¬† Basics",
    "section": "1.4 Data Visualization",
    "text": "1.4 Data Visualization\n\n1.4.1 Qualitative (categorical) data\nUsually the most important is the class relative frequency: \\[\n\\text{class relative frequency}=\\frac{\\text{class frequency}}{n}.\n\\]\nTo display it, we could use table, bar chart or pie chart.\n\nExample 1.3 (possum) ¬†\n\n\nClick to expand.\n\nHere we still consider the possum dataset.\n\nlibrary(DAAG)\ndata(possum)\nhead(possum)\n##     case site Pop sex age hdlngth skullw totlngth taill footlgth earconch  eye\n## C3     1    1 Vic   m   8    94.1   60.4     89.0  36.0     74.5     54.5 15.2\n## C5     2    1 Vic   f   6    92.5   57.6     91.5  36.5     72.5     51.2 16.0\n## C10    3    1 Vic   f   6    94.0   60.0     95.5  39.0     75.4     51.9 15.5\n## C15    4    1 Vic   f   6    93.2   57.1     92.0  38.0     76.1     52.2 15.2\n## C23    5    1 Vic   f   2    91.5   56.3     85.5  36.0     71.0     53.2 15.1\n## C24    6    1 Vic   f   1    93.1   54.8     90.5  35.5     73.2     53.6 14.2\n##     chest belly\n## C3   28.0    36\n## C5   28.5    33\n## C10  30.0    34\n## C15  28.0    34\n## C23  28.5    33\n## C24  30.0    32\n\nThe variable Pop (location where possum was trapped) is qualitative. We could use the following ways to display it. We first compute the frequency table of the variable Pop:\n\ntable(possum$Pop)\n## \n##   Vic other \n##    46    58\n\nor relative frequency table:\n\ntable(possum$Pop)/length(possum$Pop)\n## \n##       Vic     other \n## 0.4423077 0.5576923\n\nThen we could draw the barplot of this variable:\n\nbarplot(table(possum$Pop))\n\n\n\n\n\n\n\n\nand the pie plot:\n\npie(table(possum$Pop))\n\n\n\n\n\n\n\n\nNote that table is handling the statistics, while barplot and pie draw on top of the result from table.\n\n\n\n\n1.4.2 Quantitative (numerical) data\nFor quantitative data, we would like to see the histogram, as well as computing some statistics: min, max, quartiles, median and mean.\n\nHistogram is an approximation of the distribution. In other words, we split the range into small segments (called bins), and count the frequency or relative frequency of data falling into these bins.\nBox plot consists of a box, two lines and possibly some points:\n\nThe box in the box plot extends from the lower quartile to the upper quartile. The difference between the upper quartile and the lower quartile is called the inter-quartile range (IQR).\nThe lines, known as whiskers, extend to one and a half times the interquartile range, but they stop at the most extreme data points that fall within this range.\nThe points, considered as outliers, are those which are not covered by the box and the lines.\n\n\nHistograms and boxplots help check normality and outliers before fitting regression. These summaries approximate the underlying distribution of the variable. In regression, we later apply similar tools to the residuals to check model assumptions.\n\nExample 1.4 (possum) ¬†\n\n\nClick to expand.\n\nWe still consider the possum dataset. We could use summary to compute the major statistics.\n\nlibrary(DAAG)\ndata(possum)\nsummary(possum)\n##       case             site          Pop     sex         age       \n##  Min.   :  1.00   Min.   :1.000   Vic  :46   f:43   Min.   :1.000  \n##  1st Qu.: 26.75   1st Qu.:1.000   other:58   m:61   1st Qu.:2.250  \n##  Median : 52.50   Median :3.000                     Median :3.000  \n##  Mean   : 52.50   Mean   :3.625                     Mean   :3.833  \n##  3rd Qu.: 78.25   3rd Qu.:6.000                     3rd Qu.:5.000  \n##  Max.   :104.00   Max.   :7.000                     Max.   :9.000  \n##                                                     NA's   :2      \n##     hdlngth           skullw         totlngth         taill      \n##  Min.   : 82.50   Min.   :50.00   Min.   :75.00   Min.   :32.00  \n##  1st Qu.: 90.67   1st Qu.:54.98   1st Qu.:84.00   1st Qu.:35.88  \n##  Median : 92.80   Median :56.35   Median :88.00   Median :37.00  \n##  Mean   : 92.60   Mean   :56.88   Mean   :87.09   Mean   :37.01  \n##  3rd Qu.: 94.72   3rd Qu.:58.10   3rd Qu.:90.00   3rd Qu.:38.00  \n##  Max.   :103.10   Max.   :68.60   Max.   :96.50   Max.   :43.00  \n##                                                                  \n##     footlgth        earconch          eye            chest          belly      \n##  Min.   :60.30   Min.   :40.30   Min.   :12.80   Min.   :22.0   Min.   :25.00  \n##  1st Qu.:64.60   1st Qu.:44.80   1st Qu.:14.40   1st Qu.:25.5   1st Qu.:31.00  \n##  Median :68.00   Median :46.80   Median :14.90   Median :27.0   Median :32.50  \n##  Mean   :68.46   Mean   :48.13   Mean   :15.05   Mean   :27.0   Mean   :32.59  \n##  3rd Qu.:72.50   3rd Qu.:52.00   3rd Qu.:15.72   3rd Qu.:28.0   3rd Qu.:34.12  \n##  Max.   :77.90   Max.   :56.20   Max.   :17.80   Max.   :32.0   Max.   :40.00  \n##  NA's   :1\n\nThe variable hdlngth (head length) is quantitative. We first show the histogram. We could use breaks to control the number of bins. Note that the function hist does not only draw the histogram; it also provides many useful pieces of information.\n\nres &lt;- hist(possum$hdlngth, breaks=10)\n\n\n\n\n\n\n\nres\n## $breaks\n##  [1]  82  84  86  88  90  92  94  96  98 100 102 104\n## \n## $counts\n##  [1]  1  6  2 12 22 27 22  7  3  0  2\n## \n## $density\n##  [1] 0.004807692 0.028846154 0.009615385 0.057692308 0.105769231 0.129807692\n##  [7] 0.105769231 0.033653846 0.014423077 0.000000000 0.009615385\n## \n## $mids\n##  [1]  83  85  87  89  91  93  95  97  99 101 103\n## \n## $xname\n## [1] \"possum$hdlngth\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\nThen we could show the box plot. You may compare the lines in the plot with the summary table and the histogram above.\n\nboxplot(possum$hdlngth)\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.3 Relations among multiple variables\nWe could show the relation between two variables in a scatterplot. Scatterplots help us assess linearity, detect outliers, and decide whether transformation is needed.\n\nExample 1.5 (possum) ¬†\n\n\nClick to expand.\n\nWhen both variables are numerical and continuous:\n\nplot(possum$hdlngth, possum$skullw)\n\n\n\n\n\n\n\n\nWhen one variable is categorical:\n\nplot(as.factor(possum$Pop), possum$skullw)\n\n\n\n\n\n\n\n\nNote that in this case, the categorical data has to be a factor. And once it is cast into a factor, the plot is multiple box plots for each category. This type of grouped boxplot corresponds to comparing group means and is directly related to one-way ANOVA and regression with dummy variables. It will be discussed in later sections.\nWe can see pair plots for each pair of variables. Pair plots are very important because they help reveal relationships among predictors. Note that before creating the plot, we have to cast pop and sex into factors.\n\npossum$Pop &lt;- as.factor(possum$Pop)\npossum$sex &lt;- as.factor(possum$sex)\npairs(possum)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Healy, K. (2025). Gssrdoc: General social survey documentation for use in r.\n\n\n[2] Wickham, H. (2014). Tidy data. Journal of Statistical Software 59.\n\n\n[3] Mendenhall, W. and Sincich, T. (2020). A second course in statistics: Regression analyisis. Pearson Education, Inc, [Hoboken, New Jersey].",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html",
    "href": "contents/1/prob.html",
    "title": "2¬† Probability",
    "section": "",
    "text": "2.1 Notations",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#notations",
    "href": "contents/1/prob.html#notations",
    "title": "2¬† Probability",
    "section": "",
    "text": "\\(Y\\): a random variable (captial letters)\n\\(y\\): a sample of \\(Y\\)\n\\(\\Pr\\qty(Y\\in A\\mid\\theta)\\): the probability of \\(Y\\) being in \\(A\\)\n\\(p(y\\mid\\theta)=\\Pr\\qty(Y=y\\mid\\theta)\\): the probability mass function (discrete case)\n\\(f(y\\mid\\theta)=\\displaystyle\\dv{y}\\Pr\\qty(Y\\leq y\\mid\\theta)\\): the probability density function (continuous case)\n\\(\\Exp\\qty(Y)\\): the expectation of \\(Y\\)\n\\(\\Var\\qty(Y)\\): the variance of \\(Y\\)",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#random-variables",
    "href": "contents/1/prob.html#random-variables",
    "title": "2¬† Probability",
    "section": "2.2 Random variables",
    "text": "2.2 Random variables\n\nDefinition 2.1 (Expectation) \\[\n\\Exp\\mqty[u(X)] = \\int_{-\\infty}^{\\infty}u(x)f(x)\\dl3x.\n\\]\n\n\nDefinition 2.2 ¬†\n\n\\(\\mu=\\Exp(X)\\) is called the mean value of \\(X\\).\n\\(\\sigma^2=\\Var(X)=\\Exp\\mqty[(X-\\mu)^2]\\) is called the variance of \\(X\\).\n\\(M_X(t)=\\Exp\\mqty[\\me^{tX}]\\) is called the moment generating function of \\(X\\).\n\n\n\nProposition 2.1 ¬†\n\n\\(\\Exp\\mqty[ag(X)+bh(X)]=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)]\\).\n\\(\\Var\\mqty[X]=\\Exp\\mqty[(X-\\mu)^2]=\\Exp(X^2)-\\mu^2\\).\nIf \\(X\\) and \\(Y\\) are independent, \\(\\Var\\mqty[aX+bY]=a^2\\Var(X)+b^2\\Var(Y)\\).\n\n\n\nClick for proof.\n\n\\[\n\\begin{split}\n\\Exp\\mqty[ag(X)+bh(X)]&=\\int_{-\\infty}^{\\infty}\\mqty[ag(x)+bh(x)]f(x)\\dl3x\\\\\n                 &=a\\int_{-\\infty}^{\\infty}g(x)f(x)\\dl3x+b\\int_{-\\infty}^{\\infty}h(x)f(x)\\dl3x\\\\\n                 &=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)].\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp\\mqty[(X-\\mu)^2]&=\\Exp\\mqty[\\qty(X^2-2\\mu X+\\mu^2)]=\\Exp(X^2)-2\\mu\\Exp(X)+\\Exp(\\mu^2)\\\\\n&=\\Exp(X^2)-2\\mu\\mu+\\mu^2=\\Exp(X^2)-\\mu^2.\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Var\\mqty[aX]&=\\Exp(a^2X^2)-a^2\\mu^2=a^2\\qty(\\Exp(X^2)-\\mu^2)=a^2\\Var(X),\\\\\n\\Var\\mqty[X+Y]&=\\Exp((X+Y)^2)-(\\Exp(X+Y))^2\\\\\n&=\\Exp(X^2)+\\Exp(Y^2)+2\\Exp(XY)-\\Exp(X)^2-\\Exp(Y)^2-2\\Exp(X)\\Exp(Y)\\\\\n&=\\Var(X)+\\Var(Y)+2(E(XY)-E(X)E(Y))\\\\\n&=\\Var(X)+\\Var(Y),\\\\\n\\Var\\mqty[aX+bY]&=a^2\\Var(X)+b^2\\Var(Y).\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nAssume \\(X_1,\\ldots, X_n\\) i.i.d. with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then \\(\\Var(\\frac1n\\sum X_i)=\\sigma^2/n\\). This implies that the more samples you pick, the smaller the variance is. This explains why, when possible, we want a large sample size. Note that we don‚Äôt specify any concrete distribution in this remark. This is related to estimation, which will be discussed in detail later.\n\n\n\n2.2.1 R code\nR has built-in random variables with different distributions. The naming convention is a prefix d-, p-, q- and r- together with the name of distribution.\n\nd-: density function of the given distribution;\np-: cumulative density function of the given distribution;\nq-: quantile function of the given distribution (which is the inverse of p- function);\nr-: random sampling from the given distribution.\n\n\nExample 2.1 (Normal distribution) ¬†\n\n\nClick to expand.\n\n\nx &lt;- seq(-4, 4, length=100)\ny &lt;- dnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n\n\n\n\n\n\n\n\n\nx &lt;- seq(-4, 4, length=100)\ny &lt;- pnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n\n\n\n\n\n\n\n\n\nqnorm(0.025)\n## [1] -1.959964\nqnorm(0.5)\n## [1] 0\nqnorm(0.975)\n## [1] 1.959964\n\n\nrnorm(10)\n##  [1]  0.58816887 -0.19119097 -1.04581242 -1.06323842 -1.44962628  1.39717300\n##  [7] -0.60509323 -0.20214579  0.64415165 -0.06536277",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#some-important-distributions",
    "href": "contents/1/prob.html#some-important-distributions",
    "title": "2¬† Probability",
    "section": "2.3 Some important distributions",
    "text": "2.3 Some important distributions\n\n2.3.1 Normal Distribution\n\nTheorem 2.1 (Normal Sample Mean‚ÄìVariance) Let \\(X_1,\\ldots,X_n\\sim N(\\mu,\\sigma^2)\\) i.i.d. Define\n\nSample mean: \\(\\bar X=\\frac1n\\sum_{i=1}^nX_i\\)\nSample variance: \\(s^2=\\frac1{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2\\)\n\nThen\n\n\\(\\bar X\\sim N(\\mu,\\frac{\\sigma^2}n)\\)\n\\(\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)\n\\(\\bar X\\) and \\(s^2\\) are independent.\n\n\nThe complete proof is lengthy and out of scope of the course. Please refer to [1] for details.\n\n\n2.3.2 Student‚Äôs t-Distribution\n\nTheorem 2.2 (Student‚Äôs t-Distribution [1]) Let\n\n\\(Z\\sim N(0,1)\\) be a standard normal random variable\n\\(U\\sim \\chi_{\\nu}^2\\) be a chi-square random variable with \\(\\nu\\) degrees of freedom.\n\\(Z\\) and \\(U\\) are independent.\n\nThen \\[\nT=\\frac{Z}{\\sqrt{U/\\nu}}\n\\] has a Student‚Äôs t-distribution with \\(\\nu\\) degrees of freedom: \\(T\\sim t_{\\nu}\\).\n\nThese two theorems are usually used together. Let \\(X_1,\\ldots,X_n\\sim N(\\mu,\\sigma^2)\\) i.i.d. Then\n\n\\(\\bar X\\sim N(\\mu,\\sigma^2/n)\\);\n\\(U=\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\).\n\nTherefore\n\n\\(Z=\\frac{\\bar X-\\mu}{\\sigma/\\sqrt n}\\sim N(0,1)\\);\n\\(Z\\) and \\(U\\) are indepedent.\n\nSo \\[\nT=\\frac{Z}{\\sqrt{U/\\nu}}=\\frac{\\frac1{\\sigma/\\sqrt n}(\\bar X-\\mu)}{\\sqrt{\\frac{(n-1)s^2}{\\sigma^2}/(n-1)}}=\\frac{\\bar X-\\mu}{s/\\sqrt n}\\sim t_{n-1}.\n\\]\nIn other words, if we standardize the sample mean using the sample standard deviation, we obtain a statistic that follows a t-distribution. This result is mainly used in the t-test.\n\n\n\n\n\n\nNote\n\n\n\nThe normal distribution and the t-distribution are both bell-shaped and symmetric. When the sample size is large (typically (n &gt; 30)), the t-distribution becomes very similar to the standard normal distribution, so the normal approximation is usually acceptable. When the sample size is small, however, the t-distribution has noticeably heavier tails, and it is better to use the t-distribution directly for inference.\n\n\n\n\n\n\n[1] Hogg, R. V., McKean, J. W. and Craig, A. T. (2020). Introduction to mathematical statistics. Pearson, Harlow, Essex, United Kingdom.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html",
    "href": "contents/1/inferences.html",
    "title": "3¬† Inferences",
    "section": "",
    "text": "3.1 Inferential statistics",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html#inferential-statistics",
    "href": "contents/1/inferences.html#inferential-statistics",
    "title": "3¬† Inferences",
    "section": "",
    "text": "Definition 3.1 (Population and sample [1]) ¬†\n\nA population data set is a collection (or set) of data measured on all experimental units of interest to you.\nA sample is a subset of data selected from a population.\nA random sample of \\(n\\) experimental units is one selected from the population in such a way that every different sample of size \\(n\\) has an equal probability of selection.\n\n\n\nDefinition 3.2 (Statistical inference [1]) ¬†\n\nA statistical inference is an estimate, prediction, or some other generatlization about a population based on information contianed in a sample.\nA measure of reliability is a statement about the degree of uncertainty associated with a statistical inference.\n\n\n\n\n\n\n\n\nNoteInferential statistics\n\n\n\n\nIdentify population\nIdentify variable(s)\nCollect sample data\nInference about population based on sample\nMeasure of reliability for inference",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html#estimators",
    "href": "contents/1/inferences.html#estimators",
    "title": "3¬† Inferences",
    "section": "3.2 Estimators",
    "text": "3.2 Estimators\nThis section is based on [2, Chapter 4].\n\n3.2.1 Sampling\nConsider a random variable \\(X\\) with an unknown distribution. Our information about the distribution of \\(X\\) comes from a sample on \\(X\\): \\(\\qty{X_1,\\ldots,X_n}\\).\n\nThe sample ovservations \\(\\qty{X_1,\\ldots,X_n}\\) have the same distribution as \\(X\\).\n\\(n\\) denotes the sample size.\nWhen the sample is actually drawn, we use \\(x_1,\\ldots,x_n\\) as the realizations of the sample.\n\n\nDefinition 3.3 (Random sample) If the random variables \\(X_1,\\ldots, X_n\\) are i.i.d, then these random variable constitute a random sample of size \\(n\\) from the common distribution.\n\n\nDefinition 3.4 (Statistics) Let \\(X_1,\\ldots,X_n\\) denote a sample on a random variable \\(X\\). Let \\(T=T(X_1,\\ldots,X_n)\\) be a function of the sample. \\(T\\) is called a statistic. Once a sample is drawn, \\(t=T(x_1,\\ldots,x_n)\\) is called the realization of \\(T\\).\n\n\nDefinition 3.5 (Sampling distribution) ¬†\n\nThe distribution of \\(T\\) is called the sampling distribution.\nThe standard deviation of the sampling distribution is called the standard error of estimate.\n\n\n\nTheorem 3.1 (The Central Limit Theorem) For large sample sizes, the sample mean \\(\\bar{X}\\) from a population with mean \\(\\mu\\) and a standard deviation \\(\\sigma\\) has a sampling distribution that is approximately normal, regardless of the probability distribution of the sampled population.\n\n\n\n3.2.2 Point estimation\nAssume that the distribution of \\(X\\) is known down to an unknown parameter \\(\\theta\\) where \\(\\theta\\) can be a vector. Then the pdf of \\(X\\) can be written as \\(f(x;\\theta)\\). In this case we might find some statistic \\(T\\) to estimate \\(\\theta\\). This is called a point estimator of \\(\\theta\\). A realization \\(t\\) is called an estimate of \\(\\theta\\).\n\nDefinition 3.6 (Unbiasedness) Let \\(X_1,\\ldots,X_n\\) is a sample on a random varaible \\(X\\) with pdf \\(f(x;\\theta)\\). Let \\(T\\) be a statistic. We say that \\(T\\) is an unbiased estimator of \\(\\theta\\) if \\(E(T)=\\theta\\).\n\nThe typical example of estimators are sample mean and sample variance. Let \\(X\\) be a random variable, with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Consider a sample \\(\\set{X_i}\\) of size \\(n\\). By definition all \\(X_i\\)‚Äôs are i.i.d. Therefore \\(\\Exp\\qty(X_i)=\\mu\\), and \\(\\Var\\qty(X_i)=\\sigma^2\\) for any \\(i=1,\\ldots, n\\).\n\nTheorem 3.2 The following are the unbiased estimators of \\(\\mu\\) and \\(\\sigma^2\\) of \\(X\\).\n\n\\(\\bar{X}=\\dfrac1n\\sum_{i=1}^nX_i\\) is called the sample mean of the samples.\n\\(s^2=\\dfrac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2\\) is called the sample variance of the samples.\n\nThey are the unbiased estimators of \\(\\mu\\) and \\(\\sigma^2\\), respectively. \\[\n\\Exp(\\bar X)=\\mu,\\quad \\Exp(s^2)=\\sigma^2.\n\\]\n\n\nClick for proof.\n\n\\[\n\\begin{aligned}\n\\Exp\\qty(\\bar{X})&=\\Exp\\qty(\\frac1n\\sum_{i=1}^nX_i)=\\frac1n\\sum_{i=1}^n\\Exp\\qty(X_i)=\\frac1n\\sum_{i=1}^n\\mu=\\mu,\\\\\n\\Exp\\qty(s^2)&=\\frac{1}{n-1}\\Exp\\qty[\\sum_{i=1}^n(X_i-\\bar{X})^2]=\\frac{1}{n-1}\\sum_{i=1}^n\\Exp\\mqty[\\qty(X_i-\\bar{X})^2]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\Var\\qty(X_i-\\bar{X})+\\qty(\\Exp\\qty(X_i-\\bar{X}))^2)\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\Var\\qty(\\frac{n-1}{n}X_i-\\frac1nX_1-\\ldots-\\frac1nX_n)+\\qty(\\Exp\\qty(X_i)-\\Exp\\qty(\\bar{X}))^2)\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\frac{(n-1)^2}{n^2}\\Var\\qty(X_i)+\\frac1{n^2}\\Var\\qty(X_1)+\\ldots+\\frac1{n^2}\\Var\\qty(X_n))\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\frac{(n-1)^2}{n^2}\\sigma^2+\\frac1{n^2}\\sigma^2+\\ldots+\\frac1{n^2}\\sigma^2)\\\\\n&=\\frac{n}{n-1}\\frac{(n-1)^2+n-1}{n^2}\\sigma^2=\\sigma^2.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease pay attention to the denominator of the sample variance. The \\(n-1\\) is due to the degree of freedom: all \\(X_i\\)‚Äôs and \\(\\bar{X}\\) are not independent to each other.\n\n\n\n\n3.2.3 Confidence intervals\n\nDefinition 3.7 (Confidence interval) Consider a sample of \\(X\\). Fix a number \\(0&lt;\\alpha&lt;1\\). Let \\(L\\) and \\(U\\) be two statistics. We say the interval \\((L,U)\\) is a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\theta\\) if\n\\[\n1-\\alpha=\\Pr[\\theta\\in(L,U)].\n\\]\n\n\nTheorem 3.3 (Large-Sample \\(100(1-\\alpha)\\%\\) Confidence interval) \\[\nL,U=\\bar{X}\\pm z_{\\alpha/2}\\qty(\\frac{s}{\\sqrt{n}}),\n\\] where \\(z_{\\alpha/2}=1.96\\) if \\(\\alpha=5\\%\\).\n\n\nFor any \\(n\\), if \\(X_i\\sim \\mathcal N(\\mu, \\sigma^2)\\), \\(T_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\) has a Student‚Äôs \\(t\\)-distribution of degree of freedom \\(n-1\\).\nWhen \\(n\\) is big enough, for any distribution \\(X_i\\), \\(Z_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\) is approximately \\(\\mathcal N(0,1)\\).\nStudent‚Äôs \\(t\\)-distribution of degree of freedom \\(n-1\\) is approaching \\(\\mathcal N(0,1)\\) when \\(n\\) is increasing. When \\(n=30\\) they are very close to each other. Therefore in many cases Statisticians require sample size \\(\\geq30\\).\nFor large sample or small sample, the coefficients to compute confidence intervals are \\(z_{\\alpha/2}\\) or \\(t_{\\alpha/2}\\). These two numbers come from normal distribution or Student‚Äôs \\(t\\)-distribution.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html#hypothesis-test",
    "href": "contents/1/inferences.html#hypothesis-test",
    "title": "3¬† Inferences",
    "section": "3.3 Hypothesis test",
    "text": "3.3 Hypothesis test\nElements of a Statistical Test of Hypothesis\n\nNull Hypothesis \\(ùêª_0\\)\nAlternative Hypothesis \\(ùêª_ùëé\\)\nTest Statistic\nLevel of significance \\(\\alpha\\)\nRejection Region\n\\(ùëÉ\\)-Value\nConclusion\n\nGiven some data, we would like to know whether these data are ‚Äúexotic‚Äù enough‚Äìunder the assumption that the null hypothesis \\(H_0\\) is true‚Äìto justify rejecting \\(H_0\\). In other words, we compute the probability of obtaining a test statistic at least as extreme as the observed value, assuming \\(H_0\\) is true. This probability is called the p-value. Once the p-value is smaller than the chosen level of significance \\(\\alpha\\), we reject \\(H_0\\).\nConsider a test statistic \\(T(X)\\) and we observe \\(T(X)=t_{\\text{obs}}\\). Then\n\\[\np\\text{-value}=\\Pr(T(X)\\geq t_{\\text{obs}}\\mid H_0 \\text{ is true})\n\\] is the p-value for a right-tailed test. The key idea in constructing a hypohesis test is to choose a test statistic and a rejection region so that\n\\[\n\\Pr(\\text{the statistic falls in the rejection region}\\mid H_0\\text{ is true})=\\alpha.\n\\] This ensures the test has expected Type I error rate \\(\\alpha\\).\n\n\n\n\n\n\nNoteType I error vs Type II error\n\n\n\n\nType I error: rejecting \\(H_0\\) when \\(H_0\\) is actually true.\nType II error: failing to reject \\(H_0\\) when \\(H_0\\) is actually false.\n\nHypothesis test is designed to control Type I error rate (which is to reduce false positive rate, and which is to increase precision), since the significance level \\(\\alpha\\) is the probability of Type I errors.\n\\[\n\\alpha=\\Pr(\\text{reject }H_0\\mid H_0\\text{ is true}).\n\\]\nWhen using Hypothesis test, the scenario is usually that people capture some signals in order to prove an effect happens. The null hypothesis (\\(H_0\\)) is assumed to be the default case, and they want to make sure that once the signal is captured, the effect happens. In this case it is ok to miss some events that happens without the signal. In other words, people prioritize not making a false claim than missing an opportunity.\nWe could balance Type I and Type II errors by controlling \\(\\alpha\\).\n\nReduing \\(\\alpha\\) will make the test less likely to make Type I errors but increase the likelihood of Type II errors.\nIncreasing sample size reduces probability of making both types of errors, which can improve the test‚Äôs reliability.\n\n\n\n\n3.3.1 t-test\nA t-test is used to test a hypothesis about a population mean when the population standard deviation is unknown and the sample size is small or moderate. In our case, we consider the standard one-sample t-test: given a set of random observations, we want to determine whether the population mean of the underlying random variable is equal to 0 or not.\nAssume that the given values are \\(\\{x_1,x_2,\\ldots,x_n\\}\\), and the underlying random variable is \\(X\\sim N(\\mu, \\sigma^2)\\). The hypotheses are\n\n\\(H_0\\): \\(\\mu=0\\).\n\\(H_a\\): \\(\\mu\\neq0\\).\n\nThe values can be treated as realizations of i.i.d random variables \\(X_i\\)‚Äôs. We have the following statistics:\n\nSample mean: \\(\\bar{X}=\\frac1n\\sum_{i=1}^nX_i\\).\nSample standard deviation: \\(s=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2}\\)\nSample size: \\(n\\).\n\nThese statistics have sampling distributions that can be described exactly under the normality assumption.\nBased on the discussion in Section 2.3.2, we have that\n\n\\(Z=\\frac{\\bar X-\\mu}{\\sigma/\\sqrt n}\\sim N(0,1)\\)\n\\(U=\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)\n\\(Z\\) and \\(U\\) are independent, and the degree of freedom of \\(U\\) is \\(\\nu = n-1\\),\n\\(t=\\frac{Z}{\\sqrt{U/(n-1)}}\\sim t_{n-1}\\).\n\nSince the alternative hypothesis is \\(\\mu\\neq0\\), we need to consider both tails, corresponding to \\(\\mu&gt;0\\) and \\(\\mu&lt;0\\). Therefore the boundary of the rejection region is chosen so that each tail has probability \\(\\alpha/2\\). In other words, the critical value \\(t_{1-\\alpha/2,n-1}\\) satisfies, under \\(H_0\\), \\[\n\\Pr(|t|&gt;t_{1-\\alpha/2,n-1})=\\alpha.\n\\] This critical value \\(t_{1-\\alpha/2,n-1}\\) is usually found from a t-distribution table or via the inverse c.d.f. of the t-distribution.\n\n\n\n\n[1] Mendenhall, W. and Sincich, T. (2020). A second course in statistics: Regression analyisis. Pearson Education, Inc, [Hoboken, New Jersey].\n\n\n[2] Hogg, R. V., McKean, J. W. and Craig, A. T. (2020). Introduction to mathematical statistics. Pearson, Harlow, Essex, United Kingdom.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html",
    "href": "contents/2/slr.html",
    "title": "5¬† slr",
    "section": "",
    "text": "5.1 Probabilistic model for \\(y\\)\nWe want to use sample data to investigate the relationships among a group of variables, ultimately to create a model for some variable that can be used to predict its value in the futre.\nLanguage:\nThe probabilistic model for \\(y\\) is \\[\ny=\\Exp(y)+\\text{Random error}.\n\\] It is called probabilistic since we can make a probability statement about the magnitude of the deviation between \\(y\\) and \\(\\Exp(y)\\).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#probabilistic-model-for-y",
    "href": "contents/2/slr.html#probabilistic-model-for-y",
    "title": "5¬† slr",
    "section": "",
    "text": "NoteGeneral Form of Probabilistic Model in Regression\n\n\n\n\\[\ny=\\Exp(y)+\\varepsilon\n\\] where\n\n\\(y\\): Dependent variable\n\\(\\Exp(y)\\): Mean value of \\(y\\)\n\\(\\varepsilon\\): Unexplainable or random error\n\nThis model suggests that y will come back to its mean evautually. This is why it is called regression model.\n\n\n\nDefinition 5.1 The variables used to predict \\(y\\) are called independent variables and are denoted by \\(x_i\\).\n\n\n\n\n\n\n\nTipRegression Modeling\n\n\n\n\nHypothesize the form of the model for \\(\\Exp(y)\\).\nCollect the sample data.\nUse the sample data to estimate unknown parameters in the model.\nSpecify the probability distribution of \\(\\varepsilon\\), and estimate any unknown parameters of this distribution.\nStatistically check the usefulness of the model.\nCheck the validity of the assumptions on \\(\\varepsilon\\), and make model modifications if necessary.\nWhen satisfied that the model is useful, and assumptions are met, use the model to make inferences.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#slr",
    "href": "contents/2/slr.html#slr",
    "title": "5¬† slr",
    "section": "5.2 SLR",
    "text": "5.2 SLR\n\n\n\n\n\n\nNoteA First-Order Model\n\n\n\n\\[\ny=\\beta_0+\\beta_1x+\\varepsilon\n\\] where\n\n\\(y\\): The response variable\n\\(x\\): The independent variable (variable used as a predictor of \\(y\\))\n\\(\\Exp(y)=\\beta_0+\\beta_1x\\): Deterministic component\n\\(\\varepsilon\\): Random error component\n\\(\\beta_0\\): \\(y\\)-intercept\n\\(\\beta_1\\): Slope\n\n\n\n\n5.2.1 Fitting the model\nMLE=LSE under the assumption that e is normal\ndifferent estimators\nfirst assuming that \\(\\sigma\\) is constant.\n\nSum of squares of deviations for the \\(x\\)s: \\(SS_{xx}=\\sum(x_i-\\bar{x})^2\\)\nSum of squares of deviations for the \\(y\\)s: \\(SS_{yy}=\\sum(y_i-\\bar{y})^2\\)\nSum of cross-products: \\(SS_{xy}=\\sum(x_i-\\bar{x})(y_i-\\bar{y})\\)\n\n\nTheorem 5.1 Estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). The NLL is \\[\nNLL = -\\frac{n}{2}\\ln\\qty(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum\\qty(y_i-\\beta_0-\\beta_1x_i)^2.\n\\]\n\n\nSolution. \\[\n\\begin{split}\n\\pdv{NLL}{\\beta_0}&=\n\\end{split}\n\\]",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#estimation",
    "href": "contents/2/slr.html#estimation",
    "title": "5¬† slr",
    "section": "5.3 estimation",
    "text": "5.3 estimation\nb0, b1 are estimations\n\nb0, b1 formula\nE(b0) E(b1)\nVar(b0), Var(b1)\nrelations to y: b1, ybar independent, b0, b1 are independent\n\n\\[\n\\hat{\\beta}_1=\\frac{S_{xy}}{S_{xx}},\\quad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}.\n\\]\n\\[\n\\Var(y_i-\\bar{y})=\\Var(y_i-\\frac1n\\sum y_j)=\\Var(\\frac{n-1}{n}y_i-\\sum_{j\\neq i}\\frac1ny_j)=\\frac{(n-1)^2}{n^2}\\sigma^2+\\frac{n-1}{n^2}\\sigma^2=\\frac{n-1}{n}\\sigma^2.\n\\]\nSince \\(\\sum_i (x_i-\\bar{x})=\\sum_i (y_i-\\bar{y})=0\\), we have \\[\n\\sum_i (x_i-\\bar{x})\\bar{x}=\\sum_i (x_i-\\bar{x})\\bar{y}=\\sum_i (y_i-\\bar{y})\\bar{x}=\\sum_i (y_i-\\bar{y})\\bar{y}=0.\n\\]\n\\[\n\\begin{split}\n\\Var(S_{xy})&=\\Var\\qty[\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})]=\\Var\\qty[\\sum_i (x_i-\\bar{x})y_i-\\sum_i (x_i-\\bar{x})\\bar{y}]=\\Var\\qty[\\sum_i (x_i-\\bar{x})y_i]\\\\\n&=\\sum(x_i-\\bar{x})^2\\Var(y_i)=\\sum(x_i-\\bar{x})^2\\sigma^2=S_{xx}\\sigma^2.\n\\end{split}\n\\]\n\\[\n\\Var(\\hat{\\beta}_1)=\\frac{1}{S_{xx}^2}\\Var(S_{xy})=\\frac{1}{S_{xx}^2}\\Var\\qty[\\sum x_i(y_i-\\bar{y})]=\\frac{1}{S_{xx}^2}S_{xx}\\sigma^2=\\frac{\\sigma^2}{S_{xx}}.\n\\]\n\nTheorem 5.2 (Uncorrelation) \\(\\hat{\\beta}_0\\) and \\(\\bar y\\) are uncorrelated.\n\n\nProof. \\[\n\\begin{split}\n\\Cov(\\hat{\\beta}_1,\\bar{y})=&\\Cov\\qty(\\frac{1}{S_{xx}}\\sum_i(x_i-\\bar x)(y_i-\\bar y), \\sum\\frac{1}{n} y_j)=\\sum_{i,j}\\frac{(x_i-\\bar x)}{nS_{xx}}\\Cov(y_i,y_j)\\\\\n=&\\sum_{i}\\frac{(x_i-\\bar x)}{nS_{xx}}\\sigma^2=0.\n\\end{split}\n\\] Since \\(\\hat{\\beta}_1\\) and \\(\\bar y\\) are both normal, the uncorrelation implies that they are also indepedent.\n\n\\[\n\\begin{split}\n\\Var(\\hat{\\beta}_0)&=\\Var\\qty(\\bar y-\\hat{\\beta}_1\\bar x)=\\Var(\\bar y)+\\bar{x}^2\\Var(\\hat{\\beta}_1)=\\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{nS_{xx}}=\\qty(\\frac{1}{n}+\\frac{\\bar x^2}{nS_{xx}})\\sigma^2.\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp\\qty(y_i-\\hat{y}_i)^2&=\\Exp\\qty(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i)^2\n\\end{split}\n\\]\n\\[\n\\Exp\\qty(\\sum(y_i-\\hat{y}_i)^2)=\\Exp\\qty(\\sum(y_i-\\bar{y}_i)^2) - \\Exp(S_{xy}^2) =(n-2)\\sigma^2.\n\\]\n\\[\n\\Var(y_i-\\bar y)=\\Var\\qty(y_i-\\sum\\frac1n y_j)=\\Var\\qty(\\sum_{j\\neq i}\\qty(-\\frac1n)y_j+\\frac{n-1}n y_i)=\\sum_{j\\neq i}\\frac1{n^2}\\Var(y_j)+\\frac{(n-1)^2}{n^2}\\Var{y_i}=\\frac{n-1}{n}\\sigma^2.\n\\]\n\\[\n\\Exp\\qty(y_i-\\bar y)=\\Exp(y_i)-\\Exp(\\bar y)=\\beta_0+\\beta_1x_i-\\qty(\\beta_0+\\beta_1\\bar x)=\\beta_1(x_i-\\bar x).\n\\]\n\\[\n\\sum\\qty(\\Exp\\qty(y_i-\\bar y))^2=\\sum\\qty(\\Exp(y_i)-\\Exp(\\bar y))^2=\\sum\\qty(\\beta_0+\\beta_1x_i-\\qty(\\beta_0+\\beta_1\\bar x))^2=\\sum\\qty(\\beta_1(x_i-\\bar x))^2=\\beta_1^2S_{xx}.\n\\]\n\\[\n\\begin{split}\n\\Exp\\qty(\\sum(y_i-\\bar{y}_i)^2)&=\\sum\\Exp\\qty((y_i-\\bar{y})^2)=\\sum\\qty(\\Exp\\qty(y_i-\\bar{y}))^2+\\sum\\Var(y_i-\\bar y) =\\beta_1^2S_{xx}+(n-1)\\sigma^2.\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp(S_{xy})&=\\Exp\\qty(\\sum(x_i-\\bar x)(y_i-\\bar y))=\\Exp\\qty(\\sum(x_i-\\bar x)y_i)=\\sum(x_i-\\bar x)E(y_i)\\\\\n&=\\sum(x_i-\\bar x)(\\beta_0+\\beta_1x_i)=\\sum(x_i-\\bar x)\\beta_1x_i=\\beta_1\\sum(x_i-\\bar x)x_i=\\beta_1\\sum(x_i-\\bar x)^2\\\\\n&=\\beta_1S_{xx}.\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp(S_{xy}^2)&=\\qty(\\Exp(S_{xy}))^2+\\Var(S_{xy})=\\beta_1^2S_{xx}^2+S_{xx}\\sigma^2.\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp(\\frac{S_{xy}^2}{S_{xx}})&=\\frac{1}{S_{xx}}\\beta_1^2S_{xx}^2+\\frac{1}{S_{xx}}S_{xx}\\sigma^2=\\beta_1^2S_{xx}+\\sigma^2.\n\\end{split}\n\\]\n\n\\(\\Exp(S_{xy})=\\beta_1S_{xx}\\).\n\\(\\Var(S_{xy})=S_{xx}\\sigma^2\\).\n\\(\\Exp(S_{yy})=(n-2)\\sigma^2\\). \n\\(\\Exp(S_{xy})\\)\n\\(\\Var(\\S_{xy})\\)\n\\(SST=SSE+SSR\\)\n\\(\\Exp(SST)\\)\n\\(\\Exp(SSR)\\)\n\\(\\Exp(SSE)\\)\n\\(\\Var(SST)\\)\n\\(\\Var(SSR)\\)\n\\(\\Var(SSR)\\)",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#beta_1",
    "href": "contents/2/slr.html#beta_1",
    "title": "5¬† slr",
    "section": "5.4 \\(\\beta_1\\)",
    "text": "5.4 \\(\\beta_1\\)\nhypotheis: \\(\\beta_1\\neq0\\)\n\\(\\hat{\\beta}_1\\) is normal, so t-test:\n\\[\nt=\\frac{\\hat{\\beta}_1-0}{s_{\\hat{\\beta}_1}}=\\frac{\\hat{\\beta}_1}{s/\\sqrt{S_{xx}}},\\quad s^2=\\frac{SSE}{n-2}\n\\]\n\none tailed test:\n\nHa: \\(\\beta_1&lt;0\\): \\(t&lt;-t_{\\alpha}\\): \\(p=\\Pr(t&lt;t_c)\\), \\(t_c&lt;0\\)\nHa: \\(\\beta_1&gt;0\\): \\(t&gt;t_{\\alpha}\\): \\(p=\\Pr(t&gt;t_c)\\), \\(t_c&gt;0\\)\n\ntwo tailed test:\n\nHa: \\(\\beta_1\\neq0\\): \\(\\abs{t}&gt;t_{\\alpha/2}\\): \\(p=2\\Pr(t&gt;t_c)\\), \\(t_c&gt;0\\)\n\n\n\\(t_c\\) is the computed value\n\\(\\Pr(t&gt;t_{\\alpha})=\\alpha=\\Pr(\\text{Type I error})=\\Pr(\\text{Reject }H_0\\mid H_0\\text{ is true.})\\)",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#coeffcient-of-correlation",
    "href": "contents/2/slr.html#coeffcient-of-correlation",
    "title": "5¬† slr",
    "section": "5.5 coeffcient of correlation",
    "text": "5.5 coeffcient of correlation\n\\[\nr=\\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\n\\]\nit is related to COV\n\\(\\rho\\) population coefficient of correlation, is estimated by \\(r\\)\nHypothesis:\n\\(t=r\\sqrt{n-2}/{sqrt{1-r^2}}\\)",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#coefficient-of-determination",
    "href": "contents/2/slr.html#coefficient-of-determination",
    "title": "5¬† slr",
    "section": "5.6 coefficient of determination",
    "text": "5.6 coefficient of determination\n\\[\nr^2=\\frac{S_{yy}-SSE}{S_{yy}}=1-\\frac{SSE}{S_{yy}}\n\\]\nproportion of total saple variability of the y explained by the linear relationship between y and x\nAbout 100(r2)% of the sample variation in y (measured by the total sum of squares of deviations of the sample y-values about their mean ji) can be explained by (or attributed to) using x to predict yin the straight-line model.\n\\[\nSSR=\\sum(\\hat y_i-\\bar y)^2=\\frac{S_{xy}^2}{S_{xx}}\n\\]",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#predicted-interval-and-confidence-interval",
    "href": "contents/2/slr.html#predicted-interval-and-confidence-interval",
    "title": "5¬† slr",
    "section": "5.7 predicted interval and confidence interval",
    "text": "5.7 predicted interval and confidence interval\n\\(\\sigma_{\\hat y}\\)\nSince \\(\\bar y\\) and \\(\\hat{\\beta}_1\\) are independent, we have \\[\n\\begin{split}\n\\Var(\\hat y(x))&=\\Var(\\hat{\\beta}_0+\\hat{\\beta}_1x)=\\Var(\\bar y-\\hat{\\beta}_1(x-\\bar x))=\\Var(\\bar y)+(x-\\bar x)^2\\Var(\\hat{\\beta}_1)\\\\\n&=\\frac1n\\sigma^2+(x-\\bar x)^2\\frac1{S_{xx}}\\sigma^2=\\sigma^2\\qty[\\frac1n+\\frac{(x-\\bar x)^2}{S_{xx}}].\n\\end{split}\n\\]\n\\[\n\\Var(y-\\hat y)=\\sigma^2+\\sigma^2\\qty[\\frac1n+\\frac{(x-\\bar x)^2}{S_{xx}}]=\\sigma^2\\qty[1+\\frac1n+\\frac{(x-\\bar x)^2}{S_{xx}}].\n\\]\n\nlibrary(MASS)\n\nfit &lt;- lm(medv~lstat, data=Boston)\n\nnewx = data.frame(lstat=seq(1, 40, by=0.1))\npred.int &lt;- predict(fit, newx, interval='prediction')\nconf.int &lt;- predict(fit, newx, interval='confidence')\n\nBoth pred.int and conf.int are matrices, where the first column is the fitted value, the second column and the third column are the corresponding interval bounds.\n\nplot(Boston$lstat, Boston$medv)\n\nlines(newx$lstat, pred.int[,1])\n\nlines(newx$lstat, pred.int[,2], col='red')\nlines(newx$lstat, pred.int[,3], col='red')\n\nlines(newx$lstat, conf.int[,2], col='blue')\nlines(newx$lstat, conf.int[,3], col='blue')",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/2/misc.html",
    "href": "contents/2/misc.html",
    "title": "6¬† Variable selection",
    "section": "",
    "text": "6.1 Step-wise\nolsrr ols_step_both_p",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Variable selection</span>"
    ]
  },
  {
    "objectID": "contents/2/misc.html#best-of-subsets",
    "href": "contents/2/misc.html#best-of-subsets",
    "title": "6¬† Variable selection",
    "section": "6.2 best of subsets",
    "text": "6.2 best of subsets\nleaps regsubsets",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Variable selection</span>"
    ]
  },
  {
    "objectID": "contents/2/misc.html#residual-analysis-1",
    "href": "contents/2/misc.html#residual-analysis-1",
    "title": "6¬† Variable selection",
    "section": "7.1 Residual analysis",
    "text": "7.1 Residual analysis\n\n\n\n\n\n\nNoteRegression residual\n\n\n\nThe regression residual is the observed value of the dependent variable minus the predicted value, or\n\\[\n\\hat{\\varepsilon}=y-\\hat y.\n\\]\n\n\n\n\nTheorem 7.1 ¬†\n\nThe mean of the residuals is equal to 0.\nThe standard deviation of the residuals is the standard deviation of the fitted regression model \\(s\\): \\(\\sum \\hat{\\varepsilon}^2=SSE\\), and \\(s=\\sqrt{SSE/(n-k-1)}\\).\n\n\n\n7.1.1 Plots\nWe use plots to display residuals, and detect departures from assumptions. If the assumptions concerning the error term ùúÄ are satisfied, we expect to see the residual plot that\n\nhave no trends\nhave no dramatic increases or decreases in variability\nonly a few residuals (around 5%) more than 2 estimated standard deviations (\\(2s\\)) of ùúÄ above or below 0.\n\n\n\n\n\n\n\nNoteDetecting model lack of fit with residuals\n\n\n\n\nPlot \\(\\varepsilon\\) on the vertical axis against each \\(x_i\\) on the horizontal axis.\nPlot \\(\\varepsilon\\) on the vertical axis against \\(\\hat{y}\\) on the horizontal axis.\nIn each plot, look for\n\ntrends\ndramatic changes in variability\nmore than 5% residuals that lie outside 2s of 0\n\n\n\n\n\n\n7.1.2 Partial residual plot\nThe set of partial regression residuals for the jth independent variable \\(x_j\\) is calculated as follows:\n\\[\n\\hat{\\varepsilon}^*=\\hat{\\varepsilon}+\\hat{\\beta}_jx_j\n\\]\nThe partial residuals verse \\(x_j\\) reveals more information about the relationship between \\(y\\) and \\(x_j\\).\n\ndf &lt;- read.csv(\"COFFEE2.csv\")\nhead(df)\n\n\n##   DEMAND PRICE AD\n## 1   1190   3.0  1\n## 2   1033   3.2  1\n## 3    897   3.4  1\n## 4    789   3.6  1\n## 5    706   3.8  1\n## 6    595   4.0  1\n\nThe residual plot is\n\nfit &lt;- lm(DEMAND~., data=df)\nplot(df$PRICE, fit$residuals)\n\n\n\n\n\n\n\n\nThere is an obvious trend. Therefore it implies a lack of fit.\nNow consider the partial residual plot for PRICE:\n\nplot(df$PRICE, fit$residuals+fit$coefficients['PRICE']*df$PRICE)\n\n\n\n\n\n\n\n\nNow modify the model by making \\(x_1=1/PRICE\\). Let us run the model again.\n\nfit2 &lt;- lm(DEMAND~.-PRICE+I(1/PRICE), data=df)\nsummary(fit2)\n## \n## Call:\n## lm(formula = DEMAND ~ . - PRICE + I(1/PRICE), data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -21.353  -6.721  -3.707   8.259  23.356 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -1217.343     14.898  -81.71  &lt; 2e-16 ***\n## AD             70.182      4.732   14.83 6.71e-12 ***\n## I(1/PRICE)   6986.507     56.589  123.46  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.1 on 19 degrees of freedom\n## Multiple R-squared:  0.9988, Adjusted R-squared:  0.9986 \n## F-statistic:  7731 on 2 and 19 DF,  p-value: &lt; 2.2e-16\nplot(df$PRICE, fit2$residuals)\n\n\n\n\n\n\n\n\n\nplot(df$PRICE, fit2$residuals+fit$coefficients['PRICE']*df$PRICE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipWhat You Should Expect in a Partial Residual Plot\n\n\n\nPartial residual plot is like looking at the impact of a single variable. It will show you how a certain variable is contributing to the response variable \\(y\\).\n\nYou expect to see a roughly linear pattern between the predictor and the partial residuals. If the relationship is nonlinear, it suggests a transformation or non-linear model might be better.\nNo strong curvature or pattern. Patterns like U-shapes or waves indicate the model may be missing a non-linear term (e.g., \\(x^2\\), \\(\\log(x)\\)).\nThe spread of points should be roughly even across the range of the predictor.\nThe residuals should be centered around zero, showing that the model doesn‚Äôt systematically over- or under-predict.\n\n\n\n\n\n7.1.3 Homoscedastic vs Heteroscedastic\nThis part is about whether the variance is equal.\n\n\n\n\n\n\nNote\n\n\n\n\nVariances that satisfy this property are called homoscedastic.\nUnequal variances for different settings of the independent variable(s) are said to be heteroscedastic.\n\n\n\n\nPossible reasons: the variance is a function of its mean \\(E(y)\\). In this case we perform a transformation to \\(y\\). It is called variance-stabilizing transformations.\n\n\n\nType of Response\nVariance\nStabilizing transformation\n\n\n\n\nPoisson\n\\(E(y)\\)\n\\(\\sqrt{y}\\)\n\n\nBinomial\n\\(E(y)[1-E(y)]/n\\)\n\\(\\sin^{-1}{\\sqrt y}\\)\n\n\nMultiplicative\n\\([E(y)]^2\\sigma^2\\)\n\\(\\ln{y}\\)\n\n\n\n\ndf &lt;- read.csv(\"SOCWORK.csv\")\nhead(df)\n\n\n##   EXP SALARY LNSALARY EXPSQ\n## 1   7  26075  10.1687    49\n## 2  28  79370  11.2819   784\n## 3  23  65726  11.0932   529\n## 4  18  41983  10.6450   324\n## 5  19  62308  11.0398   361\n## 6  15  41154  10.6251   225\n\n\nfit &lt;- lm(SALARY~EXP+I(EXP^2), data=df)\nsummary(fit)\n## \n## Call:\n## lm(formula = SALARY ~ EXP + I(EXP^2), data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15360.3  -4703.4   -783.5   3872.7  22716.2 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 20242.12    4422.59   4.577 3.46e-05 ***\n## EXP           522.30     616.68   0.847  0.40131    \n## I(EXP^2)       53.01      19.57   2.708  0.00941 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8123 on 47 degrees of freedom\n## Multiple R-squared:  0.8157, Adjusted R-squared:  0.8078 \n## F-statistic:   104 on 2 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nplot(fit$fitted.values, fit$residuals)\n\n\n\n\n\n\n\n\nIt has a very rough fang pattern. So we try \\(\\log\\) transformation.\n\nfit2 &lt;- lm(log(SALARY)~EXP+I(EXP^2), data=df)\nsummary(fit2)\n## \n## Call:\n## lm(formula = log(SALARY) ~ EXP + I(EXP^2), data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.35492 -0.09022 -0.01778  0.09756  0.26265 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 9.843e+00  8.479e-02 116.079  &lt; 2e-16 ***\n## EXP         4.969e-02  1.182e-02   4.203 0.000117 ***\n## I(EXP^2)    9.415e-06  3.753e-04   0.025 0.980091    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1557 on 47 degrees of freedom\n## Multiple R-squared:  0.8635, Adjusted R-squared:  0.8577 \n## F-statistic: 148.7 on 2 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nplot(fit2$fitted.values, fit2$residuals)\n\n\n\n\n\n\n\n\n\n\n7.1.4 Normality\nThe last topic to check the normality of the common distribution of the residuals. The most common tool is called Q-Q plot (Quantile-Quantile plot): a plot to compare the quantiles of your data to the quantiles of a standard normal distribution.\n\n\n\n\n\n\nNoteExpectations from a Q-Q plot\n\n\n\n\nIf the points are on the reference line, then the points follow normal distribution.\nPoints lower the line indicate a left skewness. Points above the line indicate a right skewness.\n\n\n\n\nqqnorm(fit$residuals)\nqqline(fit$residuals)\n\n\n\n\n\n\n\n\n\nqqnorm(fit2$residuals)\nqqline(fit2$residuals)\n\n\n\n\n\n\n\n\nUsually nonnormality is accopanied by heteroscedasticity. For more transformations, we could use Box-Cox approach.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that regression is robust with respect to nonnormal errors if the sample size is reasonable large. However, if the distribution of the residuals is highly skewed, you may want to search for a normalizing transformation, for example, using Box-Cox approach.\n\n\n\n\n\n\n\n\nNoteBox-Cox transformation\n\n\n\nThe Box-Cox transforamtion is defined as\n\\[\n\\begin{split}\ny(\\lambda)&=\\frac{y^{\\lambda}-1}{\\lambda}\\approx y^{\\lambda}\\quad\\text{ for }\\lambda\\neq 0,\\\\\ny(\\lambda)&=\\log(y)\\quad\\text{ for }\\lambda=0\n\\end{split}\n\\]\nAfter we apply the Box-Cox transformations with different \\(\\lambda\\) to \\(y\\), we use a model to fit the transformed data, and then study the corresponding residuals. For each model, we compute the likelihood of the model‚Äôs residuals under the assumption that the errors are noramlly distributed. Then we would pick the \\(\\lambda\\) with the maximal likelihood.\n\n\nThis part is done by the following code. We generate the Box-Cox plot with is log-likelihood vs.¬†\\(\\lambda\\). We find the best \\(\\lambda\\) based on the plot, and perform the corresponding transformation.\nLet us consider the original example.\n\nlibrary(MASS)\nboxcox(fit, data=df)\n\n\n\n\n\n\n\n\nThe peak is around \\(\\lambda=0\\), so we will apply \\(\\log\\) transformation to \\(y\\).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Variable selection</span>"
    ]
  },
  {
    "objectID": "contents/2/misc.html#detecting-outliers",
    "href": "contents/2/misc.html#detecting-outliers",
    "title": "6¬† Variable selection",
    "section": "7.2 Detecting outliers",
    "text": "7.2 Detecting outliers\n\n7.2.1 Standardized residuals\n\nDefinition 7.1 The standardized residual, denoted \\(z_i\\), is \\(z_i=\\hat{\\varepsilon}_i/s\\).\n\nOutliers are points whose standarized residual is beyond 1, 2, 3, etc.. The threshold is based on the confidence level. Usually we use 2.\n\n\n7.2.2 influential observations\n\nleverage\nCook‚Äôs distance\n\n\ndf &lt;- read.csv(\"FASTFOOD.csv\")\ndf$CITY &lt;- as.factor(df$CITY)\nhead(df)\n\n\n##   CITY TRAFFIC SALES X1 X2 X3\n## 1    1    59.3   6.3  1  0  0\n## 2    1    60.3   6.6  1  0  0\n## 3    1    82.1   7.6  1  0  0\n## 4    1    32.3   3.0  1  0  0\n## 5    1    98.0   9.5  1  0  0\n## 6    1    54.1   5.9  1  0  0\n\n\nfit &lt;- lm(SALES~TRAFFIC+CITY, data=df)\ndf$CITY &lt;- as.factor(df$CITY)\nsummary(fit)\n## \n## Call:\n## lm(formula = SALES ~ TRAFFIC + CITY, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -11.681  -7.331  -1.390   1.719  56.464 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept) -15.3532    11.0339  -1.391   0.1802  \n## TRAFFIC       0.3629     0.1679   2.161   0.0437 *\n## CITY2         5.0367    10.2984   0.489   0.6304  \n## CITY3        13.3835     7.6893   1.741   0.0979 .\n## CITY4        -1.1061     8.4226  -0.131   0.8969  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 14.86 on 19 degrees of freedom\n## Multiple R-squared:  0.2595, Adjusted R-squared:  0.1036 \n## F-statistic: 1.665 on 4 and 19 DF,  p-value: 0.1996\n\n\nDefinition 7.2 (Leverage) The leverage of the ith observation is the weight \\(h_i\\) associated with \\(y_i\\) in the equation\n\\[\n\\hat{y}_i=h_1y_1+h_2y_2+\\ldots+h_iy_i+\\ldots+h_ny_n\n\\] where \\(h_1\\), ‚Ä¶, \\(h_n\\) are functions of only the values of the independent variables in the model. It measures the influence of \\(y_i\\) on its predicted value \\(\\hat{y_i}\\).\n\n\n\n\n\n\n\nNoteRule of Thumb for detecting influence with leverage\n\n\n\n\\(y_i\\) is influential if \\(h_i&gt;\\dfrac{2(k+1)}{n}\\) where \\(k\\) is the number of variables.\n\n\n| Low Residual | High Residual Low Leverage | Unimportant | Small influence High Leverage | Maybe okay | ‚ùó High influence ‚ùó\n\nDefinition 7.3 (Cook‚Äôs distance) \\[\nD_i=\\frac{\\sum (\\hat{y}_j-\\hat{y}_{j(i)})^2}{p\\cdot MSE}\n\\] where - \\(\\hat y_j\\) = predicted value from full model - \\(\\hat{y}_{j(i)}\\) = predicted value without the i-th point - \\(p\\) = number of parameters (including intercept) - \\(MSE\\) = mean squared error\n\nValues of \\(ùê∑_ùëñ\\) can be compared to the values of the ùêπ distribution with \\(ùúà_1  = ùëò + 1\\) and \\(ùúà_2  = ùëõ ‚àí (ùëò + 1)\\) degrees of freedom.\nUsually, an observation with a value of \\(ùê∑_ùëñ\\) that falls at or above the 50th percentile of the ùêπ distribution is considered to be an influential observation.\nA range rule of thumb is: if \\(ùê∑_ùëñ  &gt;1\\) indicate that the ùëñth observation is influential and should be studied further.\n\n\n\n\n\n\nNoteRule of Thumb for Cook‚Äôs distance\n\n\n\n\n\\(D_i\\approx 0\\): not influential\n\\(D_i\\approx 0.5\\): worth looking into\n\\(D_i&gt;1\\): Highly influential, investigate further\n\n\n\nTo compute cook‚Äôs distance,\n\ncooks.distance(fit)\n##            1            2            3            4            5            6 \n## 2.315699e-06 6.612132e-07 1.142570e-02 1.247986e-02 6.688119e-02 3.454261e-04 \n##            7            8            9           10           11           12 \n## 3.838848e-04 4.156670e-04 7.281906e-03 1.816125e-02 1.235505e-02 3.045589e-04 \n##           13           14           15           16           17           18 \n## 1.195665e+00 1.963451e-02 1.560885e-02 2.454621e-02 1.454909e-02 8.278103e-03 \n##           19           20           21           22           23           24 \n## 1.980053e-02 8.088886e-04 4.219801e-03 3.581089e-04 1.047277e-02 1.760832e-01\n\nand detect influential points:\n\nwhich(cooks.distance(fit)&gt;1)\n## 13 \n## 13",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Variable selection</span>"
    ]
  },
  {
    "objectID": "contents/2/misc.html#plotfit",
    "href": "contents/2/misc.html#plotfit",
    "title": "6¬† Variable selection",
    "section": "7.3 plot(fit)",
    "text": "7.3 plot(fit)\nWe can use plot(fit) to directly get plots related to residual analysis.\nIt contains several plots.\n\nplot(fit, which=1)\n\n\n\n\n\n\n\n\nThis typically produces the residuals vs.¬†fitted values plot. It is used to assess the linearity assumption and to check for patterns in the residuals. If the residuals exhibit a random scatter around zero with no clear pattern, it suggests that the model‚Äôs assumptions are reasonable. On the other hand, if there is a pattern (e.g., curvature), it may indicate that the model is not appropriately capturing the relationship between the predictor(s) and the response variable.\n\nplot(fit, which=2)\n\n\n\n\n\n\n\n\nThis is the Q-Q plot.\n\nplot(fit, which=3)\n\n\n\n\n\n\n\n\nThis is the scale-location plot (also called a ‚Äúspread-location plot‚Äù), which helps to check for homogeneity of variance (constant variance of residuals). It plots the square root of the standardized residuals against the fitted values, and a horizontal band of points indicates that the variance is constant.\n\nplot(fit, which=5)\n\n\n\n\n\n\n\n\nThis is the residuals vs leverage plot. - x-axis: leverage - y-aixs: studentized residuals - bubble size: cook‚Äôs distance\nIn this plot: - read it horizentally to get the leverage information: to tell how influential the data is - read it vertically to get the standardized information: to tell how ‚Äúoutlier‚Äù the data is - use the cook‚Äôs distance contour line to tell the cook‚Äôs distance of the data point\nIn this case, you can see an outlier #13: it has high residual, low leverage, ok cook‚Äôs distance. So it is an OK outlier which doesn‚Äôt have a big influence.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Variable selection</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html",
    "href": "contents/3/mlr.html",
    "title": "9¬† MLR",
    "section": "",
    "text": "9.1 General form\nThe steps are very similar to simple linear regression.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#general-form",
    "href": "contents/3/mlr.html#general-form",
    "title": "9¬† MLR",
    "section": "",
    "text": "Definition 9.1 \\[\ny=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\ldots+\\beta_kx_k+\\epsilon\n\\] where\n\n\\(y\\) is the dependent variable (response variable)\n\\(x_1,\\ldots,x_k\\) are the independent variable\n\\(E(y)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\ldots+\\beta_kx_k\\) is the deterministic portion of the model\n\\(\\beta_i\\) determines the contribution of the independent variable \\(x_i\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there might be relations among \\(x_i\\)‚Äôs.\n\nIf \\(x_2=x_1^2\\), \\(x_2\\) is called a higher-order term.\nIf \\(x_3=1\\) in some cases and \\(0\\) in some other cases, \\(x_3\\) is called a coded variable. This is an example of categorical variables in regression.\n\n\n\n\n\nCollect data.\nHypothesize the form of the model.\nEstimate the unknown parameters \\(\\beta_0,\\ldots,\\beta_k\\).\nSpecify the distribution of \\(\\epsilon\\) and estimate its \\(\\sigma^2\\).\nEvaluate the utility of the model, like checking p-values and \\(R^2\\).\nVerify the model by checking the assumptions on \\(\\epsilon\\).\nUse the model. Analyze the prediction and make inferences.\n\n\n\n\n\n\n\nNoteAssumptions about \\(\\epsilon\\)\n\n\n\n\nFor any given \\(x_i\\), \\(\\epsilon\\sim \\distnormal(0,\\sigma^2)\\).\nThe random errors are independent.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#a-first-order-model-with-quantitative-predictors",
    "href": "contents/3/mlr.html#a-first-order-model-with-quantitative-predictors",
    "title": "9¬† MLR",
    "section": "9.2 A first-order model with quantitative predictors",
    "text": "9.2 A first-order model with quantitative predictors\n\nDefinition 9.2 (A First-Order Model in \\(k\\) Quantitative Independent Variables) \\[\nE(y)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\ldots+\\beta_kx_k\n\\] where \\(x_1,\\ldots,x_k\\) are \\(k\\) quantitative independent variables.\n\nAfter inputing the data, we have the following equations.\n\\[\ny_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_kx_{ik}+\\epsilon_i,\\quad i=1,\\ldots,n.\n\\]\nWrite them in term of matrices:\n\\[\n\\mathbf{y}=\\mqty[y_1\\\\\\ldots\\\\y_n],\\quad \\mathbf{X}=\\mqty[1&x_{11}&x_{12}&\\ldots&x_{1k}\\\\\\ldots&&&&\\\\1&x_{n1}&x_{n2}&\\ldots&x_{nk}],\\quad \\boldsymbol{\\beta}=\\mqty[\\beta_0\\\\\\beta_1\\\\\\ldots\\\\\\beta_k],\\quad \\boldsymbol{\\epsilon}=\\mqty[\\epsilon_1\\\\\\ldots\\\\\\epsilon_n].\n\\]\nThen the model can be expressed as \\[\n\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\]\nThe OLS estimator is used to minimize the square sum of residuals \\[\n(\\mathbf y-\\mathbf X\\boldsymbol{\\beta})^T(\\mathbf y-\\mathbf X\\boldsymbol{\\beta}).\n\\]\nTherefore \\[\n\\boldsymbol{\\hat{\\beta}}=\\argmin_{\\beta}(\\mathbf y-\\mathbf X\\boldsymbol{\\beta})^T(\\mathbf y-\\mathbf X\\boldsymbol{\\beta}).\n\\]\n\nTheorem 9.1 (The OLS estimator) The following estimator is an unbiased estimator of \\(\\boldsymbol{\\beta}\\): \\[\n\\boldsymbol{\\hat{\\beta}}=\\mathbf{(X^T X)^{-1}X^T y}\n\\] In addition, \\[\n\\Var(\\boldsymbol{\\hat{\\beta}})=\\mathbf{(X^T X)}^{-1}\\sigma^2.\n\\]\n\n\n\n\n\n\n\nNoteSLR case\n\n\n\n\n\nWhen \\(k=1\\), we have\n\\[\n\\mathbf X=\\mqty[1&1&\\ldots&1\\\\x_1&x_2&\\ldots&x_n],\\quad \\mathbf y=\\mqty[y_1\\\\\\ldots\\\\y_n].\n\\] Then\n\\[\n\\begin{split}\n\\mathbf{X^T X}&=\\mqty[1&1&\\ldots&1\\\\x_1&x_2&\\ldots&x_n]\\mqty[1&x_1\\\\1&x_2\\\\\\ldots&\\\\1&x_n]=\\mqty[n&n\\bar{x}\\\\n\\bar x&\\sum x_i^2],\\\\\n\\mathbf{X^T y}&=\\mqty[1&1&\\ldots&1\\\\x_1&x_2&\\ldots&x_n]\\mqty[y_1\\\\\\ldots\\\\y_n]=\\mqty[n\\bar y\\\\\\sum x_iy_i],\\\\\n\\mathbf{(X^T X)^{-1}X^T y}&=\\mqty[n&n\\bar{x}\\\\n\\bar x&\\sum x_i^2]^{-1}\\mqty[n\\bar y\\\\\\sum x_iy_i]\\\\\n&=\\frac{1}{n(\\sum x_i^2)-n^2\\bar x^2}\\mqty[\\sum x_i^2&-n\\bar x\\\\-n\\bar x&n]\\mqty[n\\bar y\\\\\\sum x_iy_i]\\\\\n&=\\frac{1}{n(\\sum x_i^2)-n^2\\bar x^2}\\mqty[n(\\sum x_i^2)\\bar y-n\\bar x(\\sum x_iy_i)\\\\n(\\sum x_iy_i)-n^2\\bar x\\bar y]\\\\\n&=\\frac{1}{(\\sum x_i^2)-n\\bar x^2}\\mqty[(\\sum x_i^2)\\bar y-\\bar x(\\sum x_iy_i)\\\\(\\sum x_iy_i)-n\\bar x\\bar y]\\\\\n&=\\frac{1}{\\sum x_i^2-n\\bar x^2}\\mqty[(\\sum x_i^2-n\\bar x^2)\\bar y+n\\bar x^2\\bar y-\\bar x(\\sum x_iy_i)\\\\\\sum x_iy_i-n\\bar x\\bar y]\\\\\n&=\\frac{1}{\\sum x_i^2-n\\bar x^2}\\mqty[(\\sum x_i^2-n\\bar x^2)\\bar y-\\bar x(\\sum x_iy_i-n\\bar x\\bar y)\\\\\\sum x_iy_i-n\\bar x\\bar y].\n\\end{split}\n\\]\nNote that\n\\[\n\\begin{split}\nS_{xx}&=\\sum (x_i-\\bar x)^2=\\sum(x_i^2-2x_i\\bar x+\\bar x^2)=\\sum x_i^2-2(\\sum x_i)\\bar x+n\\bar x^2=\\sum x_i^2-n\\bar x^2,\\\\\n% S_{yy}&=\\sum (y_i-\\bar y)^2=\\sum(y_i^2-2y_i\\bar y+\\bar y^2)=\\sum y_i^2-2(\\sum y_i)\\bar y+n\\bar y^2=\\sum y_i^2-n\\bar y^2,\\\\\nS_{xy}&=\\sum (x_i-\\bar x)(y_i-\\bar y)=\\sum(x_iy_i-x_i\\bar y-y_i\\bar x+\\bar x\\bar y)\\\\\n&=\\sum x_iy_i-(\\sum x_i)\\bar y-(\\sum y_i)\\bar x+n\\bar x\\bar y=\\sum x_iy_i-n\\bar x\\bar y,\n\\end{split}\n\\]\nWe have\n\\[\n\\begin{split}\n\\mathbf{(X^T X)^{-1}X^T y}&=\\frac{1}{\\sum x_i^2-n\\bar x^2}\\mqty[(\\sum x_i^2-n\\bar x^2)\\bar y-\\bar x(\\sum x_iy_i-n\\bar x\\bar y)\\\\\\sum x_iy_i-n\\bar x\\bar y]\\\\\n&=\\frac{1}{S_{xx}}\\mqty[S_{xx}(\\bar y-\\bar x(S_{xy}))\\\\S_{xy}]=\\mqty[\\bar y-\\bar x(S_{xy}/S_{xx})\\\\S_{xy}/S_{xx}].\n\\end{split}\n\\]\nTherefore in the case of \\(k=1\\), \\(\\hat{\\beta}_1=S_{xy}/S_{xx}\\) and \\(\\hat{\\beta}_0=\\bar y-\\beta_1\\bar x\\).\nIn addition, the variance of \\(\\hat{\\beta_1}\\) is the (2,2)-entry of the variance-covariance matrix \\(\\hat{\\boldsymbol{\\beta}}\\). Since\n\\[\n\\begin{split}\n\\Var(\\boldsymbol{\\hat{\\beta}})=\\mathbf{(X^T X)}^{-1}\\sigma^2=\\frac{1}{n(\\sum x_i^2)-n^2\\bar x^2}\\mqty[\\sum x_i^2&-n\\bar x\\\\-n\\bar x&n]\\sigma^2,\n\\end{split}\n\\] the (2,2)-entry is \\[\n\\begin{split}\n\\Var(\\hat{\\beta_1})&=\\frac{n}{n(\\sum x_i^2-n\\bar x^2)}\\sigma^2=\\frac{\\sigma^2}{S_{xx}}.\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIn MLR, the multicolinearity between independent variables doesn‚Äôt affect the unbiasedness of the OLS estimators. In other words, even if those independent variable are colinear, the OLS estimators is still BLUE (Best Linear Unbiased Estimators) to estimate \\(\\boldsymbol{\\hat{\\beta}}\\) as long as other standard assumptions hold. However, the variance and the stability may be impacted, and this may impact the later inference.\n\nMulticollinearity inflates the variance of the estimated coefficients.\nEven if a variable is genuinely important, its coefficient might not be statistically significant due to high standard errors.\nThe model might struggle to separate the effects of correlated predictors.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#the-analysis-of-variance",
    "href": "contents/3/mlr.html#the-analysis-of-variance",
    "title": "9¬† MLR",
    "section": "9.3 The analysis of variance",
    "text": "9.3 The analysis of variance\n\n\\(SSE=\\sum (y_i-\\hat{y}_i)^2\\).\n\\(s^2=MSE=\\frac{SSE}{n-(k+1)}\\).\n\nHypothesis test:\n\nDefinition 9.3 (The Global \\(F\\)-Test) ¬†\n\n\\(H_0\\): all \\(\\beta_i\\)‚Äôs are \\(0\\).\n\\(H_a\\): at least one \\(\\beta_i\\) is not \\(0\\).\n\\(F=\\frac{MSR}{MSE}=\\frac{(SST-SSE)/k}{SSE/[n-(k+1)]}\\).\n\\(p=\\Pr(F&gt;F_{\\alpha})\\).\n\nNote that since \\(F\\)-statistics is always positive, we only need to compute one tail of it.\n\n\nDefinition 9.4 (The individual \\(t\\)-Test for variable \\(\\beta_i\\)) ¬†\n\n\\(H_0\\): \\(\\beta_i=0\\).\n\\(H_a\\): \\(\\beta_i\\neq 0\\).\n\\(t=\\frac{\\hat{\\beta}_i}{s_{\\hat{\\beta}_i}}\\).\n\\(p=\\Pr(\\abs{t}&gt;t_{\\alpha})\\).\n\nNote that these \\(\\hat{\\beta}_i\\) and \\(s_{\\hat{\\beta}_i}\\) directly come from the OLS estimators.\n\n\n\n\n\n\n\nCaution\n\n\n\n\nYou have to pass F-test to proceed. If not, stop and modify the model.\nTry to minimize the number of t-tests. Too many t-tests leads to a high overall Type I error rate.\nWhen a parameter is not significant, there are some possibilities:\n\n\\(y\\) and \\(x_i\\) don‚Äôt have relations.\nThey have a linear relation, but Type II error occurred.\nThey have non-linear relations.\n\n\n\n\n\nExample 9.1 (a simple R code example) In base R, we need the following command to perform manual operations:\n\nas.matrix to convert a data.frame into a matrix.\n%*% is for matrix multiplication.\nt is the transpose of a matrix.\nsolve is used to compute the inverse of a matrix.\ncbind is to combine two matrices horizontally. (rbind is to combine matrices vertically, but we don‚Äôt need it here.)\n\nThen once you have two dataframes dfX and dfY, the following code can be used to compute the OLS estimators.\n\nX &lt;- as.matrix(dfX)\nX &lt;- cbind(1, X)\ny &lt;- as.matrix(dfY)\nbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n\nTo compute the variance, we should first estimate \\(\\sigma^2\\).\n\nyhat &lt;- X %*% beta\nSSE &lt;- sum((y-yhat)^2)\nn &lt;- dim(X)[1]\nk &lt;- dim(X)[2] - 1\ns2 &lt;- SSE/(n-(k+1))\nvar_beta &lt;- s2 * solve(t(X) %*% X)\n\nWe could use diag to extract the diagonal to get variance and ignore all the covariance terms. Then apply sqrt to get the standard errors.\n\nse_beta &lt;- sqrt(diag(var_beta))\n\nFrom here, we could compute the corresponding \\(t\\)-tests and \\(F\\)-test, by the formula \\(t=\\frac{\\hat{\\beta}}{s_{\\hat{\\beta}}}\\) and \\(F=\\frac{MSR}{MSE}\\).\n\nt &lt;- beta / se_beta\nMSR &lt;- sum((y_hat-mean(y))^2) / k\nMSE &lt;- s2\nF &lt;- MSR/MSE\n\nAfter we get the statistics, we could conduct the corresponding test by checking the p-values.\n\np_t &lt;- pt(abs(t), n-(k+1), lower.tail = FALSE) + pt(-abs(t), n-(k+1)) \np_f &lt;- pf(F, k, n-(k+1), lower.tail = FALSE)",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#evluation",
    "href": "contents/3/mlr.html#evluation",
    "title": "9¬† MLR",
    "section": "9.4 Evluation",
    "text": "9.4 Evluation\n\n9.4.1 \\(R^2\\)\n\nDefinition 9.5 (The multiple coefficient of determination) \\[\nR^2=1-\\frac{SSE}{SST}.\n\\] \\(R^2\\) represents the fraction of the sample variation that is explained by the model.\n\n\n\n9.4.2 \\(R_a^2\\)\n\\(R^2\\) is a ‚Äúhard‚Äù measurement for the model performance. However since adding more predictors always increases \\(R^2\\) even if they are useless, we need a way to adjust for the number of predictors. This is the adjusted \\(R^2\\).\n\nDefinition 9.6 (The adjusted multiple coefficient of determination) \\[\nR^2_a=1-\\qty[\\frac{(n-1)}{n-(k+1)}]\\qty(\\frac{SSE}{SST})=1-\\qty[\\frac{(n-1)}{n-(k+1)}]\\qty(1-R^2).\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\n\\(R_a^2\\leq R^2\\).\nFor poor-fitting models \\(R_a^2\\) can be negative.\nThe practical interpretation for \\(R_a^2\\) is the fraction of the variance explained by the model adjusted for the sample size and the number of parameters.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#analysis-of-predictions",
    "href": "contents/3/mlr.html#analysis-of-predictions",
    "title": "9¬† MLR",
    "section": "9.5 Analysis of predictions",
    "text": "9.5 Analysis of predictions\n\nC.I.\nP.I.\n\ncode are the same\n\n\n\n\n\n\nWarning\n\n\n\nExteprelation is dangerous.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#interaction-and-confounding",
    "href": "contents/3/mlr.html#interaction-and-confounding",
    "title": "9¬† MLR",
    "section": "9.6 Interaction and confounding",
    "text": "9.6 Interaction and confounding\n\nlm(y~x+z+x:z)\nlm(y~x:z)\nx2 &lt;- x*x, lm(y~x+x2)\nlm(y~x+I(x^2))\nlm(y~poly(x,2,raw=True))",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#dummy-variables",
    "href": "contents/3/mlr.html#dummy-variables",
    "title": "9¬† MLR",
    "section": "9.7 Dummy variables",
    "text": "9.7 Dummy variables\nR use factors to regonze processable categorical data.\nlm function will automatically cast processiable categorical data into dummy variables.\nCARGO has durable, fragile and semifrag\nif the variable is not numeric, it will be changed into categorical regression with dummy variables directly.\n\ncargo$CARGO as.factor(cargo$CARGO)\ncontrasts(cargo$CARGO)\n\ncontrasts is to show how dummy variables are set\nif we are changing the base level:\n\ncargo$CARGO = relevel(cargo$CARGO, ref=\"Fragile\")\ncontrasts(cargo$CARGO)\n\n\nfit &lt;- lm(COST~CARGO, data=cargo)\nsummary(fit)",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#a-test-for-comparing-nested-models",
    "href": "contents/3/mlr.html#a-test-for-comparing-nested-models",
    "title": "9¬† MLR",
    "section": "9.8 A test for comparing nested models",
    "text": "9.8 A test for comparing nested models\n\nDefinition 9.7 (Nested models) Two models are nested if one model contains all the terms of the second model and at least one additional term.\n\nThe more complex model is called the complete (or full) model.\nThe simpler model is called the reduced (or restricted) model.\n\n\nHere the main question is whether the additional terms are really necessary. We use a Hypothesis test to answer the question.\n\n\n\n\n\n\nNoteF-Test for comparing nested models\n\n\n\n\nReduced model: \\(E(y)=\\beta_0+\\beta_1x_1+\\ldots+\\beta_g x_g\\)\nComplete model: \\(E(y)=\\beta_0+\\beta_1x_1+\\ldots+\\beta_g x_g+\\beta_{g+1}x_{g+1}+\\ldots+\\beta_{k}x_k\\)\n\\(H_0: \\beta_{g+1}=\\ldots=\\beta_k=0\\).\n\\(H_a\\): at least one of the \\(\\beta_{g+1},\\ldots,\\beta_k\\) is nonzero.\n\\(\\displaystyle F=\\frac{(SSE_R-SSE_C)/(k-g)}{SSE_C/[n-(k+1)]}\\).\n\n\n\nWe could use the above formula to manually perform the test.\nor we could use R code anova(fit.reduced, fit.full)",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#dummy-variable-qualitative-variables",
    "href": "contents/3/mlr.html#dummy-variable-qualitative-variables",
    "title": "9¬† MLR",
    "section": "9.9 Dummy variable: qualitative variables",
    "text": "9.9 Dummy variable: qualitative variables",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#collinearity-in-multiple-linear-regression",
    "href": "contents/3/mlr.html#collinearity-in-multiple-linear-regression",
    "title": "9¬† MLR",
    "section": "9.10 Collinearity in Multiple Linear Regression",
    "text": "9.10 Collinearity in Multiple Linear Regression\n\nFirst issue about collinearity is that \\(\\mathbf{X^T X}\\) would be singular, or at least close to singular, that its inverse doesn‚Äôt exist or the computation is very unstable.\nThe covariance matrix \\(\\mathbf{(X^T X)}^{-1}\\sigma^2\\) will be extremely large, which means that the variance will be very large.\n\n\nDefinition 9.8 (The variance inflation factor) The variance inflation factor for the ith coefficient is \\[\nVIF_i=\\frac{1}{1-R^2_i}\n\\] where \\(R_i^2\\) is the \\(R^2\\) of regression produced by regressing \\(x_i\\) against other \\(x_j\\)‚Äôs. If one variable is colinear to other variables, its VIF is supposed to be \\(\\infty\\). If not, its VIF is supposed to be \\(1\\).\n\nUsually if VIF is more than 5 or 10, multicollinearity may be a problem.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/3/mlr.html#r-code",
    "href": "contents/3/mlr.html#r-code",
    "title": "9¬† MLR",
    "section": "9.11 R code",
    "text": "9.11 R code\nHow to use lm function.\n\nx1 &lt;- runif(100)\nx2 &lt;- runif(100)\neps &lt;- rnorm(100, 0, 0.5)\ny &lt;- 1+2*x1+3*x2+eps\nmodel &lt;- lm(y~x1+x2)\n\n\nModel coefficients\n\n\nmodel$coefficients\n## (Intercept)          x1          x2 \n##    1.102617    2.066415    2.792122\nmodel$coefficients['(Intercept)']\n## (Intercept) \n##    1.102617\nmodel$coefficients['x1']\n##       x1 \n## 2.066415\nmodel$coefficients['x2']\n##       x2 \n## 2.792122\n\nWhen using SLR, we could directly read the coefficients and plot lines.\n\nplot(x1, y)\nabline(lm(y~x1)$coefficients)\n\n\n\n\n\n\n\n\n\nSome computed values\n\nWe could directly find the residuals and the fitted values.\n\nmodel$residuals\nmodel$fitted.values\n\n\nModel properties The standard error, \\(R^2\\), \\(R_a^2\\), and \\(F\\)-statistic can be gotten from here.\n\n\nsummary(model)$sigma\n## [1] 0.5230084\nsummary(model)$r.squared\n## [1] 0.7724122\nsummary(model)$adj.r.squared\n## [1] 0.7677197\nsummary(model)$fstatistic\n##    value    numdf    dendf \n## 164.6046   2.0000  97.0000\n\nThe individual Std. error and t value and p value can also be read directly.\n\nsummary(model)$coefficients\n##             Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) 1.102617  0.1699613  6.487460 3.684887e-09\n## x1          2.066415  0.2321884  8.899734 3.179516e-14\n## x2          2.792122  0.1673629 16.683043 2.955100e-30\nsummary(model)$coefficients['x2', 't value']\n## [1] 16.68304\n\n\nWhy Does summary(lm()) Show t-tests Instead of F-tests? Regression focus:\n\nsummary(lm()) emphasizes coefficient estimates (slopes), so t-tests are more intuitive.\nIt answers: ‚ÄúIs this specific predictor‚Äôs slope ‚â† 0?‚Äù\nANOVA focus:\nanova(lm()) emphasizes variance explained by each term.\nIt answers: ‚ÄúDoes this predictor (or model) explain significant variance?‚Äù\n\nKey Takeaways For simple regression (1 predictor):\n\nThe t-test (from summary) and F-test (from anova) are identical.\nFor multiple predictors:\nt-tests assess individual slopes (adjusting for other predictors).\nF-tests assess groups of predictors (e.g., all levels of a categorical variable).\nANOVA is a generalization:\nt-tests are a special case of ANOVA when testing a single predictor.",
    "crumbs": [
      "MLR",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>MLR</span>"
    ]
  },
  {
    "objectID": "contents/app/data.html",
    "href": "contents/app/data.html",
    "title": "Appendix A ‚Äî Datasets",
    "section": "",
    "text": "A.1 possum\nThe dataset possum originates from the DAAG package, created to support the book Data Analysis and Graphics Using R [1]. The underlying data were collected in [2], with the scientific aim of quantifying morphological variation among geographically distinct populations of the mountain brushtail possum. The dataset possum contains nine morphometric measurements on each of 104 possums trapped across seven Australian sites spanning southern Victoria to central Queensland. Additional details and variable descriptions are available in the official DAAG package documentation.\nThe package can be installed by the following code.\ninstall.packages(\"DAAG\")\nThe dataset can be loaded by the following code.\nlibrary(DAAG)\ndata(possum)\nhead(possum)\n##     case site Pop sex age hdlngth skullw totlngth taill footlgth earconch  eye\n## C3     1    1 Vic   m   8    94.1   60.4     89.0  36.0     74.5     54.5 15.2\n## C5     2    1 Vic   f   6    92.5   57.6     91.5  36.5     72.5     51.2 16.0\n## C10    3    1 Vic   f   6    94.0   60.0     95.5  39.0     75.4     51.9 15.5\n## C15    4    1 Vic   f   6    93.2   57.1     92.0  38.0     76.1     52.2 15.2\n## C23    5    1 Vic   f   2    91.5   56.3     85.5  36.0     71.0     53.2 15.1\n## C24    6    1 Vic   f   1    93.1   54.8     90.5  35.5     73.2     53.6 14.2\n##     chest belly\n## C3   28.0    36\n## C5   28.5    33\n## C10  30.0    34\n## C15  28.0    34\n## C23  28.5    33\n## C24  30.0    32",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "contents/app/data.html#sec-app_possum",
    "href": "contents/app/data.html#sec-app_possum",
    "title": "Appendix A ‚Äî Datasets",
    "section": "",
    "text": "[1] Maindonald, J. H. and Braun, W. J. (2011). Data analysis and graphics using r. An example-based approach. Cambridge University Press, Cambridge.\n\n\n[2] Lindenmayer, D., Viggers, K., Cunningham, R. and Donnelly, C. (1995). Morphological variation among populations of the mountain brushtail possum, trichosurus-caninus ogilby (phalangeridae, marsupialia). Australian Journal of Zoology 43 449‚Äì58.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Datasets</span>"
    ]
  }
]