[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression 26 Spring",
    "section": "",
    "text": "Preface\nThese are notes for STAT 3113 Regression 2026 Spring at Arkansas Tech University. If you have any questions please contact me through email.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Regression 26 Spring",
    "section": "References",
    "text": "References\n\n\n[1] Lindenmayer, D.,\nViggers, K., Cunningham, R. and Donnelly, C. (1995). Morphological variation among\npopulations of the mountain brushtail possum, trichosurus-caninus ogilby\n(phalangeridae, marsupialia). Australian Journal of Zoology\n43 449‚Äì58.\n\n\n[2] Belsley, D. A.,\nKuh, E. and Welsch, R. E. (1980). Regression\ndiagnostics: Identifying influential data and sources of\ncollinearity. John Wiley & Sons, New York.\n\n\n[3] Quinlan, J. R.\n(1993). Combining instance-based and model-based learning. In\nProceedings of the tenth international conference on machine\nlearning pp 236‚Äì43. Morgan Kaufmann, Amherst, Massachusetts.\n\n\n[4] Harrison, D. and\nRubinfeld, D. L. (1978). Hedonic housing\nprices and the demand for clean air. Journal of Environmental\nEconomics and Management 5 81‚Äì102.\n\n\n[5] Wilkinson, G. N.\nand Rogers, C. E. (1973). Symbolic description of factorial\nmodels for analysis of variance. Applied Statistics\n22 392.\n\n\n[6] Wickham, H.\n(2014). Tidy data.\nJournal of Statistical Software 59.\n\n\n[7] Hogg, R. V.,\nMcKean, J. W. and Craig, A. T. (2020). Introduction to\nmathematical statistics. Pearson, Harlow, Essex, United\nKingdom.\n\n\n[8] Mendenhall, W.\nand Sincich, T. (2020). A second\ncourse in statistics: Regression analyisis. Pearson Education, Inc,\n[Hoboken, New Jersey].\n\n\n[9] Healy, K.\n(2025). Gssrdoc: General\nsocial survey documentation for use in r.\n\n\n[10] Maindonald, J.\nH. and Braun, W. J. (2011). Data\nanalysis and graphics using r. An example-based approach. Cambridge\nUniversity Press, Cambridge.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/1/basics.html",
    "href": "contents/1/basics.html",
    "title": "1¬† Basics",
    "section": "",
    "text": "1.1 Quick overview\nGiven a set of observations of variables, we would like to understand the relationships among these variables.\nThe standard procedure for developing a statistical model is as follows:\nThere are two primary goals and related ways to assess the model:\nBoth Statistical Learning and specific methods like Regression Analysis may use both assessment strategies, but the emphasis differs. In this course, the focus is on the principles of modeling and statistical inference, with prediction discussed but not treated as the primary objective.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#quick-overview",
    "href": "contents/1/basics.html#quick-overview",
    "title": "1¬† Basics",
    "section": "",
    "text": "Proposing a model informed by exploratory data analysis and relevant domain knowledge.\nEstimating the parameters of the proposed model using the available data.\nAssessing the adequacy and quality of the fitted model.\nRevising the model based on the assessment results.\n\n\n\nPrediction (Generalization): We evaluate the fitted model on unseen data, using metrics like MSE or classification error. This assesses the model‚Äôs generalization ability, which is the primary focus of subjects like Statistical Learning or Machine Learning.\nExplanation (Inference): We use measures such as \\(R^2\\) to assess the goodness-of-fit of the fitted model, and we use confidence intervals and p-values to perform statistical inference about population parameters. Although these calculations are based on the observed sample, they are interpreted as statements about the population. This approach focuses on understanding relationships among variables and is central to traditional Statistical Modeling, including Regression Analysis.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#population-vs-sample",
    "href": "contents/1/basics.html#population-vs-sample",
    "title": "1¬† Basics",
    "section": "1.2 Population vs Sample",
    "text": "1.2 Population vs Sample\n\nPopulation: the set of all elements of interest in a particular study.\nSample: is a subset of the population\nStatistical Inference: The process of using data collected on a sample to draw conclusions about a population is called statistical inference.\n\nA population is described by parameters (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\beta\\)). A sample produces statistics (e.g., \\(\\bar X\\), \\(s\\), \\(\\hat\\beta\\)), which vary across repeated samples. Statistical inference uses the sampling distribution of these statistics to draw conclusions about population parameters. This part will be revisited later in Chapter 3.\n\nExample 1.1 (gssr) ¬†\n\n\nClick to expand.\n\nAccording to a previous national media report, the average age of U.S. adults who watch national TV news is 60 years. A data analyst hypothesizes that the average age of frequent TV-news watchers is greater than 60. To test this, she uses the General Social Survey (GSS), accessible via the gssr R package [1], to draw a sample of 500 respondents and record their ages.\n\nDescribe the population: All U.S. adults who watch TV news several times a week or more often, as defined by the GSS survey question on TV-news frequency.\nDescribe the sample: A random sample of 500 GSS respondents who meet the above criterion and have valid age data.\nSample size: \\(n = 500\\)\nDescribe the statistical inference: We want to determine whether the mean age of frequent TV-news watchers is greater than 60 based on the sampled data.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#data-structure",
    "href": "contents/1/basics.html#data-structure",
    "title": "1¬† Basics",
    "section": "1.3 Data structure",
    "text": "1.3 Data structure\nA dataset is a collection of values. Values are organized in two ways. Every value belongs to a variable and an observation.\n\nDefinition 1.1 (Variables and Observations [2]) ¬†\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units.\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\n\n\n\nExample 1.2 (possum) ¬†\n\n\nClick to expand.\n\nResearchers collected body measurements for bushtail possums in Eastern Australia. They trapped 104 possums and recorded location where possum was trapped, age, gender, head length, skull width, total length, tail length, etc. for each possum. For more details see Section A.1.\n\nlibrary(DAAG)\ndata(possum)\nhead(possum)\n##     case site Pop sex age hdlngth skullw totlngth taill footlgth earconch  eye\n## C3     1    1 Vic   m   8    94.1   60.4     89.0  36.0     74.5     54.5 15.2\n## C5     2    1 Vic   f   6    92.5   57.6     91.5  36.5     72.5     51.2 16.0\n## C10    3    1 Vic   f   6    94.0   60.0     95.5  39.0     75.4     51.9 15.5\n## C15    4    1 Vic   f   6    93.2   57.1     92.0  38.0     76.1     52.2 15.2\n## C23    5    1 Vic   f   2    91.5   56.3     85.5  36.0     71.0     53.2 15.1\n## C24    6    1 Vic   f   1    93.1   54.8     90.5  35.5     73.2     53.6 14.2\n##     chest belly\n## C3   28.0    36\n## C5   28.5    33\n## C10  30.0    34\n## C15  28.0    34\n## C23  28.5    33\n## C24  30.0    32\n\nThe dimension of the dataset is\n\ndim(possum)\n## [1] 104  14\n\nIn this example, there are 14 variables, and 104 observations. Each column is a variable. In R you can use the column name to get access to the variable.\n\npossum$hdlngth\n##   [1]  94.1  92.5  94.0  93.2  91.5  93.1  95.3  94.8  93.4  91.8  93.3  94.9\n##  [13]  95.1  95.4  92.9  91.6  94.7  93.5  94.4  94.8  95.9  96.3  92.5  94.4\n##  [25]  95.8  96.0  90.5  93.8  92.8  92.1  92.8  94.3  91.4  90.6  94.4  93.3\n##  [37]  89.3  92.4  84.7  91.0  88.4  85.3  90.0  85.1  90.7  91.4  90.1  98.6\n##  [49]  95.4  91.6  95.6  97.6  93.1  96.9 103.1  99.9  95.1  94.5 102.5  91.3\n##  [61]  95.7  91.3  92.0  96.9  93.5  90.4  93.3  94.1  98.0  91.9  92.8  85.9\n##  [73]  82.5  88.7  93.8  92.4  93.6  86.5  85.8  86.7  90.6  86.0  90.0  88.4\n##  [85]  89.5  88.2  98.5  89.6  97.7  92.6  97.8  90.7  89.2  91.8  91.6  94.8\n##  [97]  91.0  93.2  93.3  89.5  88.6  92.4  91.5  93.6\n\n\n\n\nDefinition 1.2 (Quantitative and Qualitative [3]) ¬†\n\nQuantitative data are observations measured on a naturally occurring numerical scale. It is also called numerical data.\nQualitative data are nonnumerical data that can only be classified into one of a group of categories. It is also called categorical data.\n\n\nIn regression, qualitative variables must be encoded using indicator (dummy) variables.\n\nDefinition 1.3 (Discrete and Continuous [3]) ¬†\n\nDiscrete random variable: A random variable that assumes either a finite number of values or an infinite sequence of values such as 0, 1, 2‚Ä¶\nContinuous random variable: A random variable that may assume any numerical value in an interval or collection of intervals.\n\n\n\n\n\n\n\ngraph TD\n    A[All Variables] --&gt; B[Numerical]\n    A --&gt; C[Categorical]\n    B --&gt; D[Continuous]\n    B --&gt; E[Discrete]\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom variables\n\n\n\nA variable in a dataset can be modeled by a random variable. The probability density function / probability mass function of the random variable can describe the distribution of all possible values of the variable in a dataset.\nMaking a measurement is equivalent to taking a sample from the corresponding random variable. This will be discussed in detail in Chapter 3.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/basics.html#data-visualization",
    "href": "contents/1/basics.html#data-visualization",
    "title": "1¬† Basics",
    "section": "1.4 Data Visualization",
    "text": "1.4 Data Visualization\n\n1.4.1 Qualitative (categorical) data\nUsually the most important is the class relative frequency: \\[\n\\text{class relative frequency}=\\frac{\\text{class frequency}}{n}.\n\\]\nTo display it, we could use table, bar chart or pie chart.\n\nExample 1.3 (possum) ¬†\n\n\nClick to expand.\n\nHere we still consider the possum dataset.\n\nlibrary(DAAG)\ndata(possum)\nhead(possum)\n##     case site Pop sex age hdlngth skullw totlngth taill footlgth earconch  eye\n## C3     1    1 Vic   m   8    94.1   60.4     89.0  36.0     74.5     54.5 15.2\n## C5     2    1 Vic   f   6    92.5   57.6     91.5  36.5     72.5     51.2 16.0\n## C10    3    1 Vic   f   6    94.0   60.0     95.5  39.0     75.4     51.9 15.5\n## C15    4    1 Vic   f   6    93.2   57.1     92.0  38.0     76.1     52.2 15.2\n## C23    5    1 Vic   f   2    91.5   56.3     85.5  36.0     71.0     53.2 15.1\n## C24    6    1 Vic   f   1    93.1   54.8     90.5  35.5     73.2     53.6 14.2\n##     chest belly\n## C3   28.0    36\n## C5   28.5    33\n## C10  30.0    34\n## C15  28.0    34\n## C23  28.5    33\n## C24  30.0    32\n\nThe variable Pop (location where possum was trapped) is qualitative. We could use the following ways to display it. We first compute the frequency table of the variable Pop:\n\ntable(possum$Pop)\n## \n##   Vic other \n##    46    58\n\nor relative frequency table:\n\ntable(possum$Pop)/length(possum$Pop)\n## \n##       Vic     other \n## 0.4423077 0.5576923\n\nThen we could draw the barplot of this variable:\n\nbarplot(table(possum$Pop))\n\n\n\n\n\n\n\n\nand the pie plot:\n\npie(table(possum$Pop))\n\n\n\n\n\n\n\n\nNote that table is handling the statistics, while barplot and pie draw on top of the result from table.\n\n\n\n\n1.4.2 Quantitative (numerical) data\nFor quantitative data, we would like to see the histogram, as well as computing some statistics: min, max, quartiles, median and mean.\n\nHistogram is an approximation of the distribution. In other words, we split the range into small segments (called bins), and count the frequency or relative frequency of data falling into these bins.\nBox plot consists of a box, two lines and possibly some points:\n\nThe box in the box plot extends from the lower quartile to the upper quartile. The difference between the upper quartile and the lower quartile is called the inter-quartile range (IQR).\nThe lines, known as whiskers, extend to one and a half times the interquartile range, but they stop at the most extreme data points that fall within this range.\nThe points, considered as outliers, are those which are not covered by the box and the lines.\n\n\nHistograms and boxplots help check normality and outliers before fitting regression. These summaries approximate the underlying distribution of the variable. In regression, we later apply similar tools to the residuals to check model assumptions.\n\nExample 1.4 (possum) ¬†\n\n\nClick to expand.\n\nWe still consider the possum dataset. We could use summary to compute the major statistics.\n\nlibrary(DAAG)\ndata(possum)\nsummary(possum)\n##       case             site          Pop     sex         age       \n##  Min.   :  1.00   Min.   :1.000   Vic  :46   f:43   Min.   :1.000  \n##  1st Qu.: 26.75   1st Qu.:1.000   other:58   m:61   1st Qu.:2.250  \n##  Median : 52.50   Median :3.000                     Median :3.000  \n##  Mean   : 52.50   Mean   :3.625                     Mean   :3.833  \n##  3rd Qu.: 78.25   3rd Qu.:6.000                     3rd Qu.:5.000  \n##  Max.   :104.00   Max.   :7.000                     Max.   :9.000  \n##                                                     NA's   :2      \n##     hdlngth           skullw         totlngth         taill      \n##  Min.   : 82.50   Min.   :50.00   Min.   :75.00   Min.   :32.00  \n##  1st Qu.: 90.67   1st Qu.:54.98   1st Qu.:84.00   1st Qu.:35.88  \n##  Median : 92.80   Median :56.35   Median :88.00   Median :37.00  \n##  Mean   : 92.60   Mean   :56.88   Mean   :87.09   Mean   :37.01  \n##  3rd Qu.: 94.72   3rd Qu.:58.10   3rd Qu.:90.00   3rd Qu.:38.00  \n##  Max.   :103.10   Max.   :68.60   Max.   :96.50   Max.   :43.00  \n##                                                                  \n##     footlgth        earconch          eye            chest          belly      \n##  Min.   :60.30   Min.   :40.30   Min.   :12.80   Min.   :22.0   Min.   :25.00  \n##  1st Qu.:64.60   1st Qu.:44.80   1st Qu.:14.40   1st Qu.:25.5   1st Qu.:31.00  \n##  Median :68.00   Median :46.80   Median :14.90   Median :27.0   Median :32.50  \n##  Mean   :68.46   Mean   :48.13   Mean   :15.05   Mean   :27.0   Mean   :32.59  \n##  3rd Qu.:72.50   3rd Qu.:52.00   3rd Qu.:15.72   3rd Qu.:28.0   3rd Qu.:34.12  \n##  Max.   :77.90   Max.   :56.20   Max.   :17.80   Max.   :32.0   Max.   :40.00  \n##  NA's   :1\n\nThe variable hdlngth (head length) is quantitative. We first show the histogram. We could use breaks to control the number of bins. Note that the function hist does not only draw the histogram; it also provides many useful pieces of information.\n\nres &lt;- hist(possum$hdlngth, breaks=10)\n\n\n\n\n\n\n\nres\n## $breaks\n##  [1]  82  84  86  88  90  92  94  96  98 100 102 104\n## \n## $counts\n##  [1]  1  6  2 12 22 27 22  7  3  0  2\n## \n## $density\n##  [1] 0.004807692 0.028846154 0.009615385 0.057692308 0.105769231 0.129807692\n##  [7] 0.105769231 0.033653846 0.014423077 0.000000000 0.009615385\n## \n## $mids\n##  [1]  83  85  87  89  91  93  95  97  99 101 103\n## \n## $xname\n## [1] \"possum$hdlngth\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\nThen we could show the box plot. You may compare the lines in the plot with the summary table and the histogram above.\n\nboxplot(possum$hdlngth)\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.3 Relations among multiple variables\nWe could show the relation between two variables in a scatterplot. Scatterplots help us assess linearity, detect outliers, and decide whether transformation is needed.\n\nExample 1.5 (possum) ¬†\n\n\nClick to expand.\n\nWhen both variables are numerical and continuous:\n\nplot(possum$hdlngth, possum$skullw)\n\n\n\n\n\n\n\n\nWhen one variable is categorical:\n\nplot(as.factor(possum$Pop), possum$skullw)\n\n\n\n\n\n\n\n\nNote that in this case, the categorical data has to be a factor. And once it is cast into a factor, the plot is multiple box plots for each category. This type of grouped boxplot corresponds to comparing group means and is directly related to one-way ANOVA and regression with dummy variables. It will be discussed in later sections.\nWe can see pair plots for each pair of variables. Pair plots are very important because they help reveal relationships among predictors. Note that before creating the plot, we have to cast pop and sex into factors.\n\npossum$Pop &lt;- as.factor(possum$Pop)\npossum$sex &lt;- as.factor(possum$sex)\npairs(possum)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Healy, K. (2025). Gssrdoc: General social survey documentation for use in r.\n\n\n[2] Wickham, H. (2014). Tidy data. Journal of Statistical Software 59.\n\n\n[3] Mendenhall, W. and Sincich, T. (2020). A second course in statistics: Regression analyisis. Pearson Education, Inc, [Hoboken, New Jersey].",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html",
    "href": "contents/1/prob.html",
    "title": "2¬† Probability",
    "section": "",
    "text": "2.1 Notations",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#notations",
    "href": "contents/1/prob.html#notations",
    "title": "2¬† Probability",
    "section": "",
    "text": "\\(Y\\): a random variable (captial letters)\n\\(y\\): a sample of \\(Y\\)\n\\(\\Pr\\qty(Y\\in A\\mid\\theta)\\): the probability of \\(Y\\) being in \\(A\\)\n\\(p(y\\mid\\theta)=\\Pr\\qty(Y=y\\mid\\theta)\\): the probability mass function (discrete case)\n\\(f(y\\mid\\theta)=\\displaystyle\\dv{y}\\Pr\\qty(Y\\leq y\\mid\\theta)\\): the probability density function (continuous case)\n\\(\\Exp\\qty(Y)\\): the expectation of \\(Y\\)\n\\(\\Var\\qty(Y)\\): the variance of \\(Y\\)",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#random-variables",
    "href": "contents/1/prob.html#random-variables",
    "title": "2¬† Probability",
    "section": "2.2 Random variables",
    "text": "2.2 Random variables\n\nDefinition 2.1 (Expectation) \\[\n\\Exp\\mqty[u(X)] = \\int_{-\\infty}^{\\infty}u(x)f(x)\\dl3x.\n\\]\n\n\nDefinition 2.2 ¬†\n\n\\(\\mu=\\Exp(X)\\) is called the mean value of \\(X\\).\n\\(\\sigma^2=\\Var(X)=\\Exp\\mqty[(X-\\mu)^2]\\) is called the variance of \\(X\\).\n\\(M_X(t)=\\Exp\\mqty[\\me^{tX}]\\) is called the moment generating function of \\(X\\).\n\n\n\nProposition 2.1 ¬†\n\n\\(\\Exp\\mqty[ag(X)+bh(X)]=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)]\\).\n\\(\\Var\\mqty[X]=\\Exp\\mqty[(X-\\mu)^2]=\\Exp(X^2)-\\mu^2\\).\nIf \\(X\\) and \\(Y\\) are independent, \\(\\Var\\mqty[aX+bY]=a^2\\Var(X)+b^2\\Var(Y)\\).\n\n\n\nClick for proof.\n\n\\[\n\\begin{split}\n\\Exp\\mqty[ag(X)+bh(X)]&=\\int_{-\\infty}^{\\infty}\\mqty[ag(x)+bh(x)]f(x)\\dl3x\\\\\n                 &=a\\int_{-\\infty}^{\\infty}g(x)f(x)\\dl3x+b\\int_{-\\infty}^{\\infty}h(x)f(x)\\dl3x\\\\\n                 &=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)].\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp\\mqty[(X-\\mu)^2]&=\\Exp\\mqty[\\qty(X^2-2\\mu X+\\mu^2)]=\\Exp(X^2)-2\\mu\\Exp(X)+\\Exp(\\mu^2)\\\\\n&=\\Exp(X^2)-2\\mu\\mu+\\mu^2=\\Exp(X^2)-\\mu^2.\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Var\\mqty[aX]&=\\Exp(a^2X^2)-a^2\\mu^2=a^2\\qty(\\Exp(X^2)-\\mu^2)=a^2\\Var(X),\\\\\n\\Var\\mqty[X+Y]&=\\Exp((X+Y)^2)-(\\Exp(X+Y))^2\\\\\n&=\\Exp(X^2)+\\Exp(Y^2)+2\\Exp(XY)-\\Exp(X)^2-\\Exp(Y)^2-2\\Exp(X)\\Exp(Y)\\\\\n&=\\Var(X)+\\Var(Y)+2(E(XY)-E(X)E(Y))\\\\\n&=\\Var(X)+\\Var(Y),\\\\\n\\Var\\mqty[aX+bY]&=a^2\\Var(X)+b^2\\Var(Y).\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nAssume \\(X_1,\\ldots, X_n\\) i.i.d. with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then \\(\\Var(\\frac1n\\sum X_i)=\\sigma^2/n\\). This implies that the more samples you pick, the smaller the variance is. This explains why, when possible, we want a large sample size. Note that we don‚Äôt specify any concrete distribution in this remark. This is related to estimation, which will be discussed in detail later.\n\n\n\n2.2.1 R code\nR has built-in random variables with different distributions. The naming convention is a prefix d-, p-, q- and r- together with the name of distribution.\n\nd-: density function of the given distribution;\np-: cumulative density function of the given distribution;\nq-: quantile function of the given distribution (which is the inverse of p- function);\nr-: random sampling from the given distribution.\n\n\nExample 2.1 (Normal distribution) ¬†\n\n\nClick to expand.\n\n\nx &lt;- seq(-4, 4, length=100)\ny &lt;- dnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n\n\n\n\n\n\n\n\n\nx &lt;- seq(-4, 4, length=100)\ny &lt;- pnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n\n\n\n\n\n\n\n\n\nqnorm(0.025)\n## [1] -1.959964\nqnorm(0.5)\n## [1] 0\nqnorm(0.975)\n## [1] 1.959964\n\n\nrnorm(10)\n##  [1]  0.3832064  0.4811536  0.9407225  0.3473203  0.9705847  0.8000386\n##  [7]  0.2221451  1.1320445 -1.9443368 -0.3623345",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#some-relations",
    "href": "contents/1/prob.html#some-relations",
    "title": "2¬† Probability",
    "section": "2.3 Some relations",
    "text": "2.3 Some relations\n\n2.3.1 Covariance\nAssuming we have two random variables \\(X\\) and \\(Y\\), and we have two sets of realizations \\(x_1,\\ldots, x_n\\) and \\(y_1,\\ldots, y_n\\).\nThe sum of squares and cross-products are defined as\n\n\\(SS_{xx}=\\sum_{i=1}^n(x_i-\\bar x)^2\\)\n\\(SS_{yy}=\\sum_{i=1}^n(y_i-\\bar y)^2\\)\n\\(SS_{xy}=\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)\\)\n\nwhere \\(\\bar x\\) and \\(\\bar y\\) are sample mean. Note that if we normalize these quantities by \\(n-1\\), we will get the usual unbiased sample varaince and covariance.\n\nDefinition 2.3 (Covariance and sample covariance) ¬†\n\n\\(\\Cov(X,Y)=\\Exp[(X-\\mu_X)(Y-\\mu_Y)]\\), where \\(\\mu_X=\\Exp(X)\\) and \\(\\mu_Y=\\Exp(Y)\\).\n\\(s_{xy}=\\frac1{n-1}SS_{xy}=\\frac1{n-1}\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)\\).\n\n\n\n\n2.3.2 Pearson correlation coefficient\n\nDefinition 2.4 (Pearson correlation coefficient \\(r\\)) \\[\nr=\\cor(x,y)=\\frac{SS_{xy}}{\\sqrt{{SS_{xx}SS_{yy}}}}.\n\\]\n\n\nTheorem 2.1 (Properties of Correlation) ¬†\n\n\\(-1\\leq r\\leq 1\\).\n\n\\(r&gt;0\\) means positive correlation.\n\\(r&lt;0\\) means negative correlation.\n\\(r=0\\) means no positive or negative linear correlation.\n\n\\(\\cor(x,y)=\\cor(y, x)\\)\nThe correlation coefficient \\(r\\) is scaleless (which means that it doesn‚Äôt depend on the units of measurement used for the variables).\n\n\nThe Pearson correlation coefficient is used to measure the correlation of the two variables. If the relation is more closely stick to a straight line, the relation is stronger, and the resulted \\(r\\) will be bigger. If \\(r=1\\), \\(X\\) and \\(Y\\) will have a perfectly linear relation. This is not the same as the slope.\n\nExample 2.2 Let \\(x_i=i\\) and \\(y_i=0.001x_i\\). Then the slope is \\(0.001\\) and the \\(r=1\\). We may add a small noise to it so \\(y_i=0.001x_i+\\varepsilon_i\\) while \\(\\varepsilon_i\\sim N(0,0.001)\\). Then the slope is \\(0.001\\) but \\(r\\) is still close to \\(1\\).\n\nx &lt;- seq(1, 100)\ny &lt;- 0.001 * x + rnorm(100, 0, 1e-3)\nplot(x, y)\nabline(0, 0.001, col='red')\n\n\n\n\n\n\n\ncor(x, y)\n## [1] 0.9995427\n\n\n\n\n2.3.3 Independence\nIndependence is the relation between two random variables.\n\nDefinition 2.5 (Independence) Random variables \\(X\\) and \\(ùëå\\) are said to be independent if their joint distribution factorizes into the product of their marginal distributions. That is, for all \\(x\\), \\(y\\) \\[\nF_{X,Y}(x,y)=F_X(x)F_Y(y).\n\\]\nIf a joint density exists, this is equivalently written as\n\\[\nf_{X,Y}(x,y)=f_X(x)f_Y(y).\n\\]\nIt is equivalent to say that for all measurable functions \\(g\\) and \\(h\\),\n\\[\n\\Exp[g(X)h(Y)]=\\Exp[g(X)]\\Exp[h(Y)].\n\\]\n\nIndependence concerns the entire joint distribution, not just a summary statistic. Specifically, it cannot be derived from \\(\\Cov\\) and \\(\\cor\\).\n\nTheorem 2.2 If \\(X\\) and \\(Y\\) are independent, then \\(\\Cov(X,Y)=0\\) and \\(\\cor(X,Y)=0\\). The converse is not true in general.\n\n\\(\\Cov(X,Y)=0\\) (equivalently, \\(\\cor(X,Y)=0\\)) means that \\(X\\) and \\(Y\\) have no positive or negative linear relationship. However, this does not imply that \\(X\\) and \\(Y\\) are unrelated; they may still exhibit a nonlinear dependence.\n\nExample 2.3 ¬†\n\nx &lt;- seq(-10, 10)\ny &lt;- abs(x)\ncov(x, y)\n## [1] 0\ncor(x, y)\n## [1] 0\nplot(x, y)\n\n\n\n\n\n\n\n\nThey are not independent because \\(y\\) is literally defined as a function of \\(x\\). But their covariance and correlation are \\(0\\)s.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#some-important-distributions",
    "href": "contents/1/prob.html#some-important-distributions",
    "title": "2¬† Probability",
    "section": "2.4 Some important distributions",
    "text": "2.4 Some important distributions\n\n2.4.1 Normal Distribution\n\nTheorem 2.3 (Normal Sample Mean‚ÄìVariance) Let \\(X_1,\\ldots,X_n\\sim N(\\mu,\\sigma^2)\\) i.i.d. Define\n\nSample mean: \\(\\bar X=\\frac1n\\sum_{i=1}^nX_i\\)\nSample variance: \\(s^2=\\frac1{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2\\)\n\nThen\n\n\\(\\bar X\\sim N(\\mu,\\frac{\\sigma^2}n)\\)\n\\(\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)\n\\(\\bar X\\) and \\(s^2\\) are independent.\n\n\nThe complete proof is lengthy and out of scope of the course. Please refer to [1] for details.\n\n\n2.4.2 Student‚Äôs t-Distribution\n\nTheorem 2.4 (Student‚Äôs t-Distribution [1]) Let\n\n\\(Z\\sim N(0,1)\\) be a standard normal random variable\n\\(U\\sim \\chi_{\\nu}^2\\) be a chi-square random variable with \\(\\nu\\) degrees of freedom.\n\\(Z\\) and \\(U\\) are independent.\n\nThen \\[\nT=\\frac{Z}{\\sqrt{U/\\nu}}\n\\] has a Student‚Äôs t-distribution with \\(\\nu\\) degrees of freedom: \\(T\\sim t_{\\nu}\\).\n\nThese two theorems are usually used together. Let \\(X_1,\\ldots,X_n\\sim N(\\mu,\\sigma^2)\\) i.i.d. Then\n\n\\(\\bar X\\sim N(\\mu,\\sigma^2/n)\\);\n\\(U=\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\).\n\nTherefore\n\n\\(Z=\\frac{\\bar X-\\mu}{\\sigma/\\sqrt n}\\sim N(0,1)\\);\n\\(Z\\) and \\(U\\) are indepedent.\n\nSo \\[\nT=\\frac{Z}{\\sqrt{U/\\nu}}=\\frac{\\frac1{\\sigma/\\sqrt n}(\\bar X-\\mu)}{\\sqrt{\\frac{(n-1)s^2}{\\sigma^2}/(n-1)}}=\\frac{\\bar X-\\mu}{s/\\sqrt n}\\sim t_{n-1}.\n\\]\nIn other words, if we standardize the sample mean using the sample standard deviation, we obtain a statistic that follows a t-distribution. This result is mainly used in the t-test.\n\n\n\n\n\n\nNote\n\n\n\nThe normal distribution and the t-distribution are both bell-shaped and symmetric. When the sample size is large (typically (n &gt; 30)), the t-distribution becomes very similar to the standard normal distribution, so the normal approximation is usually acceptable. When the sample size is small, however, the t-distribution has noticeably heavier tails, and it is better to use the t-distribution directly for inference.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/prob.html#miscs",
    "href": "contents/1/prob.html#miscs",
    "title": "2¬† Probability",
    "section": "2.5 Miscs",
    "text": "2.5 Miscs\n\nTheorem 2.5 (Empirical Rule (Normal Data)) If a random variable \\(X\\) is approximately normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then:\n\nAbout \\(68\\%\\) of observations lie within \\(1\\sigma\\) of the mean: [ ]\nAbout \\(95\\%\\) of observations lie within \\(2\\sigma\\) of the mean: [ ]\nAbout \\(99.7\\%\\) of observations lie within \\(3\\sigma\\) of the mean: [ ]\n\nThis is commonly known as the 68‚Äì95‚Äì99.7 rule.\n\n\nRemark 2.1 (Precision of the 95% Rule). The statement that ‚Äúabout \\(95\\%\\) of observations lie within \\(2\\sigma\\) of the mean‚Äù is an approximation.\nIf [ X N(,^2), ] then the exact probability statement is [ P(- 1.96X + 1.96) = 0.95. ]\nThus, for normally distributed data:\n\nThe exact 95% cutoff is \\(\\pm 1.96\\sigma\\)\nThe value \\(\\pm 2\\sigma\\) is a convenient rounding\n\n\n\nRemark 2.2 (Why \\(\\boldsymbol{2\\sigma}\\) Is Still Used). The cutoff \\(\\pm 2\\sigma\\) is widely used because:\n\nIt is easy to remember and communicate.\nThe difference from \\(1.96\\sigma\\) is small: [ P(|Z|) . ]\nIt provides a useful mental model of variability.\n\nIn contrast, the value \\(1.96\\) arises naturally in formal statistical inference, such as confidence intervals and hypothesis testing.\n\n\nRemark 2.3 (Rule-of-Thumb vs.¬†Exact Normal Theory). \n\nThe 68‚Äì95‚Äì99.7 rule is a descriptive heuristic.\nThe values \\(1.645\\), \\(1.96\\), and \\(2.576\\) come from exact normal quantiles.\n\nUnderstanding this distinction is essential for: - Confidence intervals - Hypothesis testing - Interpretation of statistical uncertainty\n\n\n\n\n\n[1] Hogg, R. V., McKean, J. W. and Craig, A. T. (2020). Introduction to mathematical statistics. Pearson, Harlow, Essex, United Kingdom.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html",
    "href": "contents/1/inferences.html",
    "title": "3¬† Inferences",
    "section": "",
    "text": "3.1 Inferential statistics",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html#inferential-statistics",
    "href": "contents/1/inferences.html#inferential-statistics",
    "title": "3¬† Inferences",
    "section": "",
    "text": "Definition 3.1 (Population and sample [1]) ¬†\n\nA population data set is a collection (or set) of data measured on all experimental units of interest to you.\nA sample is a subset of data selected from a population.\nA random sample of \\(n\\) experimental units is one selected from the population in such a way that every different sample of size \\(n\\) has an equal probability of selection.\n\n\n\nDefinition 3.2 (Statistical inference [1]) ¬†\n\nA statistical inference is an estimate, prediction, or some other generatlization about a population based on information contianed in a sample.\nA measure of reliability is a statement about the degree of uncertainty associated with a statistical inference.\n\n\n\n\n\n\n\n\nNoteInferential statistics\n\n\n\n\nIdentify population\nIdentify variable(s)\nCollect sample data\nInference about population based on sample\nMeasure of reliability for inference",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html#estimators",
    "href": "contents/1/inferences.html#estimators",
    "title": "3¬† Inferences",
    "section": "3.2 Estimators",
    "text": "3.2 Estimators\nThis section is based on [2, Chapter 4].\n\n3.2.1 Sampling\nConsider a random variable \\(X\\) with an unknown distribution. Our information about the distribution of \\(X\\) comes from a sample on \\(X\\): \\(\\qty{X_1,\\ldots,X_n}\\).\n\nThe sample ovservations \\(\\qty{X_1,\\ldots,X_n}\\) have the same distribution as \\(X\\).\n\\(n\\) denotes the sample size.\nWhen the sample is actually drawn, we use \\(x_1,\\ldots,x_n\\) as the realizations of the sample.\n\n\nDefinition 3.3 (Random sample) If the random variables \\(X_1,\\ldots, X_n\\) are i.i.d, then these random variable constitute a random sample of size \\(n\\) from the common distribution.\n\n\nDefinition 3.4 (Statistics) Let \\(X_1,\\ldots,X_n\\) denote a sample on a random variable \\(X\\). Let \\(T=T(X_1,\\ldots,X_n)\\) be a function of the sample. \\(T\\) is called a statistic. Once a sample is drawn, \\(t=T(x_1,\\ldots,x_n)\\) is called the realization of \\(T\\).\n\n\nDefinition 3.5 (Sampling distribution) ¬†\n\nThe distribution of \\(T\\) is called the sampling distribution.\nThe standard deviation of the sampling distribution is called the standard error of estimate.\n\n\n\nTheorem 3.1 (The Central Limit Theorem) For large sample sizes, the sample mean \\(\\bar{X}\\) from a population with mean \\(\\mu\\) and a standard deviation \\(\\sigma\\) has a sampling distribution that is approximately normal, regardless of the probability distribution of the sampled population.\n\n\n\n3.2.2 Point estimation\nAssume that the distribution of \\(X\\) is known down to an unknown parameter \\(\\theta\\) where \\(\\theta\\) can be a vector. Then the pdf of \\(X\\) can be written as \\(f(x;\\theta)\\). In this case we might find some statistic \\(T\\) to estimate \\(\\theta\\). This is called a point estimator of \\(\\theta\\). A realization \\(t\\) is called an estimate of \\(\\theta\\).\n\nDefinition 3.6 (Unbiasedness) Let \\(X_1,\\ldots,X_n\\) is a sample on a random varaible \\(X\\) with pdf \\(f(x;\\theta)\\). Let \\(T\\) be a statistic. We say that \\(T\\) is an unbiased estimator of \\(\\theta\\) if \\(E(T)=\\theta\\).\n\nThe typical example of estimators are sample mean and sample variance. Let \\(X\\) be a random variable, with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Consider a sample \\(\\set{X_i}\\) of size \\(n\\). By definition all \\(X_i\\)‚Äôs are i.i.d. Therefore \\(\\operatorname{E}\\qty(X_i)=\\mu\\), and \\(\\operatorname{Var}\\qty(X_i)=\\sigma^2\\) for any \\(i=1,\\ldots, n\\).\n\nTheorem 3.2 The following are the unbiased estimators of \\(\\mu\\) and \\(\\sigma^2\\) of \\(X\\).\n\n\\(\\bar{X}=\\dfrac1n\\sum_{i=1}^nX_i\\) is called the sample mean of the samples.\n\\(s^2=\\dfrac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2\\) is called the sample variance of the samples.\n\nThey are the unbiased estimators of \\(\\mu\\) and \\(\\sigma^2\\), respectively. \\[\n\\operatorname{E}(\\bar X)=\\mu,\\quad \\operatorname{E}(s^2)=\\sigma^2.\n\\]\n\n\nClick for proof.\n\n\\[\n\\begin{aligned}\n\\operatorname{E}\\qty(\\bar{X})&=\\operatorname{E}\\qty(\\frac1n\\sum_{i=1}^nX_i)=\\frac1n\\sum_{i=1}^n\\operatorname{E}\\qty(X_i)=\\frac1n\\sum_{i=1}^n\\mu=\\mu,\\\\\n\\operatorname{E}\\qty(s^2)&=\\frac{1}{n-1}\\operatorname{E}\\qty[\\sum_{i=1}^n(X_i-\\bar{X})^2]=\\frac{1}{n-1}\\sum_{i=1}^n\\operatorname{E}\\mqty[\\qty(X_i-\\bar{X})^2]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\operatorname{Var}\\qty(X_i-\\bar{X})+\\qty(\\operatorname{E}\\qty(X_i-\\bar{X}))^2)\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\operatorname{Var}\\qty(\\frac{n-1}{n}X_i-\\frac1nX_1-\\ldots-\\frac1nX_n)+\\qty(\\operatorname{E}\\qty(X_i)-\\operatorname{E}\\qty(\\bar{X}))^2)\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\frac{(n-1)^2}{n^2}\\operatorname{Var}\\qty(X_i)+\\frac1{n^2}\\operatorname{Var}\\qty(X_1)+\\ldots+\\frac1{n^2}\\operatorname{Var}\\qty(X_n))\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\qty(\\frac{(n-1)^2}{n^2}\\sigma^2+\\frac1{n^2}\\sigma^2+\\ldots+\\frac1{n^2}\\sigma^2)\\\\\n&=\\frac{n}{n-1}\\frac{(n-1)^2+n-1}{n^2}\\sigma^2=\\sigma^2.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease pay attention to the denominator of the sample variance. The \\(n-1\\) is due to the degree of freedom: all \\(X_i\\)‚Äôs and \\(\\bar{X}\\) are not independent to each other.\n\n\n\n\n3.2.3 Confidence intervals\n\nDefinition 3.7 (Confidence interval) Consider a sample of \\(X\\). Fix a number \\(0&lt;\\alpha&lt;1\\). Let \\(L\\) and \\(U\\) be two statistics. We say the interval \\((L,U)\\) is a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\theta\\) if\n\\[\n1-\\alpha=\\Pr[\\theta\\in(L,U)].\n\\]\n\n\nTheorem 3.3 (Large-Sample \\(100(1-\\alpha)\\%\\) Confidence interval) \\[\nL,U=\\bar{X}\\pm z_{\\alpha/2}\\qty(\\frac{s}{\\sqrt{n}}),\n\\] where \\(z_{\\alpha/2}=1.96\\) if \\(\\alpha=5\\%\\).\n\n\nFor any \\(n\\), if \\(X_i\\sim \\mathcal N(\\mu, \\sigma^2)\\), \\(T_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\) has a Student‚Äôs \\(t\\)-distribution of degree of freedom \\(n-1\\).\nWhen \\(n\\) is big enough, for any distribution \\(X_i\\), \\(Z_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\) is approximately \\(\\mathcal N(0,1)\\).\nStudent‚Äôs \\(t\\)-distribution of degree of freedom \\(n-1\\) is approaching \\(\\mathcal N(0,1)\\) when \\(n\\) is increasing. When \\(n=30\\) they are very close to each other. Therefore in many cases Statisticians require sample size \\(\\geq30\\).\nFor large sample or small sample, the coefficients to compute confidence intervals are \\(z_{\\alpha/2}\\) or \\(t_{\\alpha/2}\\). These two numbers come from normal distribution or Student‚Äôs \\(t\\)-distribution.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/1/inferences.html#hypothesis-test",
    "href": "contents/1/inferences.html#hypothesis-test",
    "title": "3¬† Inferences",
    "section": "3.3 Hypothesis test",
    "text": "3.3 Hypothesis test\nElements of a Statistical Test of Hypothesis\n\nNull Hypothesis \\(ùêª_0\\)\nAlternative Hypothesis \\(ùêª_ùëé\\)\nTest Statistic\nLevel of significance \\(\\alpha\\)\nRejection Region\n\\(ùëÉ\\)-Value\nConclusion\n\nGiven some data, we would like to know whether these data are ‚Äúexotic‚Äù enough‚Äìunder the assumption that the null hypothesis \\(H_0\\) is true‚Äìto justify rejecting \\(H_0\\). In other words, we compute the probability of obtaining a test statistic at least as extreme as the observed value, assuming \\(H_0\\) is true. This probability is called the p-value. Once the p-value is smaller than the chosen level of significance \\(\\alpha\\), we reject \\(H_0\\).\nConsider a test statistic \\(T(X)\\) and we observe \\(T(X)=t_{\\text{obs}}\\). Then\n\\[\np\\text{-value}=\\Pr(T(X)\\geq t_{\\text{obs}}\\mid H_0 \\text{ is true})\n\\] is the p-value for a right-tailed test. The key idea in constructing a hypohesis test is to choose a test statistic and a rejection region so that\n\\[\n\\Pr(\\text{the statistic falls in the rejection region}\\mid H_0\\text{ is true})=\\alpha.\n\\] This ensures the test has expected Type I error rate \\(\\alpha\\).\n\n\n\n\n\n\nNoteType I error vs Type II error\n\n\n\n\nType I error: rejecting \\(H_0\\) when \\(H_0\\) is actually true.\nType II error: failing to reject \\(H_0\\) when \\(H_0\\) is actually false.\n\nHypothesis test is designed to control Type I error rate (which is to reduce false positive rate, and which is to increase precision), since the significance level \\(\\alpha\\) is the probability of Type I errors.\n\\[\n\\alpha=\\Pr(\\text{reject }H_0\\mid H_0\\text{ is true}).\n\\]\nWhen using Hypothesis test, the scenario is usually that people capture some signals in order to prove an effect happens. The null hypothesis (\\(H_0\\)) is assumed to be the default case, and they want to make sure that once the signal is captured, the effect happens. In this case it is ok to miss some events that happens without the signal. In other words, people prioritize not making a false claim than missing an opportunity.\nWe could balance Type I and Type II errors by controlling \\(\\alpha\\).\n\nReduing \\(\\alpha\\) will make the test less likely to make Type I errors but increase the likelihood of Type II errors.\nIncreasing sample size reduces probability of making both types of errors, which can improve the test‚Äôs reliability.\n\n\n\n\n3.3.1 t-test\nA t-test is used to test a hypothesis about a population mean when the population standard deviation is unknown and the sample size is small or moderate. In our case, we consider the standard one-sample t-test: given a set of random observations, we want to determine whether the population mean of the underlying random variable is equal to 0 or not.\nAssume that the given values are \\(\\{x_1,x_2,\\ldots,x_n\\}\\), and the underlying random variable is \\(X\\sim N(\\mu, \\sigma^2)\\). The hypotheses are\n\n\\(H_0\\): \\(\\mu=0\\).\n\\(H_a\\): \\(\\mu\\neq0\\).\n\nThe values can be treated as realizations of i.i.d random variables \\(X_i\\)‚Äôs. We have the following statistics:\n\nSample mean: \\(\\bar{X}=\\frac1n\\sum_{i=1}^nX_i\\).\nSample standard deviation: \\(s=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2}\\)\nSample size: \\(n\\).\n\nThese statistics have sampling distributions that can be described exactly under the normality assumption.\nBased on the discussion in Section 2.4.2, we have that\n\n\\(Z=\\frac{\\bar X-\\mu}{\\sigma/\\sqrt n}\\sim N(0,1)\\)\n\\(U=\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)\n\\(Z\\) and \\(U\\) are independent, and the degree of freedom of \\(U\\) is \\(\\nu = n-1\\),\n\\(t=\\frac{Z}{\\sqrt{U/(n-1)}}\\sim t_{n-1}\\).\n\nSince the alternative hypothesis is \\(\\mu\\neq0\\), we need to consider both tails, corresponding to \\(\\mu&gt;0\\) and \\(\\mu&lt;0\\). Therefore the boundary of the rejection region is chosen so that each tail has probability \\(\\alpha/2\\). In other words, the critical value \\(t_{1-\\alpha/2,n-1}\\) satisfies, under \\(H_0\\), \\[\n\\Pr(|t|&gt;t_{1-\\alpha/2,n-1})=\\alpha.\n\\] This critical value \\(t_{1-\\alpha/2,n-1}\\) is usually found from a t-distribution table or via the inverse c.d.f. of the t-distribution.\n\n\n\n\n[1] Mendenhall, W. and Sincich, T. (2020). A second course in statistics: Regression analyisis. Pearson Education, Inc, [Hoboken, New Jersey].\n\n\n[2] Hogg, R. V., McKean, J. W. and Craig, A. T. (2020). Introduction to mathematical statistics. Pearson, Harlow, Essex, United Kingdom.",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html",
    "href": "contents/2/intro.html",
    "title": "4¬† Intro to regression",
    "section": "",
    "text": "4.1 Probabilistic model for \\(y\\)\nWe want to use sample data to investigate the relationships among a group of variables, ultimately to create a model for some variable that can be used to predict its value in the futre.\nLanguage:\nThe probabilistic model for \\(y\\) is \\[\ny=\\operatorname{E}(y)+\\text{Random error}.\n\\] It is called probabilistic since we can make a probability statement about the magnitude of the deviation between \\(y\\) and \\(\\operatorname{E}(y)\\).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Intro to regression</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#probabilistic-model-for-y",
    "href": "contents/2/intro.html#probabilistic-model-for-y",
    "title": "4¬† Intro to regression",
    "section": "",
    "text": "NoteGeneral Form of Probabilistic Model in Regression\n\n\n\n\\[\ny=\\operatorname{E}(y)+\\varepsilon\n\\] where\n\n\\(y\\): Dependent variable\n\\(\\operatorname{E}(y)\\): Mean value of \\(y\\)\n\\(\\varepsilon\\): Unexplainable or random error\n\nThis model suggests that y will come back to its mean evautually. This is why it is called regression model.\n\n\n\nDefinition 4.1 The variables used to predict \\(y\\) are called independent variables and are denoted by \\(x_i\\).\n\n\n\n\n\n\n\nTipRegression Modeling\n\n\n\n\nHypothesize the form of the model for \\(\\operatorname{E}(y)\\).\nCollect the sample data.\nUse the sample data to estimate unknown parameters in the model.\nSpecify the probability distribution of \\(\\varepsilon\\), and estimate any unknown parameters of this distribution.\nStatistically check the usefulness of the model.\nCheck the validity of the assumptions on \\(\\varepsilon\\), and make model modifications if necessary.\nWhen satisfied that the model is useful, and assumptions are met, use the model to make inferences.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Intro to regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html",
    "href": "contents/2/slr.html",
    "title": "5¬† Simple Linear regression",
    "section": "",
    "text": "5.1 SLR\nThe purpose of the section is to estimate \\(\\beta_0\\), \\(\\beta_1\\) as well as \\(\\sigma^2\\).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#slr",
    "href": "contents/2/slr.html#slr",
    "title": "5¬† Simple Linear regression",
    "section": "",
    "text": "Definition 5.1 (A First-Order Model) \\[\ny=\\beta_0+\\beta_1x+\\varepsilon\n\\] where\n\n\\(y\\): The response variable\n\\(x\\): The independent variable (variable used as a predictor of \\(y\\))\n\\(\\operatorname{E}(y)=\\beta_0+\\beta_1x\\): Deterministic component\n\\(\\varepsilon\\): Random error component\n\\(\\beta_0\\): \\(y\\)-intercept\n\\(\\beta_1\\): Slope\n\nFor \\(\\varepsilon\\), we have the following assumptions:\n\n\\(\\varepsilon\\) follows Normal distribution for all \\(x\\).\nThe mean is constant to be \\(0\\).\nThe variance is constant \\(\\sigma^2\\) (homoscedastic).\nThe errors associated with two observations are independent.\n\nIn other words, \\(\\varepsilon_i\\sim N(0,\\sigma^2)\\) i.i.d.\n\n\nDefinition 5.2 (Regression equation) Simple linear regression equation is \\[\n\\operatorname{E}(y)=\\beta_0+\\beta_1x.\n\\]\n\n\nDefinition 5.3 (Estimated regression equation) The estimated simple linear regression equation is \\[\n\\hat y=\\hat\\beta_0+\\hat\\beta_1x.\n\\] \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are estimators of \\(\\beta_0\\) and \\(\\beta_1\\).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#estimation",
    "href": "contents/2/slr.html#estimation",
    "title": "5¬† Simple Linear regression",
    "section": "5.2 Estimation",
    "text": "5.2 Estimation\n\n\n5.2.1 OLS estimators\nThe standard approach is known as ordinary least squares (OLS) estimation. In this method, model parameters are chosen to minimize the sum of squared residuals between the observed responses and the values predicted by the model. From an optimization perspective, OLS minimizes a loss function that measures the discrepancy between the data and the fitted model.\nHere are more details.\nConsider the linear regression model \\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i,\\quad i=1,\\ldots, n.\n\\]\nIn order to find the best parameters, we need to minimize the sum of squared errors \\[\nL(\\beta_0, \\beta_1) = \\sum_{i=1}^n\\varepsilon_i^2=\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1x_i)^2.\n\\] Since this loss can be treated as a function of \\(\\beta_0\\) and \\(\\beta_1\\), to minimize it, we could consider the critical point, which is\n\\[\n\\pdv{L}{\\beta_0}=0,\\quad \\pdv{L}{\\beta_1}=0.\n\\] In other words, we have\n\\[\n\\begin{split}\n\\pdv{L}{\\beta_0}&=\\sum_{i=1}^n2(y_i-\\beta_0-\\beta_1x_i)(-1)=-2\\qty(\\sum_{i=1}^ny_i-n\\beta_0-\\beta_1(\\sum_{i=1}^nx_i))=-2n(\\bar y-\\beta_0-\\beta_1\\bar x)=0,\\\\\n\\pdv{L}{\\beta_1}&=\\sum_{i=1}^n2(y_i-\\beta_0-\\beta_1x_i)(-x_i).\n\\end{split}\n\\] The first equation gives us \\(\\beta_0=\\bar y-\\beta_1\\bar x\\). Then the second equation is \\[\n\\begin{split}\n\\pdv{L}{\\beta_1}&=\\sum_{i=1}^n2(y_i-\\beta_0-\\beta_1x_i)(-x_i)=-2\\sum_{i=1}^n(y_i-\\bar y+\\beta_1\\bar x-\\beta_1x_i)(x_i)\\\\\n&=-2\\sum_{i=1}^n(y_i-\\bar y-\\beta_1(x_i-\\bar x))(x_i)=-2\\sum_{i=1}^n(y_i-\\bar y-\\beta_1(x_i-\\bar x))(x_i-\\bar x)\\\\\n&=-2\\qty(\\sum_{i=1}^n(y_i-\\bar y)(x_i-\\bar x)-\\beta_1\\sum_{i=1}^n(x_i-\\bar x)(x_i-\\bar x))\\\\\n&=-2\\qty(SS_{xy}-\\beta_1SS_{xx})=0.\n\\end{split}\n\\]\nTherefore, the solution to the equations, which is the best parameters that minimize the sum of square errors, is\n\nTheorem 5.1 \\[\n\\hat{\\beta}_1=\\frac{SS_{xy}}{SS_{xx}},\\quad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}.\n\\]\n\n\n\n5.2.2 MLE estimators\nSince we assume \\(\\varepsilon_i\\sim \\mathcal N(0,\\sigma^2)\\) i.i.d, we could actually compute the likelihood function explicitly. Recall that \\(y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\\), then \\[\n\\begin{split}\np(y_i\\mid x_i,\\beta_0,\\beta_1,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\qty(-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}).\n\\end{split}\n\\] Take all data into consideration. Let \\(\\mathbf x=(x_1,\\ldots,x_n)\\) and \\(\\mathbf y=(y_1,\\ldots,y_n)\\). Assuming that all \\(\\varepsilon_i\\)‚Äôs are independent, then the likelihood function is \\[\nL(\\beta_0,\\beta_1,\\sigma^2\\mid \\mathbf x, \\mathbf y)=\\prod_{i=1}^n\\frac1{\\sqrt{2\\pi\\sigma^2}}\\exp\\qty(-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}).\n\\] We could take negative log to make it simpler \\[\n\\operatorname{nll}(\\beta_0,\\beta_1,\\sigma^2\\mid \\mathbf x, \\mathbf y)=\\frac n2\\ln\\qty(2\\pi\\sigma^2)+\\frac1{2\\sigma^2}\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1x_i)^2.\n\\] This derived loss function is called the negative log-likelihood (NLL). Maximizing the likelihood is equivalent to minimizing \\(\\operatorname{nll}\\).\nFor fixed \\(\\sigma^2\\), minimizing \\(\\operatorname{nll}\\) over \\((\\beta_0,\\beta_1)\\) is equivalent to minimizing \\(\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1x_i)^2\\), so the MLE of \\((\\beta_0,\\beta_1)\\) is the same as the OLS estimator.\nTo estimate \\(\\sigma^2\\), treat \\(\\sigma^2\\) as a variable and differentiate: \\[\n\\pdv{\\operatorname{nll}}{\\sigma^2}=\\frac{n}2\\frac1{\\sigma^2}-\\frac12\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1x_i)^2=0.\n\\] Setting this derivative to zero yields\n\nTheorem 5.2 \\[\n\\hat\\sigma_{MLE}^2=\\frac1n\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)^2=\\frac{SSE}{n}.\n\\]\n\nSimilar to the sample variance scenario, the MLE estimator for variance is biased. The unbiased estimator of the error variance is the sample variance of the residuals. \\[\ns^2=\\frac{SSE}{n-2}.\n\\]\n\n\n5.2.3 Properties of these estimators\n\nTheorem 5.3 \\[\n\\operatorname{E}(\\hat{\\beta}_0)=\\beta_0, \\quad \\operatorname{E}(\\hat{\\beta}_1)=\\beta_1,\\quad \\operatorname{E}(s^2)=\\sigma^2.\n\\]\n\n\nClick for proof.\n\nThe key point is to treat all \\(x_i\\)‚Äôs as constants, and \\(y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\\) is a random variable due to \\(\\varepsilon_i\\). Note that \\(\\operatorname{E}(y_i)=\\beta_0+\\beta_1x_i\\) and \\(\\operatorname{E}(\\bar y)=\\frac1n\\sum_{i=1}^n\\operatorname{E}(y_i)=\\beta_0+\\beta_1\\bar x\\). \\[\n\\begin{split}\n\\operatorname{E}(SS_{xy})&=\\operatorname{E}\\qty(\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y))\\\\\n&=\\sum_{i=1}^n(x_i-\\bar x)(\\operatorname{E}(y_i)-\\operatorname{E}(\\bar y))\\\\\n&=\\sum_{i=1}^n(x_i-\\bar x)(\\beta_0+\\beta_1x_i-\\beta_0-\\beta_1\\bar x)\\\\\n&=\\sum_{i=1}^n(x_i-\\bar x)\\beta_1(x_i-\\bar x)\\\\\n&=\\beta_1SS_{xx}.\n\\end{split}\n\\] Therefore \\[\n\\operatorname{E}(\\hat\\beta_1)=\\operatorname{E}\\qty(\\frac{SS_{xy}}{SS_{xx}})=\\beta_1.\n\\] We now show that \\(\\hat\\beta_0\\) is an unbiased estimator of \\(\\beta_0\\). Recall that \\[\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x.\n\\] Taking expectations and using linearity of expectation, \\[\n\\begin{aligned}\n\\operatorname{E}(\\hat\\beta_0)\n&= \\operatorname{E}(\\bar y) - \\bar x\\,\\operatorname{E}(\\hat\\beta_1) \\\\\n&= (\\beta_0 + \\beta_1 \\bar x) - \\bar x\\,\\beta_1 \\\\\n&= \\beta_0.\n\\end{aligned}\n\\] Therefore, \\(\\hat\\beta_0\\) is an unbiased estimator of \\(\\beta_0\\).\nFor \\(s^2\\), it requires more knowledge from \\(\\chi^22\\) distribution.\n\n\\(\\varepsilon_i\\sim \\mathcal N(0,\\sigma^2)\\) i.i.d, and \\(x_i\\)‚Äôs are all constants.\n\\(e_i=y_i-\\hat y_i=\\varepsilon_i-(\\hat\\beta_0-\\beta_0)+(\\hat\\beta_1-\\beta_1)x_i\\). Note that \\(\\hat\\beta_0-\\beta_0\\) and \\(\\hat\\beta_1-\\beta_1\\) are functions of all \\(\\varepsilon_i\\)‚Äôs so each residual depends on the entire error vector.\nSince \\(SSE=\\sum_{i=1}^ne_i^2\\), by Theorem¬†2.3, one can show that \\(\\frac{SSE}{\\sigma^2}\\sim \\chi_{n-2}^2\\). The loss of 2 degrees of freedom comes from estimating \\(\\beta_0\\) and \\(\\beta_1\\).\nTherefore \\(\\operatorname{E}\\qty(\\frac{SSE}{\\sigma^2})=n-2\\).\nTherefore \\(\\operatorname{E}(s^2)=\\frac1{n-2}\\operatorname{E}(SSE)=\\sigma^2\\).\n\n\n\n\nTheorem 5.4 \\[\n\\operatorname{Var}(\\hat{\\beta}_1)=\\frac{\\sigma^2}{SS_{xx}}, \\quad \\operatorname{Var}(\\hat{\\beta}_0)=\\qty(\\frac{1}{n}+\\frac{\\bar x^2}{nSS_{xx}})\\sigma^2, \\quad \\operatorname{Var}(s^2)=\\frac{2\\sigma^4}{n-2}.\n\\]\n\n\nClick for proof.\n\n\\[\n\\operatorname{Var}(\\hat{\\beta}_1)=\\frac{1}{SS_{xx}^2}\\operatorname{Var}(SS_{xy})=\\frac{1}{SS_{xx}^2}\\operatorname{Var}\\qty[\\sum x_i(y_i-\\bar{y})]=\\frac{1}{SS_{xx}^2}SS_{xx}\\sigma^2=\\frac{\\sigma^2}{SS_{xx}}.\n\\]\nSince \\(\\operatorname{Cov}(\\bar y ,\\hat\\beta_1)=0\\), we have \\[\n\\begin{split}\n\\operatorname{Var}(\\hat{\\beta}_0)&=\\operatorname{Var}\\qty(\\bar y-\\hat{\\beta}_1\\bar x)=\\operatorname{Var}(\\bar y)+\\bar{x}^2\\operatorname{Var}(\\hat{\\beta}_1)=\\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{nSS_{xx}}=\\qty(\\frac{1}{n}+\\frac{\\bar x^2}{nSS_{xx}})\\sigma^2.\n\\end{split}\n\\]\nSince \\(\\frac{SSE}{\\sigma^2}\\sim\\chi^2_{n-2}\\), \\(\\var(\\frac{SSE}{\\sigma^2})=2(n-2)\\). Therefore \\[\n\\operatorname{Var}(s^2)=\\operatorname{Var}(\\frac{SSE}{n-2})=\\frac1{(n-2)^2}\\operatorname{Var}(SSE)=\\frac1{(n-2)^2}(\\sigma^2)^22(n-2)=\\frac{2\\sigma^4}{n-2}.\n\\]\n\n\nNote that these variance formulas involve \\(\\sigma^2\\). They come from a theoretical (population) analysis, so they can depend on unknown parameters. If we want numerical estimates of these variances from data, we must replace the unknown quantities with estimators and rebuild the variance calculations accordingly (typically by plugging in \\(s^2\\) for \\(\\sigma^2\\), and using the sample versions of any other unknown terms).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#beta_1",
    "href": "contents/2/slr.html#beta_1",
    "title": "5¬† Simple Linear regression",
    "section": "5.3 \\(\\beta_1\\)",
    "text": "5.3 \\(\\beta_1\\)\nThe most important question for \\(\\beta_1\\) is whether it is \\(0\\). In other words, we want to know whether \\(X\\) and \\(Y\\) have a linear relation.\n\nNull hypotheis: \\(H_0: \\beta_1=0\\).\nAlternative hypotheis: \\(H_a: \\beta_1\\neq0\\).\n\nSince \\(\\hat{\\beta}_1=\\beta_1+\\sum_{i=1}^n\\qty(\\dfrac{x_i-\\bar x}{SS_{xx}})\\varepsilon_i\\) is normal, we could use a t-test.\n\\[\nt=\\frac{\\hat{\\beta}_1-0}{s_{\\hat{\\beta}_1}}=\\frac{\\hat{\\beta}_1}{s/\\sqrt{SS_{xx}}},\\quad s^2=\\frac{SSE}{n-2}\n\\]\nWe mainly use a two-tailed test. Therefore we compute the corresponding p-value, which is the probability of getting a statistic at least as extreme as the observed value, assuming \\(H_0\\) is true. Let \\(t_c=\\frac{\\hat\\beta_1}{s/\\sqrt{SS_{xx}}}\\) be the observed test statistic. Then\n\\[\np\\text{-value} = \\Pr(\\abs{t}&gt;\\abs{t_c}\\mid H_0),\n\\] where \\(t\\sim t_{n-2}\\) under \\(H_0\\). We reject \\(H_0\\) if the p-value is smaller than or equal to \\(\\alpha\\).\n\nIf we reject \\(H_0\\), there is sufficient statistical evidence at level \\(\\alpha\\) to conclude that \\(\\beta_1\\neq 0\\). In this case, we say the linear relationship between \\(X\\) and \\(Y\\) is statistically significant.\nIf we fail to reject \\(H_0\\), we do not have enough evidence to determine whether \\(\\beta_1\\) differs from \\(0\\). In this case, we say the relationship is not statistically significant.\n\n\n\n\n\n\n\nImportant\n\n\n\nFailing to reject \\(H_0\\) does not mean \\(\\beta_1=0\\). It means there is insufficient evidence, given the data and model, to conclude that \\(\\beta_1\\neq0\\).\n\n\n\n5.3.1 Confidence interval\nSince \\(\\operatorname{Var}(\\hat\\beta_1)=\\frac{\\sigma^2}{SS+{xx}}\\approx\\frac{s^2}{SS_{xx}}\\), the standard deviation of \\(\\hat\\beta_1\\) can be estimated by \\(s_{\\hat\\beta_1}=s/\\sqrt{SS_{xx}}\\). Then the \\((1-\\alpha)\\times 100\\%\\) confidence interval for \\(\\beta_1\\) is \\[\nCI=\\hat\\beta_1\\pm t_{\\alpha/2,n-2}\\cdot \\frac{s}{\\sqrt{SS_{xx}}}.\n\\] In the hypothesis test described above, the null hypothesis is rejected if the observed \\(t_c\\) is sufficiently extreme, that is,\n\\[\n\\abs{t_c}=\\abs{\\frac{\\hat\\beta_1}{s/\\sqrt{SS_{xx}}}}&gt;t_{\\alpha/2, n-2}.\n\\] Equivalently, \\[\n\\abs{\\hat\\beta_1}&gt;t_{\\alpha/2, n-2}\\cdot \\frac{s}{\\sqrt{SS_{xx}}}.\n\\] Therefore it means that rejecting null hypothesis is the same as the confidence interval of \\(\\beta_1\\) doesn‚Äôt contain 0.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#sum-of-squares",
    "href": "contents/2/slr.html#sum-of-squares",
    "title": "5¬† Simple Linear regression",
    "section": "5.4 Sum of squares",
    "text": "5.4 Sum of squares\n\nTheorem 5.5 (The fundamental ANOVA identity) Define\n\n\\(SSE=\\sum (y_i-\\hat y_i)^2\\): sum of squared errors\n\\(SSR=\\sum (\\hat y_i-\\bar y)^2\\): sum of squares regression\n\\(SST=\\sum (y_i-\\bar y)^2\\): sum of squares total (also denoted \\(SS_{yy}\\))\n\nThese quantities satisfy \\[\nSST = SSR+SSE.\n\\]\n\n\nClick for proof.\n\n\\[\n\\begin{split}\nSSR&=\\sum (\\hat y_i-\\bar y)^2=\\sum (\\hat\\beta_0+\\hat\\beta_1x_i-\\bar y)^2=\\sum (\\bar y-\\hat\\beta_1\\bar x+\\hat\\beta_1x_i-\\bar y)^2\\\\\n&=\\sum (\\hat\\beta_1(x_i-\\bar x))^2=(\\hat\\beta_1)^2\\sum (x_i-\\bar x)^2\\\\\n&=\\frac{SS_{xy}^2}{SS_{xx}^2}SS_{xx}=\\frac{SS_{xy}^2}{SS_{xx}},\\\\\nSSE&=\\sum ( y_i-\\hat y_i)^2=\\sum (y_i-\\hat\\beta_0-\\hat\\beta_1x_i)^2=\\sum (y_i-\\bar y+\\hat\\beta_1\\bar x-\\hat\\beta_1x_i)^2\\\\\n&=\\sum ((y_i-\\bar y)-\\hat\\beta_1(x_i-\\bar x))^2=\\sum (y_i-\\bar y)^2-2\\hat\\beta_1(x_i-\\bar x)(y_i-\\bar y)+(\\hat\\beta_1(x_i-\\bar x))^2\\\\\n&=\\sum (y_i-\\bar y)^2-2\\hat\\beta_1\\sum(x_i-\\bar x)(y_i-\\bar y)+(\\hat\\beta_1)^2\\sum(x_i-\\bar x)^2\\\\\n&=SST-2\\frac{SS_{xy}}{SS_{xx}}SS_{xy}+\\qty(\\frac{SS_{xy}}{SS_{xx}})^2SS_{xx}\\\\\n&=SST-2\\frac{SS_{xy}^2}{SS_{xx}}+\\frac{SS_{xy}^2}{SS_{xx}}=SST-\\frac{SS_{xy}^2}{SS_{xx}}\\\\\n&=SST-SSR.\n\\end{split}\n\\] Therefore \\[\nSST=SSR+SSE.\n\\]\n\n\nThese three qualities are essential in interepretating regression models.\n\nSST measures the total variability in the response variable.\nSSR measures the variation explained by the regression model.\nSSE measures the unexplained (residual) variation.\n\nSSR measures the variation explained by the regression model. Since this variation is captured by the fitted regression line, it is considered explained by the model. The total variance is described by SST. So we would like the portion that is explained by our model is as high as possible. In other words, we would like \\(SSR/SST\\) is as high as possible. Since it is very important, we assign a variable to it.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#coefficient-of-determination",
    "href": "contents/2/slr.html#coefficient-of-determination",
    "title": "5¬† Simple Linear regression",
    "section": "5.5 Coefficient of Determination",
    "text": "5.5 Coefficient of Determination\nThe coefficient of determination is defined as \\[\nr^2=\\frac{SST-SSE}{SST}=1-\\frac{SSE}{SST}=\\frac{SSR}{SST}\n\\] It represents the proportion of the total sample variability in \\(ùëå\\) that is explained by the linear relationship between \\(ùëå\\) and \\(X\\). Equivalently, we have the following statement.\n\n\n\n\n\n\nNote\\(r^2\\) interpretation\n\n\n\nAbout \\(100(r^2)\\%\\) of the total variation in the sample \\(y\\)-values (measured by the total sum of squares about \\(\\bar y\\)) can be explained by using \\(X\\) to predict \\(Y\\) in the simple linear regression model.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#coefficent-of-correlation",
    "href": "contents/2/slr.html#coefficent-of-correlation",
    "title": "5¬† Simple Linear regression",
    "section": "5.6 Coefficent of Correlation",
    "text": "5.6 Coefficent of Correlation\nRecall that the Pearson coefficent of correlation is defined as follows. \\[\nr=\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}.\n\\]\nSince \\(\\hat\\beta_1=\\frac{SS_{xy}}{SS_{xx}}\\), we have\n\\[\nr=\\hat\\beta_1\\frac{\\sqrt{SS_{xx}}}{\\sqrt{SS_{yy}}}=\\hat\\beta_1\\frac{s_x}{s_y}\n\\] where \\(s_x\\) and \\(s_y\\) are the sample standard deviations of \\(X\\) and \\(Y\\). This suggests that \\(r\\) is the standarized slope, while \\(\\hat\\beta_1\\) dependes on the units of measurement of \\(X\\) and \\(Y\\).\nThis \\(r\\) and the previous \\(r^2\\) is related. \\[\n\\abs{r}=\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}=\\sqrt{\\frac{SS_{xy}^2}{SS_{xx}SS_{yy}}}=\\sqrt{\\frac{SSR}{SST}}=\\sqrt{r^2}.\n\\] This is the reason we choose these notations. Note that I ignore the sign of \\(r\\) in this computation. When using \\(\\sqrt{r^2}\\) to compute \\(r\\), we have to manually choose the sign for \\(r\\) based on other indicators.\nSince \\(r\\) and \\(\\beta_1\\) essentially describe the same linear relationship, we can also use \\(r\\) to perform the hypothesis test. We have \\[\nt=\\frac{\\hat\\beta_1}{s/\\sqrt{SS_{xx}}}=\\frac{\\hat\\beta_1 \\sqrt{SS_{xx}}}{s}=r \\sqrt{\\frac{SS_{yy}}{SSE/(n-2)}}=r\\sqrt{n-2}/{\\sqrt{1-r^2}}\n\\]",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#the-anova-table",
    "href": "contents/2/slr.html#the-anova-table",
    "title": "5¬† Simple Linear regression",
    "section": "5.7 The ANOVA table",
    "text": "5.7 The ANOVA table\nNow that we have defined these sums of squares, they are typically organized into an ANOVA Table. This table provides a convenient summary for assessing the overall significance of the regression model using the F-test.\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSum of Squares (SS)\nMean Square (MS)\nF\n\n\n\n\nRegression\n1\nSSR\nMSR = SSR / 1\nF = MSR / MSE\n\n\nError\nn ‚àí 2\nSSE\nMSE = SSE / (n ‚àí 2)\n\n\n\nTotal\nn ‚àí 1\nSST\n\n\n\n\n\nThe F-test examines whether the regression model, as a whole, explains a significant portion of the variability in the response variable. In the case of simple linear regression which only contains one independent variable, the F-test and the t-test are equivalent. In particular, the F-statistic is exactly the square of the corresponding t-statistic:\n\\[\nF=t^2.\n\\]\nWe will discuss the F-test in more details when we move on to multiple linear regression (MLR).",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html#prediction-interval-and-confidence-interval",
    "href": "contents/2/slr.html#prediction-interval-and-confidence-interval",
    "title": "5¬† Simple Linear regression",
    "section": "5.8 Prediction interval and Confidence interval",
    "text": "5.8 Prediction interval and Confidence interval\nWhen using a regression model, we need to quantify how uncertain our predictions are. There are two different types of intervals, depending on what we wish to estimate:\n\na confidence interval for the mean response \\(\\operatorname{E}(Y\\mid X=x)\\)\na prediction interval for a new individual observation \\(Y\\mid X=x\\).\n\n\n5.8.1 Estimate the variance\nFirst a preliminary result is that \\(\\bar y\\) and \\(\\hat\\beta_1\\) as random variables are uncorrelated (covariance is 0).\n\nLemma 5.1 \\(\\operatorname{Cov}(\\bar y, \\hat\\beta_1)=0\\).\n\n\nClick for proof.\n\n\\[\n\\begin{split}\n\\bar y&=\\beta_0+\\beta_1\\bar x+\\bar\\varepsilon,\\\\\n\\hat\\beta_1&=\\frac{1}{SS_{xx}}\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)=\\frac1{SS_{xx}}\\sum_{i=1}^n(x_i-\\bar x)y_i\\\\\n&=\\frac1{SS_{xx}}\\sum_{i=1}^n(x_i-\\bar x)(\\beta_0+\\beta_1x_i+\\varepsilon_i)\\\\\n&=\\frac1{SS_{xx}}\\sum_{i=1}^n(x_i-\\bar x)(\\beta_0+\\beta_1x_i)+\\frac1{SS_{xx}}\\sum_{i=1}^n(x_i-\\bar x)\\varepsilon_i.\n\\end{split}\n\\] Since \\(\\beta_0+\\beta_1\\bar x\\) and \\(\\frac1{SS_{xx}}\\sum_{i=1}^n(x_i-\\bar x)(\\beta_0+\\beta_1x_i)\\) are considered as constants, we have \\[\n\\begin{split}\n\\operatorname{Cov}(\\bar y,\\hat\\beta_1)&=\\operatorname{Cov}(\\bar\\varepsilon, \\sum_{i=1}^n(x_i-\\bar x)\\varepsilon_i)=\\operatorname{Cov}(\\sum_{j=1}^n\\varepsilon_j, \\sum_{i=1}^n(x_i-\\bar x)\\varepsilon_i)\\\\\n&=\\sum_{i,j=1}^n\\operatorname{Cov}(\\varepsilon_j, (x_i-\\bar x)\\varepsilon_i)=\\sum_{i=1}^n\\operatorname{Cov}(\\varepsilon_i, (x_i-\\bar x)\\varepsilon_i)\\\\\n&=\\sum_{i=1}^n(x_i-\\bar x)=0.\n\\end{split}\n\\]\n\n\n\nTheorem 5.6 Let \\(x\\) be a fixed value.\n\nThe variance of the estimated mean response is \\[\n\\operatorname{Var}(\\operatorname{E}(y))=\\sigma^2\\qty[\\frac1n+\\frac{(x-\\bar x)^2}{S_{xx}}].\n\\]\nThe variance of a new observation is \\[\n\\operatorname{Var}(y)=\\sigma^2\\qty[1+\\frac1n+\\frac{(x-\\bar x)^2}{S_{xx}}].\n\\]\n\n\n\nClick for proof.\n\nSince \\(\\bar y\\) and \\(\\hat{\\beta}_1\\) are uncorrelated, we have \\[\n\\begin{split}\n\\operatorname{Var}(\\operatorname{E}(y))&=\\operatorname{Var}(\\hat\\beta_0+\\hat\\beta_1x)=\\operatorname{Var}(\\bar y+\\hat\\beta_1(x-\\bar x))=\\operatorname{Var}(\\bar y)+(x-\\bar x)^2\\operatorname{Var}(\\hat\\beta_1)\\\\\n&=\\frac1n\\sigma^2+(x-\\bar x)^2\\frac{\\sigma^2}{SS_{xx}}\\\\\n&=\\sigma^2\\qty[\\frac1n+\\frac{(x-\\bar x)^2}{SS_{xx}}],\\\\\n\\operatorname{Var}(y)&=\\operatorname{Var}(\\operatorname{E}(y)+\\varepsilon)=\\operatorname{Var}(\\operatorname{E}(y))+\\operatorname{Var}(\\varepsilon)=\\sigma^2\\qty[\\frac1n+\\frac{(x-\\bar x)^2}{SS_{xx}}]+\\sigma^2\\\\\n&=\\sigma^2\\qty[1+\\frac1n+\\frac{(x-\\bar x)^2}{S_{xx}}].\n\\end{split}\n\\]\n\n\nTherefore, these two variances lead to:\n\na confidence interval for \\(\\operatorname{E}(Y\\mid X=x)\\)\na prediction interval for a new \\(Y\\mid X=x\\).\n\nThe prediction interval is always wider, because it includes both estimation uncertainty and irreducible error.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Simple Linear regression</span>"
    ]
  },
  {
    "objectID": "contents/2/rcode.html",
    "href": "contents/2/rcode.html",
    "title": "6¬† R Code",
    "section": "",
    "text": "6.1 Simulated dataset\nWhen we look at a simple linear regression model, we need to pay attention to the following qualities and their intepretations.\nWe‚Äôll use a simulated dataset throughout to demonstrate each concept.\nset.seed(123)\nn &lt;- 30\nx &lt;- runif(n, 1, 10)\ny &lt;- 20 + 5 * x + rnorm(n, 0, 5)\nplot(x, y)",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>R Code</span>"
    ]
  },
  {
    "objectID": "contents/2/rcode.html#lm-function-and-reports",
    "href": "contents/2/rcode.html#lm-function-and-reports",
    "title": "6¬† R Code",
    "section": "6.2 lm function and reports",
    "text": "6.2 lm function and reports\nIn R, the model is described by the formula language y~x. It represents the model \\(y=\\beta_0+\\beta_1x+\\varepsilon\\). The formula language will be discussed in more details in MLR.\nWe then use the lm function to solve the model and perform the analysis. What is left is to extract important information from the variable model.\n\nmodel &lt;- lm(y ~ x)\n\n\n\n\n\n\n\nTipWilkinson‚ÄìRogers notation and Formula language\n\n\n\nG. N. Wilkinson and C. E. Rogers introduced an algebraic system for describing the structure of linear and factorial models in [1]. This system later became known as the Wilkinson‚ÄìRogers notation.\nJohn Chambers implemented this notation in the S language and developed the formula language for statistical model specification in the 1980s. This design was subsequently inherited by the R language.\n\n\n\nplot(x, y)\nabline(coef(model), col='red')\n\n\n\n\n\n\n\n\nThe main purpose is to understand the following two tables.\n\nsummary(model)\n## \n## Call:\n## lm(formula = y ~ x)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.7147  -3.6014   0.0977   3.5281   9.4061 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  23.2998     2.3323    9.99 9.84e-11 ***\n## x             4.4834     0.3497   12.82 3.07e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.938 on 28 degrees of freedom\n## Multiple R-squared:  0.8545, Adjusted R-squared:  0.8493 \n## F-statistic: 164.4 on 1 and 28 DF,  p-value: 3.073e-13\n\n\nanova_alt(model)\n## Analysis of Variance Table\n## \n##        Df     SS     MS     F          P\n## Source  1 4008.3 4008.3 164.4 3.0734e-13\n## Error  28  682.7   24.4                 \n## Total  29 4691.0\n\nNote that we don‚Äôt use the built-in anova function since it doesn‚Äôt provide all info we need. We use an alternative version of the anova fucntion. It can also be downloaded from here.\n\n\nCode\n### Defind anova_alt function;\nanova_alt &lt;- function(object, reg_collapse = TRUE, ...) {\n    if (length(list(object, ...)) &gt; 1L) {\n        return(anova.lmlist(object, ...))\n    }\n    if (!inherits(object, \"lm\")) {\n        warning(\"calling anova.lm(&lt;fake-lm-object&gt;) ...\")\n    }\n    w &lt;- object$weights\n    ssr &lt;- sum(if (is.null(w)) object$residuals^2 else w * object$residuals^2)\n    mss &lt;- sum(if (is.null(w)) {\n        object$fitted.values^2\n    } else {\n        w *\n            object$fitted.values^2\n    })\n    if (ssr &lt; 1e-10 * mss) {\n        warning(\"ANOVA F-tests on an essentially perfect fit are unreliable\")\n    }\n    dfr &lt;- df.residual(object)\n    p &lt;- object$rank\n    if (p &gt; 0L) {\n        p1 &lt;- 1L:p\n        comp &lt;- object$effects[p1]\n        asgn &lt;- object$assign[stats:::qr.lm(object)$pivot][p1]\n        nmeffects &lt;- c(\"(Intercept)\", attr(object$terms, \"term.labels\"))\n        tlabels &lt;- nmeffects[1 + unique(asgn)]\n        ss &lt;- c(vapply(split(comp^2, asgn), sum, 1), ssr)\n        df &lt;- c(lengths(split(asgn, asgn)), dfr)\n        if (reg_collapse) {\n            if (attr(object$terms, \"intercept\")) {\n                collapse_p &lt;- 2:(length(ss) - 1)\n                ss &lt;- c(ss[1], sum(ss[collapse_p]), ss[length(ss)])\n                df &lt;- c(df[1], sum(df[collapse_p]), df[length(df)])\n                tlabels &lt;- c(tlabels[1], \"Source\")\n            } else {\n                collapse_p &lt;- 1:(length(ss) - 1)\n                ss &lt;- c(sum(ss[collapse_p]), ss[length(ss)])\n                df &lt;- c(df[1], sum(df[collapse_p]), df[length(df)])\n                tlabels &lt;- c(\"Regression\")\n            }\n        }\n    } else {\n        ss &lt;- ssr\n        df &lt;- dfr\n        tlabels &lt;- character()\n        if (reg_collapse) {\n            collapse_p &lt;- 1:(length(ss) - 1)\n            ss &lt;- c(sum(ss[collapse_p]), ss[length(ss)])\n            df &lt;- c(df[1], sum(df[collapse_p]), df[length(df)])\n        }\n    }\n\n    ms &lt;- ss / df\n    f &lt;- ms / (ssr / dfr)\n    P &lt;- pf(f, df, dfr, lower.tail = FALSE)\n    table &lt;- data.frame(df, ss, ms, f, P)\n    table &lt;- rbind(\n        table,\n        colSums(table)\n    )\n    if (attr(object$terms, \"intercept\")) {\n        table$ss[nrow(table)] &lt;- table$ss[nrow(table)] - table$ss[1]\n    }\n    table$ms[nrow(table)] &lt;- table$ss[nrow(table)] / table$df[nrow(table)]\n    table[length(P):(length(P) + 1), 4:5] &lt;- NA\n    table[(length(P) + 1), 3] &lt;- NA\n    dimnames(table) &lt;- list(\n        c(tlabels, \"Error\", \"Total\"),\n        c(\n            \"Df\", \"SS\", \"MS\", \"F\",\n            \"P\"\n        )\n    )\n    if (attr(object$terms, \"intercept\")) {\n        table &lt;- table[-1, ]\n        table$MS[nrow(table)] &lt;- table$MS[nrow(table)] * (table$Df[nrow(table)]) / (table$Df[nrow(table)] - 1)\n        table$Df[nrow(table)] &lt;- table$Df[nrow(table)] - 1\n    }\n    structure(table,\n        heading = c(\"Analysis of Variance Table\\n\"),\n        class = c(\"anova\", \"data.frame\")\n    )\n}\n\n\n\n6.2.1 Intercept (\\(\\hat\\beta_0\\))\n\\(\\hat\\beta_0\\) is the estimated intercept. It represents the expected value of \\(y\\) when \\(x = 0\\).\n\nbhat_0 &lt;- coef(model)['(Intercept)']\nbhat_0\n## (Intercept) \n##    23.29982\n\nIt can also be read from the summary table.\n\ncoefficients(summary(model))['(Intercept)', 'Estimate']\n## [1] 23.29982\n\n\n\n\n\n\n\nCaution\n\n\n\nIn some situations, the value \\(x=0\\) has no real-world meaning or is far outside the observed range of the data. Consequently, the intercept \\(\\hat\\beta_0\\), which represents the predicted response when \\(x=0\\), lacks a practical interpretation, although it remains an essential component of the regression model.\n\n\n\n\n6.2.2 Slope (\\(\\hat\\beta_1\\))\n\\(\\hat\\beta_1\\) is the estimated slope. It represents the expected change in \\(y\\) for a one-unit increase in \\(x\\). In other words, for each increase of 1 in the value of \\(x\\), the value of \\(y\\) will increase on average an amount equal to the slope.\n\nbhat_1 &lt;- coef(model)['x']\nbhat_1\n##        x \n## 4.483374\n\nIt can also be read from the summary table.\n\ncoefficients(summary(model))[\"x\", \"Estimate\"]\n## [1] 4.483374\n\n\n\n6.2.3 Fitted Values (\\(\\hat{y}\\))\nInterpretation: Predicted values of \\(y\\) based on the regression equation for each observed \\(x\\).\n\nfitted(model)\n##        1        2        3        4        5        6        7        8 \n## 39.38705 59.59160 44.28556 63.41327 65.73140 29.62141 49.09245 63.79263 \n##        9       10       11       12       13       14       15       16 \n## 50.03380 46.20777 66.39177 46.07539 55.12342 50.88916 31.93624 64.09146 \n##       17       18       19       20       21       22       23       24 \n## 37.71292 29.48031 41.01492 66.29777 63.67643 55.73807 53.62788 67.90235 \n##       25       26       27       28       29       30 \n## 54.24116 56.37266 49.73646 51.75704 39.45090 33.71928\n\nThese values are exactly \\(\\hat y_i=\\hat\\beta_0+\\hat\\beta_1x_i\\). You may compare the numbers with the manuall computation.\n\nbhat_0+bhat_1*x\n##  [1] 39.38705 59.59160 44.28556 63.41327 65.73140 29.62141 49.09245 63.79263\n##  [9] 50.03380 46.20777 66.39177 46.07539 55.12342 50.88916 31.93624 64.09146\n## [17] 37.71292 29.48031 41.01492 66.29777 63.67643 55.73807 53.62788 67.90235\n## [25] 54.24116 56.37266 49.73646 51.75704 39.45090 33.71928\n\n\n\n6.2.4 Sum of Squares (SSE, SSR, SST)\n\nSST (Total Sum of Squares): Total variation in \\(y\\)\nSSR (Regression Sum of Squares): Variation explained by the model\nSSE (Error Sum of Squares): Unexplained variation\nRelationship: \\(SST = SSR + SSE\\)\n\nThey can all be directly read from the anova table.\n\nSSR &lt;- anova_alt(model)[\"Source\", \"SS\"]\nSSE &lt;- anova_alt(model)[\"Error\", \"SS\"]\nSST &lt;- anova_alt(model)[\"Total\", \"SS\"]\n\nSSR\n## [1] 4008.298\nSSE\n## [1] 682.6885\nSST\n## [1] 4690.986\n\nYou may double check the values with the manual calculations.\n\ny_hat &lt;- fitted(model)\ny_mean &lt;- mean(y)\nSSR &lt;- sum((y_hat - y_mean)^2)\nSSE &lt;- sum((y - y_hat)^2)\nSST &lt;- sum((y - y_mean)^2)\n\nSSR\n## [1] 4008.298\nSSE\n## [1] 682.6885\nSST\n## [1] 4690.986\n\nWe could also verify that SST=SSE+SSR.\n\nSST - SSR - SSE\n## [1] -1.250555e-12\n\n\n\n6.2.5 Degrees of Freedom\nIn general,\n\n\\(df_{total}\\) = \\(n-1\\)\n\\(df_{model}\\) = number of predictors (excluding intercept)\n\\(df_{residual}\\) = \\(n\\) - number of estimated parameters\n\nFor simple linear regression, \\(df_{residual} = n - 2\\) since we estimate 2 parameters: \\(\\beta_0\\) and \\(\\beta_1\\).\n\ndf_total &lt;- length(y) - 1\ndf_residual &lt;- df.residual(model)\ndf_model &lt;- 1\n\ndf_total\n## [1] 29\ndf_residual\n## [1] 28\ndf_model\n## [1] 1\n\nThe degree of freedom is also recorded in the anova table.\n\ndf_total &lt;- anova_alt(model)[\"Total\", \"Df\"]\ndf_residual &lt;- anova_alt(model)[\"Error\", \"Df\"]\ndf_model &lt;- anova_alt(model)[\"Source\", \"Df\"]\n\ndf_total\n## [1] 29\ndf_residual\n## [1] 28\ndf_model\n## [1] 1\n\nNote that \\(df_{total}=df_{residual}+df_{model}\\).\n\ndf_total - df_residual - df_model\n## [1] 0\n\nIn addition, the third column in the anova table can be computed from SS column and Df column.\n\n\\(MSR=SSR/df_{model}\\)\n\\(MSE=SSE/df_{residual}\\)\n\n\nSSR / df_model - anova_alt(model)[\"Source\", \"MS\"]\n## [1] 9.094947e-13\n\n\nSSE / df_residual - anova_alt(model)[\"Error\", \"MS\"]\n## [1] 0\n\n\n\n6.2.6 Residual Variance (\\(s^2\\)) and Standard error (\\(s\\))\n\\(s^2\\) measures the variance of observed values (\\(\\sigma^2\\)) from fitted values. Lower values indicate better fit. It is estimated by SSE/(n-2) which is MSE. Therefore it can be directly read from the anova table\n\nanova_alt(model)['Error', 'MS']\n## [1] 24.38173\n\nThe standard error \\(s\\) is the square root of \\(s^2\\). Its interpretation is the same as the regular standard error: we expect most (approximately \\(95\\%\\)) of the observed \\(ùë¶\\)-values to lie within \\(2ùë†\\) of their respective least squares predicted values \\(\\hat y\\).\nThe standard error can be directly read from the summary table.\n\nsigma(model)\n## [1] 4.937786\n\nWe could verify the relation between these two values.\n\nanova_alt(model)[\"Error\", \"MS\"] - sigma(model)^2\n## [1] 0\n\n\n\n6.2.7 Coefficient of Determination (\\(R^2\\))\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\). It ranges from 0 to 1, with higher values indicating better fit.\n\\(R^2\\) (and the adjusted \\(R^2\\) which will be discussed in MLR) can be read directly from the summary table. Note that in base R, r.squared and adj.r.squared don‚Äôt have their own functions. So we have to directly read the value from the summary table.\n\nsummary(model)$r.squared\n## [1] 0.854468\nsummary(model)$adj.r.squared\n## [1] 0.8492704\n\nOn the other side, we could also estimate \\(R^2\\) using SSR/SST based on the values from the avnoa table.\n\nSSR/SST\n## [1] 0.854468\n\nBy the way, \\(R^2\\) is the square of the Pearson coefficient of correlation \\(r\\).\n\nr &lt;- cor(x, y)\nsummary(model)$r.squared - r^2\n## [1] 0\n\n\n\n6.2.8 Standard Error of Slope (\\(s_{\\hat\\beta_1}\\)) and t-test\nWe estimate the variance of \\(\\hat\\beta_1\\) and perform the t-test. The related statistics can be read directly from the summary table.\n\ncoefficients(summary(model))['x', ]\n##     Estimate   Std. Error      t value     Pr(&gt;|t|) \n## 4.483374e+00 3.496691e-01 1.282176e+01 3.073410e-13\n\nWe could access each statistic directly.\n\ns_beta1 &lt;- coefficients(summary(model))[\"x\", \"Std. Error\"]\nt_statistic &lt;- coefficients(summary(model))[\"x\", \"t value\"]\np_value_beta1 &lt;- coefficients(summary(model))[\"x\", \"Pr(&gt;|t|)\"]\n\ns_beta1\n## [1] 0.3496691\nt_statistic\n## [1] 12.82176\np_value_beta1\n## [1] 3.07341e-13\n\nNote that once we have t-statistic and the degree of freedom, we can use the t-distribution formula to compute the p-value.\n\nt_statistic &lt;- coefficients(summary(model))[\"x\", \"t value\"]\ndof_r &lt;- df.residual(model)\npt(abs(t_statistic), dof_r, lower.tail = FALSE) + pt(-abs(t_statistic), dof_r)\n## [1] 3.07341e-13\n\nBy the way, t-statistic can also be computed by the formula \\(t=\\hat\\beta_1/s_{\\hat\\beta_1}\\).\n\nbhat_1/s_beta1\n##        x \n## 12.82176\n\n\n\n6.2.9 Confidence interval for the slope\nSince we have an estimator for \\(\\beta_1\\) and the standard error for the estimator \\(s_{\\hat\\beta_1}\\), we could build the confidence interval for the estimation.\nIn R, we could directly use confint to compute it.\n\nconfint(model, level=0.95)\n##                2.5 %    97.5 %\n## (Intercept) 18.52231 28.077324\n## x            3.76711  5.199639\n\nIts interpretation is as the regular confidence interval: we are 95% confident that for each unit increase in \\(x\\), the estimated \\(y\\) increases between 3.7671096 to 5.1996392.\n\n\n6.2.10 Overall Model Significance (F-test)\nThe F-test tests whether the regression model is significant overall. For simple linear regression, this is equivalent to the t-test for \\(\\beta_1\\).\nThe F-statistic can be found from both the summary table and the the anova table.\n\nsummary(model)$fstatistic[\"value\"]\n##    value \n## 164.3976\n\nIf we only use the summary table, the corresponding p-value should be computed from the F-statistic.\n\nf_statistic &lt;- summary(model)$fstatistic[\"value\"]\nnof &lt;- summary(model)$fstatistic[\"numdf\"]\ndof_r &lt;- summary(model)$fstatistic[\"dendf\"]\npf(f_statistic, nof, dof_r, lower.tail = FALSE)\n##       value \n## 3.07341e-13\n\nThe F-statistic and the p-value can also be found directly from the anova table.\n\nanova_alt(model)[\"Source\", \"F\"]\n## [1] 164.3976\nanova_alt(model)[\"Source\", \"P\"]\n## [1] 3.07341e-13\n\nIt is also computed by MSR/MSE.\n\nanova_alt(model)[\"Source\", \"MS\"] / anova_alt(model)[\"Error\", \"MS\"]\n## [1] 164.3976\n\nFor simple linear regression, F-test only tests one variable. So it is equivalent to the t-test for \\(\\beta_1\\). The relation is that in one variable case, the F-statistic is the square of the t-statistic.\n\nf_statistic - t_statistic^2\n##        value \n## 1.136868e-13\n\nIn addition, the p-values for t-test and F-test are exactly the same in simple linear regression.\n\n\n6.2.11 Residuals (\\(e_i = y_i - \\hat{y}_i\\))\n\\(e_i\\) stands for the differences between observed and predicted values.\n\nresiduals(model)\n##           1           2           3           4           5           6 \n##   7.4885015   3.3713865 -10.7146876   4.8292907  -0.7743252  -7.9104910 \n##           7           8           9          10          11          12 \n##  -1.4175726  -3.7637969  -3.8636805  -3.7853000  -6.7677385   3.5135781 \n##          13          14          15          16          17          18 \n##   1.1341252  -5.8113444   3.9644430   3.5329812  -3.1143338   1.8879962 \n##          19          20          21          22          23          24 \n##   3.1321842   5.7628012   4.7960369   3.2076747  -0.1146320   0.3099803 \n##          25          26          27          28          29          30 \n##  -1.6367588  -2.9623235  -1.2930738  -6.3476343   9.4060724   3.9406407\n\nThe residual analysis is a very important topic. The whole analysis can be summarized in the following plot. The details will be discussed in the last part of this course.\n\npar(mfrow = c(2, 2))\nplot(model)\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\n\n\n6.2.12 Confidence and Prediction Intervals\n\nConfidence Interval: CI for the mean response at a given \\(x\\) value\nPrediction Interval: CI for an individual new observation (wider than the CI computed above)\n\n\nnewx = data.frame(x=seq(0, 10, by=0.1))\npred.int &lt;- predict(model, newx, interval='prediction')\nconf.int &lt;- predict(model, newx, interval='confidence')\n\nBoth pred.int and conf.int are matrices, where the first column is the fitted value, the second column and the third column are the corresponding interval bounds.\n\nplot(x, y)\n\nlines(newx$x, pred.int[, 'fit'])\n\nlines(newx$x, pred.int[, 'lwr'], col = \"red\")\nlines(newx$x, pred.int[, 'upr'], col = \"red\")\n\nlines(newx$x, conf.int[, 'lwr'], col = \"blue\")\nlines(newx$x, conf.int[, 'upr'], col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Wilkinson, G. N. and Rogers, C. E. (1973). Symbolic description of factorial models for analysis of variance. Applied Statistics 22 392.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>R Code</span>"
    ]
  },
  {
    "objectID": "contents/2/project1.html",
    "href": "contents/2/project1.html",
    "title": "7¬† Example: SLR phase",
    "section": "",
    "text": "7.1 The Boston Housing Dataset\nThe Boston housing dataset originates from a hedonic price analysis of 506 census tracts in the Boston Standard Metropolitan Statistical Area conducted by Harrison and Rubinfeld to estimate households‚Äô willingness to pay for improvements in air quality, particularly reductions in nitrogen oxides (NOx) concentrations [1]. The data were assembled primarily from 1970 U.S. Census housing tabulations combined with modeled environmental pollution measures. Since its original publication, the dataset has been widely disseminated as the Boston dataset in econometrics, statistics, and machine learning, where it is commonly used to illustrate and evaluate linear regression methods. It was notably employed by Belsley, Kuh, and Welsch in Regression Diagnostics: Identifying Influential Data and Sources of Collinearity (pp.¬†244‚Äì261) [2] to demonstrate diagnostic techniques for detecting influential observations and multicollinearity, and has subsequently appeared in numerous methodological studies, including Quinlan‚Äôs work on combining instance-based and model-based learning [3].\nIn R, the MASS library contains Boston data set, which has 506 rows and 14 columns. The dataset records medv (median house value) and 13 predictors for 506 neighborhoods around Boston.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Example: SLR phase</span>"
    ]
  },
  {
    "objectID": "contents/2/project1.html#explore-the-boston-housing-dataset",
    "href": "contents/2/project1.html#explore-the-boston-housing-dataset",
    "title": "7¬† Example: SLR phase",
    "section": "7.2 Explore the Boston Housing Dataset",
    "text": "7.2 Explore the Boston Housing Dataset\nFirst of all, let‚Äôs take a look at the variables in the dataset Boston.\n\nlibrary(MASS)\nhead(Boston)\n##      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n## 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n## 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n## 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n##   medv\n## 1 24.0\n## 2 21.6\n## 3 34.7\n## 4 33.4\n## 5 36.2\n## 6 28.7\nnames(Boston)\n##  [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n##  [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"\ndim(Boston)\n## [1] 506  14\n\n\n\nnames() ‚Äî print out all the variable names;\nhead() ‚Äî print out the first 5 rows of the dataset;\ndim() ‚Äî print out the shape of the dataset.\n\n\nYou may use ?Boston to look at the brief describption.\nSince we would like to directly work with varibles in the dataset, it is better to attach it to the working space.\n\nattach(Boston)\n\n\n7.2.1 Data Type\nFirst we need to know the data type of every variable. Based on the information we have, we know\n\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\ncrim\nPer capita crime rate by town\nNumerical, continuous\n\n\nzn\nProportion of residential land zoned for lots over 25,000 sq.ft.\nNumerical, continuous\n\n\nindus\nProportion of non-retail business acres per town\nNumerical, continuous\n\n\nchas\nCharles River dummy variable (1 if tract bounds river; 0 otherwise)\nCategorical, nominal\n\n\nnox\nNitrogen oxides concentration (parts per 10 million)\nNumerical, continuous\n\n\nrm\nAverage number of rooms per dwelling\nNumerical, continuous\n\n\nage\nProportion of owner-occupied units built prior to 1940\nNumerical, continuous\n\n\ndis\nWeighted mean distance to five Boston employment centers\nNumerical, continuous\n\n\nrad\nIndex of accessibility to radial highways (larger = better accessibility)\nCategorical, ordinal\n\n\ntax\nFull-value property-tax rate per $10,000\nNumerical, continuous\n\n\nptratio\nPupil‚Äìteacher ratio by town\nNumerical, continuous\n\n\nblack\n\\(1000(B_k - 0.63)^2\\), where \\(B_k\\) is the proportion of Black residents\nNumerical, continuous\n\n\nlstat\nPercentage of lower-status population\nNumerical, continuous\n\n\nmedv\nMedian value of owner-occupied homes (in $1,000s)\nNumerical, continuous\n\n\n\n\n\n7.2.2 Summary of all the variables\nUsually we would like to check two things: the distribution of each variable, and finding all missing values.\nAre there any missing values?\n\nsapply(Boston, anyNA)\n##    crim      zn   indus    chas     nox      rm     age     dis     rad     tax \n##   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE \n## ptratio   black   lstat    medv \n##   FALSE   FALSE   FALSE   FALSE\n\nFrom the output, we know there is no missing values in the Boston dataset.\nNext, we find the summary of all the 13 variables as follows. These summary shows a brief glance of the distributions of all variables.\n\nsummary(Boston)\n##       crim                zn             indus            chas        \n##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n##  1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n##       nox               rm             age              dis        \n##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n##       rad              tax           ptratio          black       \n##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  \n##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  \n##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44  \n##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  \n##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  \n##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  \n##      lstat            medv      \n##  Min.   : 1.73   Min.   : 5.00  \n##  1st Qu.: 6.95   1st Qu.:17.02  \n##  Median :11.36   Median :21.20  \n##  Mean   :12.65   Mean   :22.53  \n##  3rd Qu.:16.95   3rd Qu.:25.00  \n##  Max.   :37.97   Max.   :50.00\n\nNote that chas and rad are supposed to be categorical data. In this summary they are still treated as numerical. Therefore we could change their type if necessary.\n\nchas &lt;- factor(chas)\nrad &lt;- factor(rad)\n\nsummary(Boston)\n##       crim                zn             indus            chas        \n##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n##  1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n##       nox               rm             age              dis        \n##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n##       rad              tax           ptratio          black       \n##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  \n##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  \n##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44  \n##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  \n##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  \n##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  \n##      lstat            medv      \n##  Min.   : 1.73   Min.   : 5.00  \n##  1st Qu.: 6.95   1st Qu.:17.02  \n##  Median :11.36   Median :21.20  \n##  Mean   :12.65   Mean   :22.53  \n##  3rd Qu.:16.95   3rd Qu.:25.00  \n##  Max.   :37.97   Max.   :50.00\n\n\n\n7.2.3 Histogram\nWe could draw histogram for a particular variable, say medv.\n\nhist(medv)\n\n\n\n\n\n\n\n\nWe could also draw all the histogram for all numerical variables in the data frame Boston with the help of `Hmisc::hist.data.frame. Note that categorical data are automatically removed.\n\nlibrary(Hmisc)\nhist.data.frame(Boston)\n\n\n\n\n\n\n\n\n\n\n7.2.4 Scatter plot\nFinally, the most import plot for regression should be the scatter plot. For example, let us see the relation between lstat and medv.\n\nplot(lstat, medv)",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Example: SLR phase</span>"
    ]
  },
  {
    "objectID": "contents/2/project1.html#simple-linear-regression",
    "href": "contents/2/project1.html#simple-linear-regression",
    "title": "7¬† Example: SLR phase",
    "section": "7.3 Simple Linear Regression",
    "text": "7.3 Simple Linear Regression\nWe will seek to predict medv using 13 predictors such as rm (average number of rooms per house), age (proportion of the owner-occupied units built prior to 1940), or lstat (percent of households with low socioeconomic status).\n\n7.3.1 Find a strongest linear correlation\nIn general there has to be a more complicated analysis about choosing the best variable. In this case in order to demonstrate the idea, we will only use the most correlted numerical variable.\nHere, we use corrlation matrix to find the independent variable which has the strongest linear correlation to the dependent variable.\nWe first find out the numeric columns.\n\nBoston_num &lt;- Boston[, sapply(Boston, is.numeric)] \n\nThen we compute the corrlation matrix. We would like to find the column that is most correlated to medv. We find that lstat and medv has the strongest linear correlation.\n\ncor_matrix &lt;- cor(Boston_num)\nround(cor_matrix, 2)\n##          crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio\n## crim     1.00 -0.20  0.41 -0.06  0.42 -0.22  0.35 -0.38  0.63  0.58    0.29\n## zn      -0.20  1.00 -0.53 -0.04 -0.52  0.31 -0.57  0.66 -0.31 -0.31   -0.39\n## indus    0.41 -0.53  1.00  0.06  0.76 -0.39  0.64 -0.71  0.60  0.72    0.38\n## chas    -0.06 -0.04  0.06  1.00  0.09  0.09  0.09 -0.10 -0.01 -0.04   -0.12\n## nox      0.42 -0.52  0.76  0.09  1.00 -0.30  0.73 -0.77  0.61  0.67    0.19\n## rm      -0.22  0.31 -0.39  0.09 -0.30  1.00 -0.24  0.21 -0.21 -0.29   -0.36\n## age      0.35 -0.57  0.64  0.09  0.73 -0.24  1.00 -0.75  0.46  0.51    0.26\n## dis     -0.38  0.66 -0.71 -0.10 -0.77  0.21 -0.75  1.00 -0.49 -0.53   -0.23\n## rad      0.63 -0.31  0.60 -0.01  0.61 -0.21  0.46 -0.49  1.00  0.91    0.46\n## tax      0.58 -0.31  0.72 -0.04  0.67 -0.29  0.51 -0.53  0.91  1.00    0.46\n## ptratio  0.29 -0.39  0.38 -0.12  0.19 -0.36  0.26 -0.23  0.46  0.46    1.00\n## black   -0.39  0.18 -0.36  0.05 -0.38  0.13 -0.27  0.29 -0.44 -0.44   -0.18\n## lstat    0.46 -0.41  0.60 -0.05  0.59 -0.61  0.60 -0.50  0.49  0.54    0.37\n## medv    -0.39  0.36 -0.48  0.18 -0.43  0.70 -0.38  0.25 -0.38 -0.47   -0.51\n##         black lstat  medv\n## crim    -0.39  0.46 -0.39\n## zn       0.18 -0.41  0.36\n## indus   -0.36  0.60 -0.48\n## chas     0.05 -0.05  0.18\n## nox     -0.38  0.59 -0.43\n## rm       0.13 -0.61  0.70\n## age     -0.27  0.60 -0.38\n## dis      0.29 -0.50  0.25\n## rad     -0.44  0.49 -0.38\n## tax     -0.44  0.54 -0.47\n## ptratio -0.18  0.37 -0.51\n## black    1.00 -0.37  0.33\n## lstat   -0.37  1.00 -0.74\n## medv     0.33 -0.74  1.00\n\nThe matrix can be visualized. lstat is also the most obvious choice from the plot.\n\nlibrary(ggcorrplot)\nggcorrplot(cor_matrix, hc.order = TRUE)\n\n\n\n\n\n\n\n\nFor the next step, we will explore the linear relationship between the two variables. That is, \\(y\\)=medv, \\(x\\)=lstat.\n\nplot(lstat, medv)\n\n\n\n\n\n\n\n\nThe data appears to demostrate a straight-line relationshiop. As the percentage of lower status of the population (lstat) increase, the median home value decrease, which fits with common sense. The scatterplot shows curvilinear relation, which suggests a curivilinear model might be a better fit. In this chapter, we fit the straight-line model using lm.\nHere is a quick glance about how lm works.\n\nmodel &lt;- lm(medv ~ lstat, data = Boston)\nmodel\n## \n## Call:\n## lm(formula = medv ~ lstat, data = Boston)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       34.55        -0.95\ncoef(model)\n## (Intercept)       lstat \n##  34.5538409  -0.9500494\n\nThe estimated regression equation by using least squares method is \\(\\hat{y}\\)=34.5538409\\(+\\)(-0.9500494)\\(x\\).\n\n\n7.3.2 Test of model utility\nWe would like to see the output of the model.\n\nanova_alt(model)\n## Analysis of Variance Table\n## \n##         Df    SS      MS      F          P\n## Source   1 23244 23243.9 601.62 5.0811e-88\n## Error  504 19472    38.6                  \n## Total  505 42716\nsummary(model)\n## \n## Call:\n## lm(formula = medv ~ lstat, data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nLet us check the confidence interval for \\(\\beta_1\\).\n\n# Combine to display the coef and confidence interval for the parameters together;\ncoef &lt;- coefficients(summary(model))\ncoef_CI &lt;- confint(model, level = 0.95)\ncbind(coef, coef_CI)\n##               Estimate Std. Error   t value      Pr(&gt;|t|)     2.5 %     97.5 %\n## (Intercept) 34.5538409 0.56262735  61.41515 3.743081e-236 33.448457 35.6592247\n## lstat       -0.9500494 0.03873342 -24.52790  5.081103e-88 -1.026148 -0.8739505\n\n\nSignificance: We test the \\(H_0:\\beta_1=0\\) vs \\(H_a: \\beta_1\\neq 0\\). For simple linear regression, the result would be the same as the correlation hypothesis test above. We will conclude percentage of lower status (lstat) is a significant predictor to the median house price (medv).\nCI for \\(\\beta_1\\): The confidence interval for slope \\(\\beta_1\\) is (-1.0261482, -0.8739505). We are 95% confident that the mean median home price decreases around 0.874 to 1.026 thousands of dollors per additional percent increase in low-status of the population.\nsd for \\(\\beta_1\\): The estimated standard deviation of \\(\\varepsilon\\) is \\(s\\)=6.2157604, which implies that most of the observed median home price will fall within in approximately \\(2s\\)=12.4 thousands of dollars of their respective predicted values when using the least squares line.\n\\(R^2\\): The coefficient of determination is 0.544. This value implies that about 54% of the sample variation in median home price is explained by the low-status percent and the median home price. However, as we note that \\(R^2\\) is not very high, we need to modify our model in the future.\nPrediction: With relatively small \\(2s\\), significant linear relationship between lstat and medv, we could get confidence interval and prediction interval as follows.\n\n\nplot(lstat, medv)\n\nnewx &lt;- data.frame(lstat = seq(min(lstat), max(lstat), by = 0.1))\npred.int &lt;- predict(model, newx, interval = \"prediction\")\nconf.int &lt;- predict(model, newx, interval = \"confidence\")\n\nlines(newx$lstat, pred.int[, \"fit\"])\n\nlines(newx$lstat, pred.int[, \"lwr\"], col = \"red\")\nlines(newx$lstat, pred.int[, \"upr\"], col = \"red\")\n\nlines(newx$lstat, conf.int[, \"lwr\"], col = \"blue\")\nlines(newx$lstat, conf.int[, \"upr\"], col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n7.3.3 Residual Analysis\nThe detailed residual analysis will be covered in the future. Here we only show the very basic analysis, that is the plot of the residual vs the predictor variable lstat.\n\nres &lt;- resid(model)\nplot(lstat, res)\n\n\n\n\n\n\n\n\nWe could find that there is a clear u-shape in the residual plot. And the variance shows non-constant variance issue, as the size of the residuals decreases as the value of lstat increase. This residual plot indicates that a multiplicative model may be appropriate.\n\n\n\n\n[1] Harrison, D. and Rubinfeld, D. L. (1978). Hedonic housing prices and the demand for clean air. Journal of Environmental Economics and Management 5 81‚Äì102.\n\n\n[2] Belsley, D. A., Kuh, E. and Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley & Sons, New York.\n\n\n[3] Quinlan, J. R. (1993). Combining instance-based and model-based learning. In Proceedings of the tenth international conference on machine learning pp 236‚Äì43. Morgan Kaufmann, Amherst, Massachusetts.",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Example: SLR phase</span>"
    ]
  },
  {
    "objectID": "contents/app/data.html",
    "href": "contents/app/data.html",
    "title": "Appendix A ‚Äî Datasets",
    "section": "",
    "text": "A.1 possum\nThe dataset possum originates from the DAAG package, created to support the book Data Analysis and Graphics Using R [1]. The underlying data were collected in [2], with the scientific aim of quantifying morphological variation among geographically distinct populations of the mountain brushtail possum. The dataset possum contains nine morphometric measurements on each of 104 possums trapped across seven Australian sites spanning southern Victoria to central Queensland. Additional details and variable descriptions are available in the official DAAG package documentation.\nThe package can be installed by the following code.\ninstall.packages(\"DAAG\")\nThe dataset can be loaded by the following code.\nlibrary(DAAG)\ndata(possum)\nhead(possum)\n##     case site Pop sex age hdlngth skullw totlngth taill footlgth earconch  eye\n## C3     1    1 Vic   m   8    94.1   60.4     89.0  36.0     74.5     54.5 15.2\n## C5     2    1 Vic   f   6    92.5   57.6     91.5  36.5     72.5     51.2 16.0\n## C10    3    1 Vic   f   6    94.0   60.0     95.5  39.0     75.4     51.9 15.5\n## C15    4    1 Vic   f   6    93.2   57.1     92.0  38.0     76.1     52.2 15.2\n## C23    5    1 Vic   f   2    91.5   56.3     85.5  36.0     71.0     53.2 15.1\n## C24    6    1 Vic   f   1    93.1   54.8     90.5  35.5     73.2     53.6 14.2\n##     chest belly\n## C3   28.0    36\n## C5   28.5    33\n## C10  30.0    34\n## C15  28.0    34\n## C23  28.5    33\n## C24  30.0    32",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "contents/app/data.html#sec-app_possum",
    "href": "contents/app/data.html#sec-app_possum",
    "title": "Appendix A ‚Äî Datasets",
    "section": "",
    "text": "[1] Maindonald, J. H. and Braun, W. J. (2011). Data analysis and graphics using r. An example-based approach. Cambridge University Press, Cambridge.\n\n\n[2] Lindenmayer, D., Viggers, K., Cunningham, R. and Donnelly, C. (1995). Morphological variation among populations of the mountain brushtail possum, trichosurus-caninus ogilby (phalangeridae, marsupialia). Australian Journal of Zoology 43 449‚Äì58.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "contents/app/r.html",
    "href": "contents/app/r.html",
    "title": "Appendix B ‚Äî Quick intro to R",
    "section": "",
    "text": "B.1 For those who are from other languages, especially Python",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Quick intro to R</span>"
    ]
  },
  {
    "objectID": "contents/app/r.html#for-those-who-are-from-other-languages-especially-python",
    "href": "contents/app/r.html#for-those-who-are-from-other-languages-especially-python",
    "title": "Appendix B ‚Äî Quick intro to R",
    "section": "",
    "text": "The character . is NOT special. It doesn‚Äôt mean packages or classes. It is just one of the characters that can be used as variable names.\nR recommends to use &lt;- to denote assigning values, although = is accepted. a &lt;- 1 and a = 1 are essentially the same. It has some benefits",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Quick intro to R</span>"
    ]
  }
]